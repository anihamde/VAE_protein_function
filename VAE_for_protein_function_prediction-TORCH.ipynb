{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDsSVgwI5Tfc"
   },
   "source": [
    "### Using a Variational Auto-encoder to predict protein fitness from evolutionary data\n",
    "\n",
    "July 20, 2017\n",
    "### Sam Sinai and Eric Kelsic\n",
    "\n",
    "\n",
    "## For the blog post associated with this notebook see [this post](https://samsinai.github.io/jekyll/update/2017/08/14/Using-a-Variational-Autoencoder-to-predict-protein-function.html). \n",
    "\n",
    "\n",
    "This notebook it organized in 3 sections. In section 1 we show our workflow for pre-processing the biological data. We then train the model on the alignment data in section 2. In section 3 we compare the predictions of the model on the [PABP yeast](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3851721/) dataset. In section 4 we report the results from analyzing multiple other datasets. Finally we pose some questions with regards to improving the model for interested researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXSFgjsE5Tfe"
   },
   "outputs": [],
   "source": [
    "### IT'S 82 x 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frm5qU-tgUz8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuXjvOf95Tfj"
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math,random,re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpJ-72V05Tfn"
   },
   "outputs": [],
   "source": [
    "#Machine learning/Stats imports \n",
    "from scipy.stats import norm\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.distributions as D\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from keras import regularizers\n",
    "# from keras.layers import LSTM, RepeatVector\n",
    "# from keras.layers import Input, Dense, Lambda, Dropout,Activation, TimeDistributed\n",
    "# from keras import backend as K\n",
    "# from keras import objectives\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRSDjG2AOKcz"
   },
   "outputs": [],
   "source": [
    "# JUST UPLOAD MANUALLY TBH\n",
    "\n",
    "# # if in colab\n",
    "\n",
    "# # models\n",
    "# print('models.py')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('models.py','wb').write(src)\n",
    "\n",
    "# # helper_tools\n",
    "# print('helper_tools.py')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('helper_tools.py','wb').write(src)\n",
    "\n",
    "# # helper_tools_for_plotting\n",
    "# print('helper_tools_for_plotting.py')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('helper_tools_for_plotting.py','wb').write(src)\n",
    "\n",
    "# # singles\n",
    "# print('PABP_YEAST_Fields2013-singles.csv')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('PABP_YEAST_Fields2013-singles.csv','wb').write(src)\n",
    "\n",
    "# # doubles\n",
    "# print('PABP_YEAST_Fields2013-doubles.csv')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('PABP_YEAST_Fields2013-doubles.csv','wb').write(src)\n",
    "\n",
    "# # PABP_YEAST_hmmerbit_t0.2_r50000.reweight\n",
    "# print('PABP_YEAST_hmmerbit_t0.2_r50000.reweight')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('PABP_YEAST_hmmerbit_t0.2_r50000.reweight','wb').write(src)\n",
    "\n",
    "# # PABP_YEAST_hmmerbit_plmc_n5_m30_f50_t0.2_r115-210_id100_b48.a2m\n",
    "# print('PABP_YEAST_hmmerbit_plmc_n5_m30_f50_t0.2_r115-210_id100_b48.a2m')\n",
    "# from google.colab import files\n",
    "# src = list(files.upload().values())[0]\n",
    "# open('PABP_YEAST_hmmerbit_plmc_n5_m30_f50_t0.2_r115-210_id100_b48.a2m','wb').write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYzU8dDr5Tfr"
   },
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rDzv0SK5Tf2"
   },
   "source": [
    "## 1.  Data pre-processing\n",
    "\n",
    "Defining the alphabet that is used for Amino-Acids throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eeo0BTU5Tf3"
   },
   "outputs": [],
   "source": [
    "#Invariants\n",
    "ORDER_KEY=\"XILVAGMFYWEDQNHCRKSTPBZ-\"[::-1]\n",
    "ORDER_LIST=list(ORDER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V4RAlKO5Tf7"
   },
   "source": [
    "These are helper functions to clean and process data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIhWvpqE5Tf8"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "from helper_tools import *\n",
    "from helper_tools_for_plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RLAZpRZ5Tf_"
   },
   "source": [
    "Import the alignment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80123,
     "status": "ok",
     "timestamp": 1555720436517,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "GzBbZaRn5TgA",
    "outputId": "2f258da4-e68a-41fe-a11b-488738aeb9d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points:  50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PABP_YEAST/115-210</td>\n",
       "      <td>qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398</td>\n",
       "      <td>........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112</td>\n",
       "      <td>........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "0                                PABP_YEAST/115-210   \n",
       "1  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294   \n",
       "2  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625   \n",
       "3  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398   \n",
       "4   ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112   \n",
       "\n",
       "                                            sequence  \n",
       "0  qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...  \n",
       "1  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...  \n",
       "2  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...  \n",
       "3  ........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...  \n",
       "4  ........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pdataframe_from_alignment_file(\"PABP_YEAST_hmmerbit_plmc_n5_m30_f50_t0.2_r115-210_id100_b48.a2m\",50000)\n",
    "print (\"number of data points: \",len(data))\n",
    "data_set_size=len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNtSJ-J45TgH"
   },
   "source": [
    "Let's see how long the sequence is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79798,
     "status": "ok",
     "timestamp": 1555720436519,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "lP1Ftya65TgJ",
    "outputId": "d09419b9-12db-4e00-84a6-e39a551f91d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequence: 96\n",
      "sample sequence:  qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGFGFVHFEEEGAAKEAIDALNGMLLNGQEIYVAPHLSRkerdsq\n"
     ]
    }
   ],
   "source": [
    "print (\"length of sequence:\", len(data.iloc[0][\"sequence\"]))#, len(data.iloc[0][\"seq\"]))\n",
    "print (\"sample sequence: \", data.iloc[0][\"sequence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhRJnF_25TgP"
   },
   "source": [
    "We are only really interested in the columns that do align. This means that for every column that we include, at least 50% of sequences are not gaps. Note that this threshold is imposed by the alignment parameters loaded above. So let's make a column for that. Meanwhile, we keep track of the indices that did align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79862,
     "status": "ok",
     "timestamp": 1555720437385,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "8TQDVR3_5TgQ",
    "outputId": "427b5e82-7697-4a29-b4d1-e129d95cbd5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PABP_YEAST/115-210</td>\n",
       "      <td>qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...</td>\n",
       "      <td>KGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...</td>\n",
       "      <td>PKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKDENGNSRGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625</td>\n",
       "      <td>..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...</td>\n",
       "      <td>PKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKDENGNSRGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398</td>\n",
       "      <td>........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...</td>\n",
       "      <td>IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVDSQGQSKGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112</td>\n",
       "      <td>........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...</td>\n",
       "      <td>---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRDIRRVSLGY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "0                                PABP_YEAST/115-210   \n",
       "1  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/203-294   \n",
       "2  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/534-625   \n",
       "3  ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/312-398   \n",
       "4   ur|UPI0004E53ABB|UniRef100_UPI0004E53ABB/33-112   \n",
       "\n",
       "                                            sequence  \\\n",
       "0  qrdpslrkKGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATD...   \n",
       "1  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKD...   \n",
       "2  ..epangsPKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKD...   \n",
       "3  ........IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVD...   \n",
       "4  ........---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRD...   \n",
       "\n",
       "                                                 seq  \n",
       "0  KGSGNIFIKNLHPDIDNKALYDTFSVFGDILSSKIATDENGKSKGF...  \n",
       "1  PKFFNVYVKNLPEKYTDDDLKSEFEAFGEITSAVVVKDENGNSRGF...  \n",
       "2  PKFFNVYVKNLPEKYTDDDLKSEFEASGEITSAVVVKDENGNSRGF...  \n",
       "3  IRGLNLYLKNLDDTIDDERLKELFRPFGTIISCKVMVDSQGQSKGS...  \n",
       "4  ---ASLYVGDLDLSVTEGQLFDLFSQIGPVASVRVCRDIRRVSLGY...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices=index_of_non_lower_case_dot(data.iloc[0][\"sequence\"])\n",
    "data[\"seq\"]=list(map(prune_seq,data[\"sequence\"]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMSU9Y_B5TgV"
   },
   "source": [
    "Let's see how many columns remained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79027,
     "status": "ok",
     "timestamp": 1555720437387,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "xleyPTqp5TgW",
    "outputId": "97b8cc07-f44d-4221-fea3-3a640dbe40b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned sequence length: 82\n"
     ]
    }
   ],
   "source": [
    "print (\"pruned sequence length:\", len(data.iloc[0][\"seq\"]))\n",
    "PRUNED_SEQ_LENGTH=len(data.iloc[0][\"seq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-_CtNus5Tgh"
   },
   "source": [
    "A few optional lines of code to run. Printing indices, and deleting the sequence column so that it doesn't stay in memory for no reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZMQTeww5Tgi"
   },
   "outputs": [],
   "source": [
    "#print (indices,len(indices))\n",
    "## you can do this to reduce memory load\n",
    "#del data[\"sequence\"]\n",
    "#data.head()\n",
    "#data.iloc[0][\"sequence\"][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wXDG2d75Tgm"
   },
   "source": [
    "Another useful preprocessing step is to reweight the sequences based on some similarity threshold. This helps us feed the network with more informative samples (namely discount sequences that are very close to each other). We weight all $n$ sequences that are more that 1-$\\theta$ similar to each other as $1/n$. Computing this step is rather slow, so I've provided the sample weights for $\\theta=0.2$, which is standard in the studies I reference. You can run the process for arbitrary $\\theta$ using the commented out line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 78261,
     "status": "ok",
     "timestamp": 1555720437390,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "SQvej3505Tgn",
    "outputId": "3754c6a5-f5ba-4124-d3e7-9bf1db3dcfee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, array([0.03125   , 0.2       , 0.2       , 0.2       , 0.16666667,\n",
       "        0.2       , 0.16666667, 0.16666667, 0.0625    , 1.        ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"PABP_YEAST_hmmerbit_t0.2_r50000.reweight\",\"rb\") as to_read:\n",
    "    new_weights=np.load(to_read)\n",
    "\n",
    "#new_weights=reweight_sequences(data[\"seq\"],0.1)\n",
    "len(new_weights),new_weights[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxh7GYjv5Tgs"
   },
   "source": [
    "Next we translate the sequence into a one hot encoding and shape the input sequences into a m*n matrix. Here m is the number of the data points and $n=$ alphbet size $\\times$ sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 100071,
     "status": "ok",
     "timestamp": 1555720461307,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "NaxZeTev5Tgt",
    "outputId": "04e1d35c-ab6b-47eb-a671-775579f17f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 24 82\n",
      "(50000, 1968)\n"
     ]
    }
   ],
   "source": [
    "#Encode training data in one_hot vectors\n",
    "training_data_one_hot=[]\n",
    "labels=[]\n",
    "for i, row in data.iterrows():\n",
    "    training_data_one_hot.append(translate_string_to_one_hot(row[\"seq\"],ORDER_LIST))\n",
    "print (len(training_data_one_hot),len(training_data_one_hot[0]),len(training_data_one_hot[0][0]))\n",
    "#plt.imshow(training_data_one_hot[0],cmap=\"Greys\")\n",
    "training_data=np.array([np.array(list(sample.T.flatten())) for sample in training_data_one_hot])\n",
    "# training_data=np.array([np.array(list(sample.flatten())).T for sample in training_data_one_hot])\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOG9Ak8o5Tgz"
   },
   "outputs": [],
   "source": [
    "### seq_len = 82, alphabet_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99732,
     "status": "ok",
     "timestamp": 1555720461316,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "xJHfNVZO5Tg6",
    "outputId": "4bfadc8a-9cf2-46ac-c3dc-b8e19feb180b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 18,  5, 18, 10, 22, 16, 22,  6, 10, 21,  9,  3, 12, 22, 12, 10,\n",
       "        6, 19, 21, 15, 12,  4, 16,  5, 20, 16, 18, 12, 22, 21,  5,  5,  6,\n",
       "       22, 19,  4, 12, 13, 10, 18,  6,  5,  6, 18, 16, 18, 16, 20,  9, 16,\n",
       "       13, 13, 13, 18, 19, 19,  6, 13, 19, 22, 12, 19, 21, 10, 18, 17, 21,\n",
       "       21, 10, 18, 11, 13, 22, 15, 20, 19,  3,  9, 21,  5,  7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(training_data_one_hot[0],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rk-qXJXw5ThB"
   },
   "source": [
    "That takes care of the training data. But we also need to test our model on something. Thankfully, some neat experiments have been done to actually measure the effects of change in a sequence on the performance of the protein. We load this data next (because we want to make use of this test data to evaluate our performance at the end of each epoch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99401,
     "status": "ok",
     "timestamp": 1555720461318,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "lf5YU8165ThC",
    "outputId": "9d673c48-02f5-43aa-a2a3-492cabd449b1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mutants:  1188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mutant                               K131V\n",
       "effect_prediction_epistatic         -10.06\n",
       "effect_prediction_independent     -3.36052\n",
       "linear                           0.0425856\n",
       "Name: 87, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_full=pd.read_csv(\n",
    "    \"PABP_YEAST_Fields2013-singles.csv\", sep=\";\", comment=\"#\"\n",
    ")\n",
    "print (\"number of mutants: \",len(exp_data_full))\n",
    "exp_data_full.head()\n",
    "exp_data_full.iloc[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4mt1u7g5ThG"
   },
   "source": [
    "The \"linear\" column is the experimental data. The other two predictions are the results as obtained by the method described in the [Hopf et al paper](https://www.nature.com/nbt/journal/v35/n2/abs/nbt.3769.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99072,
     "status": "ok",
     "timestamp": 1555720461320,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "wQvNpwOH5ThH",
    "outputId": "b4dd0c43-93bd-464a-e0d0-4b6e6bd3cebd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <th>linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.745161</td>\n",
       "      <td>0.592781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <td>0.745161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.592781</td>\n",
       "      <td>0.423680</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               effect_prediction_epistatic  \\\n",
       "effect_prediction_epistatic                       1.000000   \n",
       "effect_prediction_independent                     0.745161   \n",
       "linear                                            0.592781   \n",
       "\n",
       "                               effect_prediction_independent    linear  \n",
       "effect_prediction_epistatic                         0.745161  0.592781  \n",
       "effect_prediction_independent                       1.000000  0.423680  \n",
       "linear                                              0.423680  1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_full.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIaV44Wi5ThM"
   },
   "source": [
    "The experimentalists have made the mutations on specific positions on the sequence. We restrict our target data to the subset of those experiments that we have aligned columns for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 100683,
     "status": "ok",
     "timestamp": 1555720463251,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "diiLhyir5ThN",
    "outputId": "7818f016-5023-43f1-b65a-644a74b0114b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mutant</th>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <th>linear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>G126C</td>\n",
       "      <td>-5.663638</td>\n",
       "      <td>-0.027602</td>\n",
       "      <td>0.449027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>G126E</td>\n",
       "      <td>-6.611062</td>\n",
       "      <td>-1.827612</td>\n",
       "      <td>0.588928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>G126D</td>\n",
       "      <td>-7.270577</td>\n",
       "      <td>-1.180094</td>\n",
       "      <td>0.229853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>G126N</td>\n",
       "      <td>-5.809167</td>\n",
       "      <td>0.387443</td>\n",
       "      <td>0.679435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>G126S</td>\n",
       "      <td>-4.617248</td>\n",
       "      <td>0.661686</td>\n",
       "      <td>0.721788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>G126R</td>\n",
       "      <td>-5.582381</td>\n",
       "      <td>1.144148</td>\n",
       "      <td>0.313690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>G126W</td>\n",
       "      <td>-8.079901</td>\n",
       "      <td>-2.052391</td>\n",
       "      <td>0.226032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>G126V</td>\n",
       "      <td>-5.435631</td>\n",
       "      <td>-0.800307</td>\n",
       "      <td>0.230315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>N127A</td>\n",
       "      <td>-4.987206</td>\n",
       "      <td>-2.251823</td>\n",
       "      <td>0.062650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>N127C</td>\n",
       "      <td>-8.044181</td>\n",
       "      <td>-2.215639</td>\n",
       "      <td>0.024996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index mutant  effect_prediction_epistatic  effect_prediction_independent  \\\n",
       "0      1  G126C                    -5.663638                      -0.027602   \n",
       "1      2  G126E                    -6.611062                      -1.827612   \n",
       "2      3  G126D                    -7.270577                      -1.180094   \n",
       "3      4  G126N                    -5.809167                       0.387443   \n",
       "4      5  G126S                    -4.617248                       0.661686   \n",
       "5      6  G126R                    -5.582381                       1.144148   \n",
       "6      7  G126W                    -8.079901                      -2.052391   \n",
       "7      8  G126V                    -5.435631                      -0.800307   \n",
       "8      9  N127A                    -4.987206                      -2.251823   \n",
       "9     10  N127C                    -8.044181                      -2.215639   \n",
       "\n",
       "     linear  \n",
       "0  0.449027  \n",
       "1  0.588928  \n",
       "2  0.229853  \n",
       "3  0.679435  \n",
       "4  0.721788  \n",
       "5  0.313690  \n",
       "6  0.226032  \n",
       "7  0.230315  \n",
       "8  0.062650  \n",
       "9  0.024996  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OFFSET=117\n",
    "#Deciding offset requires investigating the dataset and alignment.\n",
    "exp_data_singles=pd.DataFrame(columns=exp_data_full.columns)\n",
    "#decide starting index depending on how the file is \"headered\"\n",
    "for i,row in exp_data_full[1:].iterrows():\n",
    "        pos=re.split(r'(\\d+)', row.mutant) \n",
    "        if int(pos[1])-OFFSET in indices:\n",
    "            exp_data_singles=exp_data_singles.append(row)\n",
    "exp_data_singles=exp_data_singles.reset_index()\n",
    "target_values_singles=list(exp_data_singles[\"linear\"])\n",
    "exp_data_singles.head(10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgS_02V-5ThR"
   },
   "source": [
    "Using this data, we can now make a library of sequences of single mutants, in the same form of the alignment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 100343,
     "status": "ok",
     "timestamp": 1555720463253,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "MzkrZvpc5ThS",
    "outputId": "cb0ed83f-f5e1-41ca-bbae-bee19fbb262e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187 1187\n",
      "[('K', 'K'), ('G', 'G'), ('S', 'S'), ('G', 'N'), ('N', 'N'), ('I', 'I'), ('F', 'F'), ('I', 'I'), ('K', 'K'), ('N', 'N')]\n"
     ]
    }
   ],
   "source": [
    "mutation_data=[re.split(r'(\\d+)', s) for s in exp_data_singles.mutant]\n",
    "wt_sequence=data.iloc[0].seq\n",
    "mutants=mutate_single(wt_sequence,mutation_data,offset=0,index=3) #note that you change index to 1\n",
    "\n",
    "#sanity checks\n",
    "print (len(mutants),len(exp_data_singles))\n",
    "#the mutant should be in the correct place\n",
    "print (list(zip(wt_sequence,mutants[3]))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLcwSC_55ThW"
   },
   "source": [
    "The last print above is a simple check to show that the mutation in the sequence is indeed an G to N (in the third mutant). We also make the one-hot vectors for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9wXTufC5ThW"
   },
   "outputs": [],
   "source": [
    "#Test data with wt at 0 index\n",
    "one_hot_mutants=[]\n",
    "mutants_plus=[data.iloc[0][\"seq\"]]+mutants\n",
    "for mutant in mutants_plus:\n",
    "    one_hot_mutants.append(translate_string_to_one_hot(\"\".join(mutant),ORDER_LIST))\n",
    "\n",
    "test_data_plus=np.array([np.array(list(sample.flatten())).T for sample in one_hot_mutants])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yoXxSfkw5Thb"
   },
   "source": [
    "We also construct the double mutant sequences using the same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99864,
     "status": "ok",
     "timestamp": 1555720463695,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "z_xJcnTc5Thc",
    "outputId": "408640a8-0099-4b37-b0a9-aa7b0da44b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mutants:  36522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mutant                           G169W,F170V\n",
       "effect_prediction_epistatic          -18.161\n",
       "effect_prediction_independent       -15.1525\n",
       "XY_Enrichment_score                  0.05916\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_full=pd.read_csv(\n",
    "    \"PABP_YEAST_Fields2013-doubles.csv\", sep=\";\", comment=\"#\"\n",
    ")\n",
    "print (\"number of mutants: \",len(exp_data_full))\n",
    "exp_data_full.head()\n",
    "exp_data_full.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99709,
     "status": "ok",
     "timestamp": 1555720463697,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "4zQ8U9FN5Thg",
    "outputId": "b3864c3c-fcdd-4351-9d24-203ed860bcf8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <th>XY_Enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715371</td>\n",
       "      <td>0.620022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <td>0.715371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.497743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XY_Enrichment_score</th>\n",
       "      <td>0.620022</td>\n",
       "      <td>0.497743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               effect_prediction_epistatic  \\\n",
       "effect_prediction_epistatic                       1.000000   \n",
       "effect_prediction_independent                     0.715371   \n",
       "XY_Enrichment_score                               0.620022   \n",
       "\n",
       "                               effect_prediction_independent  \\\n",
       "effect_prediction_epistatic                         0.715371   \n",
       "effect_prediction_independent                       1.000000   \n",
       "XY_Enrichment_score                                 0.497743   \n",
       "\n",
       "                               XY_Enrichment_score  \n",
       "effect_prediction_epistatic               0.620022  \n",
       "effect_prediction_independent             0.497743  \n",
       "XY_Enrichment_score                       1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_full.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 124884,
     "status": "ok",
     "timestamp": 1555720489059,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "XhaOZuAQ5Thl",
    "outputId": "bd7f06d6-8355-4c89-9e44-11a4085f9f26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mutant</th>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <th>XY_Enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>G169W,F170V</td>\n",
       "      <td>-18.161003</td>\n",
       "      <td>-15.152495</td>\n",
       "      <td>0.059160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>G169V,F170V</td>\n",
       "      <td>-13.753099</td>\n",
       "      <td>-8.845704</td>\n",
       "      <td>0.045765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>G169A,F170I</td>\n",
       "      <td>-9.115749</td>\n",
       "      <td>-3.954389</td>\n",
       "      <td>0.075799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>G169C,F170Y</td>\n",
       "      <td>-9.761077</td>\n",
       "      <td>-4.004533</td>\n",
       "      <td>0.700485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>G169A,F170S</td>\n",
       "      <td>-10.212144</td>\n",
       "      <td>-5.743002</td>\n",
       "      <td>0.061518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       mutant  effect_prediction_epistatic  \\\n",
       "0      0  G169W,F170V                   -18.161003   \n",
       "1      1  G169V,F170V                   -13.753099   \n",
       "2      2  G169A,F170I                    -9.115749   \n",
       "3      3  G169C,F170Y                    -9.761077   \n",
       "4      4  G169A,F170S                   -10.212144   \n",
       "\n",
       "   effect_prediction_independent  XY_Enrichment_score  \n",
       "0                     -15.152495             0.059160  \n",
       "1                      -8.845704             0.045765  \n",
       "2                      -3.954389             0.075799  \n",
       "3                      -4.004533             0.700485  \n",
       "4                      -5.743002             0.061518  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OFFSET=160\n",
    "#Deciding offset requires investigating the dataset and alignment.\n",
    "exp_data_doubles=pd.DataFrame(columns=exp_data_full.columns)\n",
    "#decide starting index depending on how the file is \"headered\"\n",
    "for i,row in exp_data_full[0:].iterrows():\n",
    "        pos=re.split(r'(\\d+)', row.mutant)\n",
    "        if int(pos[1])-OFFSET in indices and int(pos[3])-OFFSET in indices:\n",
    "            exp_data_doubles=exp_data_doubles.append(row)\n",
    "exp_data_doubles=exp_data_doubles.reset_index()\n",
    "exp_data_doubles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 124741,
     "status": "ok",
     "timestamp": 1555720489062,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "mg5WNyn35Thp",
    "outputId": "d443476b-f23f-4d07-f3e3-75048490a7a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <th>XY_Enrichment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143941</td>\n",
       "      <td>0.140272</td>\n",
       "      <td>0.117997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect_prediction_epistatic</th>\n",
       "      <td>0.143941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773236</td>\n",
       "      <td>0.520277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect_prediction_independent</th>\n",
       "      <td>0.140272</td>\n",
       "      <td>0.773236</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.488397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XY_Enrichment_score</th>\n",
       "      <td>0.117997</td>\n",
       "      <td>0.520277</td>\n",
       "      <td>0.488397</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  index  effect_prediction_epistatic  \\\n",
       "index                          1.000000                     0.143941   \n",
       "effect_prediction_epistatic    0.143941                     1.000000   \n",
       "effect_prediction_independent  0.140272                     0.773236   \n",
       "XY_Enrichment_score            0.117997                     0.520277   \n",
       "\n",
       "                               effect_prediction_independent  \\\n",
       "index                                               0.140272   \n",
       "effect_prediction_epistatic                         0.773236   \n",
       "effect_prediction_independent                       1.000000   \n",
       "XY_Enrichment_score                                 0.488397   \n",
       "\n",
       "                               XY_Enrichment_score  \n",
       "index                                     0.117997  \n",
       "effect_prediction_epistatic               0.520277  \n",
       "effect_prediction_independent             0.488397  \n",
       "XY_Enrichment_score                       1.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_values_doubles=list(exp_data_doubles[\"XY_Enrichment_score\"])\n",
    "exp_data_doubles.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 124792,
     "status": "ok",
     "timestamp": 1555720489309,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "aEXaWBXc5Tht",
    "outputId": "73974744-7800-425c-f526-615c6791ca97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13876 13876\n",
      "[('G', 'G'), ('K', 'K'), ('S', 'S'), ('K', 'K'), ('G', 'G'), ('F', 'F'), ('G', 'A'), ('F', 'I'), ('V', 'V'), ('H', 'H')]\n"
     ]
    }
   ],
   "source": [
    "mutation_data1=[re.split(r'(\\d+)', s.split(\",\")[0]) for s in exp_data_doubles.mutant]\n",
    "mutation_data2=[re.split(r'(\\d+)', s.split(\",\")[1]) for s in exp_data_doubles.mutant]\n",
    "wt_sequence=data.iloc[0].seq\n",
    "\n",
    "mutants_double=mutate_double(wt_sequence,mutation_data1,mutation_data2,offset=0,index=46)\n",
    "\n",
    "#sanity checks\n",
    "print (len(mutants_double),len(exp_data_doubles))\n",
    "#the mutant should be in the correct place\n",
    "print (list(zip(wt_sequence,mutants_double[2]))[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8FeMmSL5Thy"
   },
   "source": [
    "Again, we can check that the second mutant is indeed a double G169A, F170I mutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHKCTpnB5Th1"
   },
   "outputs": [],
   "source": [
    "#Test data with wt at 0 index\n",
    "one_hot_mutants=[]\n",
    "mutants_plus=[data.iloc[0][\"seq\"]]+mutants_double\n",
    "for mutant in mutants_plus:\n",
    "    one_hot_mutants.append(translate_string_to_one_hot(\"\".join(mutant),ORDER_LIST))\n",
    "\n",
    "test_data_doubles_plus=np.array([np.array(list(sample.flatten())).T for sample in one_hot_mutants])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqP5-j1e5Th5"
   },
   "source": [
    "We can combine the singles and doubles training data for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HyN5vpv5Th8"
   },
   "outputs": [],
   "source": [
    "all_test_data=np.vstack([test_data_plus,test_data_doubles_plus[1:]])\n",
    "all_test_data_flattened=np.array([np.array(list(sample.flatten())).T for sample in all_test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpqzC_Zm5TiD"
   },
   "source": [
    "This concludes the pre-processing we need to do on the data.\n",
    "\n",
    "## 2.  Training the model\n",
    "We now move on to define our neural network. This is essentially a vanilla VAE in keras (with some optimization on hyperparameters). For optimization purposes we define a callback function that reports the predictive power of the model in the end of each epoch. Note that while this passes the -test data- through the model, it is kosher because we never pass in the values we are actually interested in and the network is not in \"training phase\", i.e. no weights are updated during this pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZhvKWQo5TiH"
   },
   "outputs": [],
   "source": [
    "class rho_vs_mutants():\n",
    "    def __init__(self,mutants,test_set_size,aa_size,sequence_size):\n",
    "        self.mutants=mutants\n",
    "        self.sample_size=test_set_size\n",
    "        self.aa_size=aa_size\n",
    "        self.sequence_size=sequence_size\n",
    "        self.scores=[]\n",
    "        self.count_batch=0\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "    #This allows us to track the \"progress\" of the model on different epochs\n",
    "    def on_epoch_end(self,model,batch,logs):\n",
    "        x_decoded=model(test_data_plus[0:self.sample_size],batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(self.aa_size,self.sequence_size)\n",
    "        digit_wt = normalize(digit,axis=0, norm='l1')\n",
    "        wt_prob=compute_log_probability(digit,digit_wt)\n",
    "        fitnesses=[]\n",
    "        for sample in range(1,self.sample_size):\n",
    "            digit = x_decoded[sample].reshape(self.aa_size,self.sequence_size)\n",
    "            digit = normalize(digit,axis=0, norm='l1')\n",
    "            fitness=compute_log_probability(test_data_plus[sample].reshape(self.aa_size,self.sequence_size),digit)-wt_prob\n",
    "            fitnesses.append(fitness)\n",
    "        print (\",\"+str(spearmanr(fitnesses,target_values_singles[:self.sample_size-1])))\n",
    "        self.scores.append(spearmanr(fitnesses,target_values_singles[:self.sample_size-1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDdXA9285TiK"
   },
   "source": [
    "Now we are ready to specify the network architecture, this is adapted from [here](https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbKOIiIC5TiL"
   },
   "outputs": [],
   "source": [
    "# torch.sum(1 + model.z_log_var - (model.z_mean)**2 - torch.exp(model.z_log_var),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYnPNz4m5TiN"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "original_dim=len(ORDER_LIST)*PRUNED_SEQ_LENGTH\n",
    "output_dim=len(ORDER_LIST)*PRUNED_SEQ_LENGTH\n",
    "latent_dim = 24\n",
    "intermediate_dim = 250\n",
    "nb_epoch = 6\n",
    "epsilon_std = 1.0\n",
    "np.random.seed(42)\n",
    "lang_mod = True\n",
    "cuda = True\n",
    "\n",
    "def create_tensor(x,gpu=False):\n",
    "    if gpu:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "\n",
    "def vae_loss(x_true, x_decoded_mean, z_mean, z_log_var):\n",
    "    xent_loss = original_dim * loss1(x_decoded_mean, x_true)\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_var - (z_mean)**2 - torch.exp(z_log_var))\n",
    "    return (xent_loss + kl_loss), xent_loss, kl_loss\n",
    "\n",
    "# #Encoding Layers\n",
    "# x = Input(batch_shape=(batch_size, original_dim))\n",
    "# h = Dense(intermediate_dim,activation=\"elu\")(x)\n",
    "# h= Dropout(0.7)(h)\n",
    "# h = Dense(intermediate_dim, activation='elu')(h)\n",
    "# h=BatchNormalization(mode=0)(h)\n",
    "# h = Dense(intermediate_dim, activation='elu')(h)\n",
    "\n",
    "# #Latent layers\n",
    "# z_mean=Dense(latent_dim)(h)\n",
    "# z_log_var=Dense(latent_dim)(h)\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# #Decoding layers \n",
    "\n",
    "# decoder_1= Dense(intermediate_dim, activation='elu')\n",
    "# decoder_2=Dense(intermediate_dim, activation='elu')\n",
    "# decoder_2d=Dropout(0.7)\n",
    "# decoder_3=Dense(intermediate_dim, activation='elu')\n",
    "# decoder_out=Dense(output_dim, activation='sigmoid')\n",
    "# x_decoded_mean = decoder_out(decoder_3(decoder_2d(decoder_2(decoder_1(z)))))\n",
    "\n",
    "# vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# #Potentially better results, but requires further hyperparameter tuning\n",
    "# #optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.001, decay=0.0, nesterov=False,clipvalue=0.05)\n",
    "# vae.compile(optimizer=\"adam\", loss=vae_loss,metrics=[\"categorical_accuracy\",\"fmeasure\",\"top_k_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSh4KAZ35TiR"
   },
   "source": [
    "And run it through our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4rQYart5TiR"
   },
   "outputs": [],
   "source": [
    "x_train=training_data[:data_set_size] #this needs to be divisible by batch size and less than or equal to dataset size\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "spearman_measure=rho_vs_mutants(test_data_plus,batch_size*int(len(test_data_plus)/batch_size),len(ORDER_LIST),PRUNED_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IgcFAvI5TiU"
   },
   "outputs": [],
   "source": [
    "vae_type = 'rec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Sw-yKnOvhep"
   },
   "outputs": [],
   "source": [
    "def conv_size_func(Lin,dilation,kernel,padding=0,stride=1):\n",
    "  return int(((Lin+2*padding-dilation*(kernel-1)-1)/stride)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-5mc0eC5Tif"
   },
   "outputs": [],
   "source": [
    "if vae_type == 'full':\n",
    "    print (\"training on full\")\n",
    "    univ_dropout = [0.2]*3\n",
    "    dropout_enc = univ_dropout\n",
    "    dropout_dec = univ_dropout\n",
    "\n",
    "    layers_enc = nn.ModuleList([nn.Linear(original_dim,intermediate_dim),nn.Dropout(dropout_enc[0]),nn.ELU()])\n",
    "    for i in range(2):\n",
    "        layers_enc.append(nn.Linear(intermediate_dim,intermediate_dim))\n",
    "        layers_enc.append(nn.Dropout(dropout_enc[i+1]))\n",
    "        layers_enc.append(nn.ELU())\n",
    "\n",
    "    layers_dec = nn.ModuleList([nn.Linear(latent_dim,intermediate_dim),nn.Dropout(dropout_dec[0]),nn.ELU()])\n",
    "    for i in range(2):\n",
    "        layers_dec.append(nn.Linear(intermediate_dim,intermediate_dim))\n",
    "        layers_dec.append(nn.Dropout(dropout_dec[i+1]))\n",
    "        layers_dec.append(nn.ELU())\n",
    "\n",
    "    layers_dec.append(nn.Linear(intermediate_dim,output_dim))\n",
    "\n",
    "    layers_ae = nn.ModuleList([nn.Linear(intermediate_dim,latent_dim),nn.Linear(intermediate_dim,latent_dim)])\n",
    "elif vae_type == 'conv':\n",
    "    out_conv_enc = [50,100]\n",
    "    kernels_enc = [3,5]\n",
    "    dilations_enc = [1,3]\n",
    "    maxpools_enc = [4,3]\n",
    "    paddings_enc = [(5,5,0,0)]\n",
    "    \n",
    "    out_lin_enc = [100,500]\n",
    "    dropout_enc = [0.2,0.2]\n",
    "    \n",
    "    out_lin_dec = [100,150]\n",
    "    dropout_dec = [0.2,0.2]\n",
    "    \n",
    "    layers_enc_pre_view = nn.ModuleList([nn.Conv1d(len(ORDER_LIST),out_conv_enc[0],kernels_enc[0],stride=1,dilation=dilations_enc[0]),\n",
    "                                nn.ELU(),\n",
    "                                nn.MaxPool1d(maxpools_enc[0],padding=0),\n",
    "                                nn.ZeroPad2d(paddings_enc[0]),\n",
    "                                nn.Conv1d(out_conv_enc[0],out_conv_enc[1],kernels_enc[1],stride=1,dilation=dilations_enc[1]),\n",
    "                                nn.ELU(),\n",
    "#                                 nn.MaxPool1d(4,padding=0),\n",
    "#                                 nn.ZeroPad2d((5,5,0,0)),\n",
    "#                                 nn.Conv1d(out_conv_enc[1],out_conv_enc[2],kernels_enc[2],stride=1,dilation=dilations_enc[2]),\n",
    "#                                 nn.ELU(),\n",
    "                                nn.MaxPool1d(maxpools_enc[1],padding=0)])\n",
    "    \n",
    "    inp_len = PRUNED_SEQ_LENGTH\n",
    "    paddings_enc.append((0,0,0,0))\n",
    "    for i in range(len(out_conv_enc)):\n",
    "      inp_len = conv_size_func(inp_len,dilations_enc[i],kernels_enc[i])\n",
    "      inp_len = inp_len//maxpools_enc[i]\n",
    "      inp_len += (paddings_enc[i][0]+paddings_enc[i][1])\n",
    "    \n",
    "    enc_view = inp_len*out_conv_enc[-1]\n",
    "    print('post-convolutional size is ', enc_view)\n",
    "    \n",
    "    layers_enc_post_view = nn.ModuleList([nn.Linear(enc_view,out_lin_enc[0]),\n",
    "                                          nn.Dropout(dropout_enc[0]),\n",
    "                                          nn.ELU(),\n",
    "                                          nn.Linear(out_lin_enc[0],out_lin_enc[1]),\n",
    "                                          nn.Dropout(dropout_enc[1]),\n",
    "                                          nn.ELU()])\n",
    "    \n",
    "    layers_dec = nn.ModuleList([nn.Linear(latent_dim,out_lin_dec[0]),\n",
    "                                nn.Dropout(dropout_dec[0]),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(out_lin_dec[0],out_lin_dec[1]),\n",
    "                                nn.Dropout(dropout_dec[1]),\n",
    "                                nn.ELU(),\n",
    "                                nn.Linear(out_lin_dec[1],output_dim)])\n",
    "    \n",
    "    layers_ae = nn.ModuleList([nn.Linear(out_lin_enc[-1],latent_dim),nn.Linear(out_lin_enc[-1],latent_dim)])\n",
    "elif vae_type == 'rec':\n",
    "    univ_dropout = [0.2]*2\n",
    "    dropout_enc = univ_dropout\n",
    "    dropout_dec = univ_dropout\n",
    "    hid_size = [20,10]\n",
    "    dec_lin = False\n",
    "    \n",
    "    num_layers = 2\n",
    "    num_layers_dec = 2\n",
    "    bid = True\n",
    "    num_dirs = 2 if bid else 1\n",
    "    \n",
    "   \n",
    "    layers_enc = nn.ModuleList([nn.RNN(len(ORDER_LIST),hid_size[0],num_layers=num_layers,batch_first=True,dropout=univ_dropout[0],bidirectional=bid)])\n",
    "\n",
    "\n",
    "    if dec_lin:\n",
    "      layers_post_rec_enc = nn.ModuleList([nn.Linear(164,intermediate_dim),\n",
    "                                         nn.Dropout(dropout_enc[0]),\n",
    "                                         nn.ELU(),\n",
    "                                         nn.Linear(intermediate_dim,intermediate_dim),\n",
    "                                         nn.Dropout(dropout_enc[1]),\n",
    "                                         nn.ELU()]) # for now, not being used in rec model\n",
    "\n",
    "\n",
    "  #     layers_pre_rec_dec = nn.ModuleList([nn.Linear(latent_dim,100),\n",
    "  #                                         nn.Dropout(dropout_dec[0]),\n",
    "  #                                         nn.ELU()])\n",
    "  #     # 25 below bc bidirectional 2 layers means we have to divide 100 by 2*2\n",
    "  #     layers_dec = nn.ModuleList([nn.RNN(50,25,num_layers=2,batch_first=True,dropout=0.2,bidirectional=True)])\n",
    "  #     layers_post_rec_dec = nn.ModuleList([nn.Linear(25*2,len(ORDER_LIST))])\n",
    "\n",
    "  #     layers_ae = nn.ModuleList([nn.Linear(intermediate_dim,latent_dim),nn.Linear(intermediate_dim,latent_dim)])\n",
    "      layers_dec = nn.ModuleList([nn.Linear(latent_dim,intermediate_dim),\n",
    "                                  nn.Dropout(.2),\n",
    "                                  nn.ELU(),\n",
    "                                  nn.Linear(intermediate_dim,intermediate_dim*2),\n",
    "                                  nn.Dropout(.2),\n",
    "                                  nn.ELU(),\n",
    "                                  nn.Linear(intermediate_dim*2,output_dim)])\n",
    "      \n",
    "      layers_dec_post_rec = 0\n",
    "    \n",
    "      layers_ae = nn.ModuleList([nn.Linear(intermediate_dim,latent_dim),nn.Linear(intermediate_dim,latent_dim)])\n",
    "    \n",
    "    else: # dec_lin = False\n",
    "      layers_post_rec_enc = 0\n",
    "      \n",
    "      layers_dec = nn.ModuleList([nn.Linear(latent_dim,hid_size[1]),nn.RNN(len(ORDER_LIST),hid_size[1],num_layers=num_layers_dec,batch_first=True,dropout=univ_dropout[1],bidirectional=bid)])\n",
    "      \n",
    "      layers_dec_post_rec = nn.ModuleList([nn.Linear(hid_size[1]*num_dirs,len(ORDER_LIST))])\n",
    "      \n",
    "      layers_ae = nn.ModuleList([nn.Linear(hid_size[0],latent_dim),nn.Linear(hid_size[0],latent_dim)])\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5056,
     "status": "error",
     "timestamp": 1555721543245,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "d9wNJVdR5Tij",
    "outputId": "52260c7e-ca9c-411a-983d-3acbffc1a3f2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec\n",
      "language model training\n",
      "FAKE TRAINING SET TO ASSESS REC VALIDITY\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Pre-training | Training Loss: 25612.37890625, Training Accuracy: 0.0, Validation Loss: 24505.03125, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 4050.7898, -3612.0278, -1981.7257, -1999.9246,  3346.8716, -4452.5225,\n",
      "        -1861.5726,   -19.4124,   992.1812,   394.4849], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          13674.5703,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -18374.4336,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           9826.1094,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -7739.7559,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          23627.7148,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          15665.5586,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           8694.3418,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -21766.3086,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           9309.8057,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "            993.1493,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[ -5591.6147,   2600.9604,  -9579.4102,  12865.3535, -11475.1104,\n",
      "          -4576.7456,  -9794.5830,   9924.7334,  -5334.9766, -10005.5303],\n",
      "        [  7336.7563,  -3849.9834,  13104.3477, -17040.2012,  15367.2061,\n",
      "           6371.8970,  13148.5127, -13656.7285,   7131.6040,  13729.5068],\n",
      "        [ -3914.9038,   1974.6914,  -6970.1387,   9118.7178,  -8198.9824,\n",
      "          -3356.3953,  -7015.5771,   7238.8584,  -3797.5652,  -7275.8638],\n",
      "        [  3442.3909,  -1308.6768,   5237.3369,  -7616.0024,   6652.2842,\n",
      "           2488.9863,   5625.5522,  -5428.3003,   3144.5217,   5526.4077],\n",
      "        [ -9904.1152,   4917.7563, -16671.3809,  22409.9316, -20048.1406,\n",
      "          -8157.6709, -17077.7012,  17422.8008,  -9404.1299, -17617.2207],\n",
      "        [ -6453.6094,   3589.2937, -11266.6553,  14657.1475, -13256.5127,\n",
      "          -5599.2700, -11324.6035,  11826.5000,  -6212.7754, -11944.3486],\n",
      "        [ -3624.6099,   1716.4154,  -6094.4556,   8245.2939,  -7352.2368,\n",
      "          -2947.6365,  -6263.5356,   6344.1396,  -3438.5757,  -6409.3096],\n",
      "        [  9334.3984,  -4621.3716,  15345.9854, -20820.0684,  18592.5059,\n",
      "           7547.3901,  15817.4912, -16059.9492,   8767.5918,  16297.1650],\n",
      "        [ -3934.2251,   1934.5815,  -6551.0493,   8871.6475,  -7924.8218,\n",
      "          -3216.3237,  -6742.2109,   6859.5371,  -3724.2141,  -6937.6133],\n",
      "        [  -444.2320,   -112.2352,   -531.9186,   1036.7025,   -814.8856,\n",
      "           -151.4707,   -682.0477,    477.0759,   -369.1634,   -487.6617]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([ 15555.2148, -19530.9863,  10621.1787, -10043.4219,  26589.7031,\n",
      "         16469.7480,   9953.0967, -24857.0586,  10588.9648,   1980.1346],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([ 15555.2148, -19530.9863,  10621.1787, -10043.4219,  26589.7031,\n",
      "         16469.7480,   9953.0967, -24857.0586,  10588.9648,   1980.1346],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          26467.7559,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -1050.0482,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          17871.6289,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          23931.7285,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -39687.8281,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -14410.6982,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -7191.6011,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           9826.9316,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "            -79.5214,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -19000.3105,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[-18771.4180,  15624.9297,  -9005.4150, -14431.0195,   3908.3616,\n",
      "          20113.3613,  25051.3301, -11736.9600, -18432.4551,  17575.4551],\n",
      "        [  1812.2491,  -1633.1334,    965.2402,   1585.4573,   -459.8944,\n",
      "          -2020.2113,  -2589.8767,   1143.3425,   1829.3752,  -1760.9783],\n",
      "        [-12039.8955,   9515.7207,  -5279.5908,  -8425.3906,   2225.7410,\n",
      "          12510.9434,  15373.8438,  -7505.8345, -11560.0781,  11007.5986],\n",
      "        [-16215.0449,  13012.4365,  -7289.0747, -11665.1025,   3129.8914,\n",
      "          16994.0059,  20980.0352, -10128.6240, -15660.5801,  14929.1484],\n",
      "        [ 27764.2344, -23289.9688,  13509.0967,  21644.4844,  -5868.4634,\n",
      "         -29895.2891, -37296.8242,  17362.9785,  27363.7383, -26089.1074],\n",
      "        [  9502.5996,  -7472.8662,   4163.2393,   6602.7212,  -1704.8029,\n",
      "          -9864.9463, -12077.9248,   5907.6372,   9125.4326,  -8668.3662],\n",
      "        [  4524.8794,  -3406.5959,   1806.3250,   2884.4861,   -766.3288,\n",
      "          -4566.6807,  -5550.8428,   2825.7463,   4247.5547,  -4049.5867],\n",
      "        [ -6287.8667,   4979.9771,  -2809.3394,  -4434.8022,   1124.8783,\n",
      "           6565.8257,   8036.0811,  -3899.9753,  -6069.5483,   5753.8037],\n",
      "        [   686.3494,   -944.6743,    661.5115,   1124.6339,   -372.0152,\n",
      "          -1001.2590,  -1429.6813,    454.9189,    847.4623,   -836.9593],\n",
      "        [ 12670.6699, -10114.7764,   5678.4614,   9043.0674,  -2388.5554,\n",
      "         -13259.8242, -16317.4121,   7900.5166,  12230.7178, -11638.3047]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([ 27726.5781,  -2436.6504,  18819.0000,  24935.3320, -40640.5703,\n",
      "        -14924.1816,  -7408.7002,   9804.2920,   -265.3317, -19583.8652],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([ 27726.5781,  -2436.6504,  18819.0000,  24935.3320, -40640.5703,\n",
      "        -14924.1816,  -7408.7002,   9804.2920,   -265.3317, -19583.8652],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[ -1428.9121,    327.7817,  -3631.6233,   4595.4321,  -3354.0278,\n",
      "          -1325.9855,  -3437.2585,   3214.2913,  -1643.6924,  -3631.3555,\n",
      "          -3307.6370,   2655.9775,  -1939.8983,  -3145.1323,    992.8638,\n",
      "           3677.5774,   4226.2026,  -2623.8809,  -3515.2566,   3429.9934],\n",
      "        [  -827.5121,    539.8724,  -1667.7476,   1943.4036,  -1909.2417,\n",
      "           -810.4404,  -1445.9591,   1517.2792,   -783.0200,  -1810.3801,\n",
      "          -1366.2007,   1288.2944,   -744.5267,  -1120.7068,    316.3065,\n",
      "           1500.1443,   1824.4033,  -1033.7206,  -1512.8428,   1284.1464],\n",
      "        [  3569.3796,  -2016.0859,   6010.5415,  -8780.0605,   6503.4692,\n",
      "           3273.0857,   6583.5083,  -6421.2300,   3420.2214,   7555.2183,\n",
      "           6032.2085,  -5741.3164,   3037.8501,   4617.5396,  -1473.2670,\n",
      "          -7386.5928,  -7278.1714,   4014.2302,   6580.0015,  -6090.3618],\n",
      "        [   850.2027,   -296.7118,   1962.9363,  -2714.8779,   2208.5081,\n",
      "            932.3484,   2264.2517,  -2147.4866,    952.2508,   2249.6128,\n",
      "           2273.4932,  -1733.2881,   1078.5157,   1641.3188,   -624.5474,\n",
      "          -2271.6443,  -2651.7056,   1410.0526,   2274.5684,  -1850.9696],\n",
      "        [ -3424.6501,   1906.1981,  -6510.9932,   8441.3330,  -7871.0200,\n",
      "          -2979.5085,  -5562.6436,   6468.5547,  -3265.0493,  -6162.2593,\n",
      "          -5618.5239,   5285.6440,  -3082.8567,  -5118.4976,   1569.3380,\n",
      "           7310.7993,   9019.9961,  -4197.1724,  -5535.5015,   5487.8862],\n",
      "        [ 16952.5996, -10466.6572,  34509.0469, -36108.4531,  33819.3906,\n",
      "          15478.8643,  33669.2812, -31061.6191,  17764.6973,  35595.4961,\n",
      "          29159.1445, -24759.0449,  14980.4307,  24845.1387,  -6810.9727,\n",
      "         -33625.9062, -41774.4609,  19518.0879,  31701.3672, -27008.3281],\n",
      "        [ -2148.6750,    740.8066,  -5216.1040,   5739.9585,  -6223.7100,\n",
      "          -2275.9302,  -4850.4253,   4481.0591,  -2430.5889,  -5221.9014,\n",
      "          -4776.9819,   4146.3062,  -2910.8262,  -4304.6606,   1405.5468,\n",
      "           5277.8911,   7341.2002,  -3645.0034,  -5177.3735,   4698.2876],\n",
      "        [ -2573.2261,   1395.5156,  -5451.4468,   6476.6689,  -5941.8887,\n",
      "          -2604.3215,  -5555.5640,   5860.0161,  -2661.3188,  -4788.0342,\n",
      "          -5628.9253,   3912.5444,  -2660.2520,  -3960.2578,   1353.4130,\n",
      "           5094.4028,   6514.2017,  -3404.5281,  -5223.1133,   5281.3105],\n",
      "        [  5843.1562,  -3306.2998,  10211.2637, -14018.7158,  11490.1826,\n",
      "           5106.6489,  10542.4629, -10720.6182,   5689.9038,  11123.2168,\n",
      "           9694.6523,  -8326.4033,   4788.2388,   7999.1987,  -2281.5151,\n",
      "         -10939.5508, -14053.9854,   6143.2271,  10201.6777,  -9705.2998],\n",
      "        [ -1923.5515,    591.0616,  -4593.2012,   4992.3901,  -4600.0767,\n",
      "          -1892.9540,  -4401.3760,   3862.5125,  -2017.5223,  -3828.6436,\n",
      "          -4369.8501,   3266.6694,  -2243.9631,  -3702.8601,   1210.2043,\n",
      "           4752.8521,   5402.3032,  -2962.7422,  -4384.8857,   4071.9871]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[-4.4943e+03,  1.7494e+03,  4.2427e+03,  4.5426e+03, -4.2671e+03,\n",
      "          3.3618e+03, -4.2370e+03, -4.3060e+03, -2.0446e+02, -4.0348e+03],\n",
      "        [-2.0988e+03,  9.4850e+02,  2.0601e+03,  2.1001e+03, -2.0628e+03,\n",
      "          1.6749e+03, -2.0933e+03, -2.0893e+03,  5.9642e+01, -2.1069e+03],\n",
      "        [ 8.9864e+03, -4.0001e+03, -8.7254e+03, -9.0130e+03,  8.7562e+03,\n",
      "         -7.0660e+03,  8.8542e+03,  8.8726e+03,  1.9468e+02,  8.8047e+03],\n",
      "        [ 2.7701e+03, -1.0893e+03, -2.6246e+03, -2.7976e+03,  2.6387e+03,\n",
      "         -2.0929e+03,  2.6261e+03,  2.6648e+03,  1.5926e+02,  2.5151e+03],\n",
      "        [-8.8874e+03,  3.8413e+03,  8.6260e+03,  8.9178e+03, -8.6422e+03,\n",
      "          7.0531e+03, -8.7264e+03, -8.7507e+03,  1.3220e+02, -8.6522e+03],\n",
      "        [ 4.3539e+04, -1.9689e+04, -4.2447e+04, -4.3627e+04,  4.2540e+04,\n",
      "         -3.4343e+04,  4.3084e+04,  4.3106e+04, -5.8583e+02,  4.3032e+04],\n",
      "        [-6.8954e+03,  2.7643e+03,  6.5876e+03,  6.9523e+03, -6.6106e+03,\n",
      "          5.3129e+03, -6.6020e+03, -6.6807e+03, -1.5752e+02, -6.3923e+03],\n",
      "        [-7.3024e+03,  3.0825e+03,  7.0541e+03,  7.3375e+03, -7.0767e+03,\n",
      "          5.7646e+03, -7.1256e+03, -7.1629e+03, -1.7479e+02, -7.0225e+03],\n",
      "        [ 1.4200e+04, -6.3159e+03, -1.3852e+04, -1.4226e+04,  1.3884e+04,\n",
      "         -1.1310e+04,  1.4070e+04,  1.4070e+04,  4.2532e+01,  1.4065e+04],\n",
      "        [-5.5675e+03,  2.1973e+03,  5.2879e+03,  5.6188e+03, -5.3159e+03,\n",
      "          4.2313e+03, -5.2992e+03, -5.3696e+03, -3.6097e+02, -5.0949e+03]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([  5498.4150,   2079.8843,  -9385.6143,  -3328.6782,   9397.9365,\n",
      "        -44632.1914,   7998.6367,   7918.1807, -14491.0088,   6602.6582],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([  5498.4150,   2079.8843,  -9385.6143,  -3328.6782,   9397.9365,\n",
      "        -44632.1914,   7998.6367,   7918.1807, -14491.0088,   6602.6582],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[-19828.4141,  11163.2021, -36387.8281,  44391.9453, -44016.4844,\n",
      "         -19333.5684, -37199.0430,  40230.0977, -20457.2012, -37768.8320,\n",
      "         -37527.2188,  32052.3652, -16229.9238, -26597.9648,   8918.7705,\n",
      "          35629.4688,  39487.7305, -24111.8984, -36408.7891,  29839.8789],\n",
      "        [ 16864.6660,  -9397.7695,  26014.6309, -35590.7031,  30777.5137,\n",
      "          14355.0547,  30046.5137, -29531.1230,  15556.3555,  31833.0371,\n",
      "          29070.3828, -23630.1641,  12227.5186,  20143.9316,  -6638.2490,\n",
      "         -26990.4805, -35547.6914,  16142.1025,  26169.7520, -23219.9902],\n",
      "        [  2672.7795,  -1450.1412,   4697.6509,  -5826.2954,   5430.8428,\n",
      "           2658.5618,   4586.8291,  -5410.4429,   2865.7534,   4814.0840,\n",
      "           4982.5312,  -4081.6960,   1847.6511,   3574.9109,  -1501.7582,\n",
      "          -4728.1143,  -7028.7759,   3113.8276,   4144.7476,  -4759.4341],\n",
      "        [ -6138.3525,   3454.2173, -10240.3662,  12466.5762, -11103.7178,\n",
      "          -5836.9702, -12308.8057,  11366.3525,  -5969.2490, -12536.6650,\n",
      "         -10657.3496,   9332.0596,  -5108.8516,  -9197.5303,   2902.3167,\n",
      "          11626.9814,  13140.4375,  -7197.7095, -12066.8994,   9812.5029],\n",
      "        [-22089.7480,  11754.4102, -40917.1523,  49924.9102, -36489.7227,\n",
      "         -18354.6016, -33698.4336,  38771.2852, -19949.0020, -43831.9297,\n",
      "         -34092.2930,  32250.2324, -18266.7500, -29183.5059,   8502.5898,\n",
      "          41729.1094,  50961.6875, -24066.2988, -39145.7773,  35598.4023],\n",
      "        [ 14171.6289,  -8813.0293,  27704.6582, -34988.9375,  29203.3613,\n",
      "          13272.6514,  27606.7949, -29699.7695,  13932.1943,  26423.2949,\n",
      "          24951.0918, -21440.6934,  11263.9395,  18912.9980,  -6390.4854,\n",
      "         -25200.9727, -31058.0312,  15981.4688,  21302.8359, -22165.5312],\n",
      "        [  4301.3413,  -2000.9086,  10595.9805, -10153.1475,  10070.0537,\n",
      "           4229.6089,   8093.8691,  -8342.3789,   4896.2637,   8894.5586,\n",
      "           8131.4644,  -7587.6279,   4839.1768,   7127.5034,  -2375.7944,\n",
      "         -10261.1523, -14116.6982,   6534.5952,   9686.4189,  -9079.3145],\n",
      "        [-12520.6484,   7179.5288, -24271.1387,  27430.9727, -26318.7578,\n",
      "         -11633.0215, -23499.6602,  22756.4395, -12816.7295, -22749.7969,\n",
      "         -20921.2754,  18228.4570, -10366.4863, -16423.8496,   5317.4287,\n",
      "          21275.9434,  32201.4492, -14639.5303, -22013.6445,  21622.8594],\n",
      "        [  2972.7693,  -1748.4403,   6126.5405,  -6670.6426,   6209.1875,\n",
      "           2807.6499,   5943.9980,  -4930.4629,   2992.9297,   5102.2632,\n",
      "           4661.7188,  -3852.5269,   2138.5825,   3947.6616,  -1488.1074,\n",
      "          -5734.2378,  -5849.4922,   3495.4834,   5218.1909,  -5189.6641],\n",
      "        [-10429.3564,   5908.8032, -18879.3242,  19912.9043, -19567.8418,\n",
      "          -9036.5225, -15138.9346,  19012.3535, -10013.9590, -19313.3027,\n",
      "         -15800.8213,  14966.5078,  -7440.8657, -12188.8828,   4328.2471,\n",
      "          18137.4570,  22811.0137, -10726.4307, -14165.7588,  13709.4004]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-44305.7031,  45787.8633,  50132.7773, -48506.5273, -45516.0078,\n",
      "          46868.6680,  48346.6250, -45343.7891,  49149.0977, -48009.5625],\n",
      "        [ 33547.4688, -34423.5820, -37717.6133,  36352.0938,  34272.0625,\n",
      "         -35226.3086, -36308.8047,  34121.2539, -36880.0000,  36026.5391],\n",
      "        [  5684.8608,  -5591.6816,  -6360.0532,   5753.6475,   5806.4272,\n",
      "          -5877.7930,  -5930.3818,   5761.0278,  -5997.9971,   5892.9541],\n",
      "        [-13996.5283,  14454.5322,  15838.1396, -15302.7148, -14377.2109,\n",
      "          14789.1270,  15255.3691, -14326.3906,  15515.8828, -15149.2549],\n",
      "        [-45958.0859,  47822.2305,  52037.5859, -50808.3008, -47165.6602,\n",
      "          48758.8242,  50410.0938, -47098.7930,  51312.0977, -50078.6953],\n",
      "        [ 30262.7363, -30876.4551, -34082.8438,  32509.1602,  30974.1680,\n",
      "         -31754.7207, -32654.1504,  30855.6602, -33127.3125,  32357.2930],\n",
      "        [ 11866.1582, -12569.0400, -13621.1045,  13345.7959,  12387.1025,\n",
      "         -12762.6973, -13158.8311,  12268.9062, -13462.0967,  13147.1807],\n",
      "        [-27323.6875,  28534.3320,  31209.2793, -30355.6406, -28370.2168,\n",
      "          29142.4824,  30223.0547, -28261.4375,  30697.5234, -29955.7090],\n",
      "        [  6137.4097,  -6106.8125,  -6892.0210,   6341.8052,   6262.7397,\n",
      "          -6379.7290,  -6475.1958,   6225.6304,  -6561.5601,   6425.0435],\n",
      "        [-20192.8730,  20436.1602,  22697.8145, -21452.8672, -20632.1133,\n",
      "          21096.0723,  21645.9512, -20521.4434,  21941.1113, -21448.9512]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([ 53145.4258, -40283.7109,  -7550.3594,  16855.4492,  54190.5859,\n",
      "        -37091.2656, -14132.4756,  32742.3203,  -7974.8794,  25113.8203],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([ 53145.4258, -40283.7109,  -7550.3594,  16855.4492,  54190.5859,\n",
      "        -37091.2656, -14132.4756,  32742.3203,  -7974.8794,  25113.8203],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 1.0396e+06, -4.6802e+05, -1.0163e+06, -1.0410e+06,  1.0184e+06,\n",
      "         -8.2869e+05,  1.0333e+06,  1.0324e+06,  4.8897e+03,  1.0361e+06,\n",
      "          9.0283e+05, -9.5728e+05, -1.0346e+06,  1.0255e+06,  9.3631e+05,\n",
      "         -9.6987e+05, -1.0068e+06,  9.3409e+05, -1.0290e+06,  1.0023e+06],\n",
      "        [-3.8833e+04,  1.7499e+04,  3.7962e+04,  3.8888e+04, -3.8046e+04,\n",
      "          3.0899e+04, -3.8598e+04, -3.8569e+04, -3.4964e+02, -3.8705e+04,\n",
      "         -3.3752e+04,  3.5783e+04,  3.8656e+04, -3.8314e+04, -3.4997e+04,\n",
      "          3.6257e+04,  3.7602e+04, -3.4888e+04,  3.8447e+04, -3.7462e+04],\n",
      "        [-4.6013e+04,  2.0815e+04,  4.4986e+04,  4.6071e+04, -4.5073e+04,\n",
      "          3.6729e+04, -4.5732e+04, -4.5688e+04,  4.6577e+02, -4.5860e+04,\n",
      "         -4.0045e+04,  4.2405e+04,  4.5802e+04, -4.5396e+04, -4.1466e+04,\n",
      "          4.2959e+04,  4.4596e+04, -4.1388e+04,  4.5553e+04, -4.4389e+04],\n",
      "        [-2.4378e+04,  1.0904e+04,  2.3832e+04,  2.4410e+04, -2.3880e+04,\n",
      "          1.9511e+04, -2.4232e+04, -2.4207e+04, -8.5738e+01, -2.4297e+04,\n",
      "         -2.1192e+04,  2.2450e+04,  2.4261e+04, -2.4047e+04, -2.1953e+04,\n",
      "          2.2747e+04,  2.3613e+04, -2.1901e+04,  2.4129e+04, -2.3505e+04],\n",
      "        [-2.1419e+04,  9.6165e+03,  2.0934e+04,  2.1449e+04, -2.0985e+04,\n",
      "          1.7056e+04, -2.1291e+04, -2.1273e+04, -3.2413e+02, -2.1348e+04,\n",
      "         -1.8541e+04,  1.9701e+04,  2.1311e+04, -2.1125e+04, -1.9297e+04,\n",
      "          1.9958e+04,  2.0740e+04, -1.9233e+04,  2.1195e+04, -2.0637e+04],\n",
      "        [-8.0557e+04,  3.6268e+04,  7.8726e+04,  8.0665e+04, -7.8927e+04,\n",
      "          6.4111e+04, -8.0077e+04, -8.0006e+04, -9.8191e+02, -8.0293e+04,\n",
      "         -7.0010e+04,  7.4228e+04,  8.0196e+04, -7.9483e+04, -7.2626e+04,\n",
      "          7.5222e+04,  7.8079e+04, -7.2468e+04,  7.9769e+04, -7.7717e+04],\n",
      "        [-1.4489e+04,  6.5630e+03,  1.4166e+04,  1.4508e+04, -1.4192e+04,\n",
      "          1.1520e+04, -1.4399e+04, -1.4387e+04,  1.1829e+02, -1.4440e+04,\n",
      "         -1.2525e+04,  1.3310e+04,  1.4409e+04, -1.4285e+04, -1.3041e+04,\n",
      "          1.3485e+04,  1.4026e+04, -1.3001e+04,  1.4329e+04, -1.3946e+04],\n",
      "        [-2.5269e+04,  1.1360e+04,  2.4700e+04,  2.5305e+04, -2.4758e+04,\n",
      "          2.0112e+04, -2.5116e+04, -2.5097e+04, -3.0957e+02, -2.5184e+04,\n",
      "         -2.1917e+04,  2.3258e+04,  2.5149e+04, -2.4929e+04, -2.2766e+04,\n",
      "          2.3568e+04,  2.4465e+04, -2.2698e+04,  2.5015e+04, -2.4364e+04],\n",
      "        [-9.8733e+04,  4.4474e+04,  9.6491e+04,  9.8859e+04, -9.6722e+04,\n",
      "          7.8821e+04, -9.8150e+04, -9.8051e+04, -9.3031e+02, -9.8414e+04,\n",
      "         -8.5702e+04,  9.0862e+04,  9.8244e+04, -9.7384e+04, -8.8917e+04,\n",
      "          9.2067e+04,  9.5682e+04, -8.8728e+04,  9.7715e+04, -9.5159e+04],\n",
      "        [-9.1924e+04,  4.1397e+04,  8.9852e+04,  9.2047e+04, -9.0060e+04,\n",
      "          7.3310e+04, -9.1376e+04, -9.1293e+04, -6.9738e+02, -9.1625e+04,\n",
      "         -8.0103e+04,  8.4749e+04,  9.1514e+04, -9.0705e+04, -8.2770e+04,\n",
      "          8.5857e+04,  8.9064e+04, -8.2684e+04,  9.1024e+04, -8.8689e+04],\n",
      "        [-2.3842e+04,  1.0746e+04,  2.3314e+04,  2.3874e+04, -2.3355e+04,\n",
      "          1.9025e+04, -2.3694e+04, -2.3675e+04,  2.1306e+02, -2.3761e+04,\n",
      "         -2.0672e+04,  2.1953e+04,  2.3729e+04, -2.3516e+04, -2.1493e+04,\n",
      "          2.2248e+04,  2.3083e+04, -2.1412e+04,  2.3599e+04, -2.2993e+04],\n",
      "        [-4.3353e+04,  1.9484e+04,  4.2390e+04,  4.3412e+04, -4.2470e+04,\n",
      "          3.4525e+04, -4.3089e+04, -4.3051e+04, -1.0903e+01, -4.3207e+04,\n",
      "         -3.7685e+04,  3.9946e+04,  4.3151e+04, -4.2774e+04, -3.9053e+04,\n",
      "          4.0460e+04,  4.1978e+04, -3.8952e+04,  4.2915e+04, -4.1810e+04],\n",
      "        [-3.2207e+04,  1.4540e+04,  3.1482e+04,  3.2253e+04, -3.1554e+04,\n",
      "          2.5581e+04, -3.2011e+04, -3.1989e+04, -2.7695e+02, -3.2100e+04,\n",
      "         -2.7947e+04,  2.9661e+04,  3.2057e+04, -3.1769e+04, -2.8981e+04,\n",
      "          3.0059e+04,  3.1180e+04, -2.8939e+04,  3.1880e+04, -3.1054e+04],\n",
      "        [-7.8410e+04,  3.5128e+04,  7.6659e+04,  7.8510e+04, -7.6813e+04,\n",
      "          6.2475e+04, -7.7939e+04, -7.7859e+04, -1.5654e+02, -7.8146e+04,\n",
      "         -6.8245e+04,  7.2265e+04,  7.8052e+04, -7.7363e+04, -7.0675e+04,\n",
      "          7.3223e+04,  7.5968e+04, -7.0483e+04,  7.7630e+04, -7.5655e+04],\n",
      "        [-3.5870e+04,  1.6125e+04,  3.5066e+04,  3.5918e+04, -3.5140e+04,\n",
      "          2.8559e+04, -3.5653e+04, -3.5622e+04, -1.0132e+02, -3.5750e+04,\n",
      "         -3.1169e+04,  3.3031e+04,  3.5702e+04, -3.5386e+04, -3.2290e+04,\n",
      "          3.3479e+04,  3.4742e+04, -3.2248e+04,  3.5512e+04, -3.4591e+04],\n",
      "        [-7.2434e+04,  3.2732e+04,  7.0804e+04,  7.2527e+04, -7.0954e+04,\n",
      "          5.7769e+04, -7.1999e+04, -7.1930e+04, -9.5016e+01, -7.2195e+04,\n",
      "         -6.2850e+04,  6.6673e+04,  7.2071e+04, -7.1442e+04, -6.5282e+04,\n",
      "          6.7546e+04,  7.0167e+04, -6.5058e+04,  7.1675e+04, -6.9824e+04],\n",
      "        [-3.3219e+04,  1.4986e+04,  3.2506e+04,  3.3286e+04, -3.2533e+04,\n",
      "          2.6445e+04, -3.2971e+04, -3.3003e+04, -2.4637e+02, -3.3089e+04,\n",
      "         -2.8594e+04,  3.0545e+04,  3.3044e+04, -3.2788e+04, -2.9851e+04,\n",
      "          3.0853e+04,  3.1949e+04, -2.9699e+04,  3.2859e+04, -3.1974e+04],\n",
      "        [-2.8311e+04,  1.2703e+04,  2.7684e+04,  2.8346e+04, -2.7731e+04,\n",
      "          2.2638e+04, -2.8140e+04, -2.8108e+04,  1.9700e+02, -2.8215e+04,\n",
      "         -2.4534e+04,  2.6026e+04,  2.8158e+04, -2.7914e+04, -2.5470e+04,\n",
      "          2.6370e+04,  2.7415e+04, -2.5420e+04,  2.8004e+04, -2.7260e+04],\n",
      "        [-4.4995e+04,  2.0290e+04,  4.3981e+04,  4.5057e+04, -4.4082e+04,\n",
      "          3.5761e+04, -4.4723e+04, -4.4688e+04, -3.0010e+02, -4.4846e+04,\n",
      "         -3.9105e+04,  4.1460e+04,  4.4790e+04, -4.4399e+04, -4.0561e+04,\n",
      "          4.2004e+04,  4.3592e+04, -4.0456e+04,  4.4552e+04, -4.3396e+04],\n",
      "        [-6.9553e+04,  3.1319e+04,  6.7979e+04,  6.9642e+04, -6.8135e+04,\n",
      "          5.5524e+04, -6.9143e+04, -6.9070e+04, -6.0067e+02, -6.9327e+04,\n",
      "         -6.0405e+04,  6.3982e+04,  6.9191e+04, -6.8591e+04, -6.2562e+04,\n",
      "          6.4839e+04,  6.7370e+04, -6.2462e+04,  6.8820e+04, -6.7003e+04],\n",
      "        [-2.2062e+04,  9.8806e+03,  2.1572e+04,  2.2091e+04, -2.1615e+04,\n",
      "          1.7590e+04, -2.1927e+04, -2.1907e+04,  2.1569e+01, -2.1987e+04,\n",
      "         -1.9177e+04,  2.0319e+04,  2.1960e+04, -2.1764e+04, -1.9860e+04,\n",
      "          2.0599e+04,  2.1358e+04, -1.9832e+04,  2.1844e+04, -2.1273e+04],\n",
      "        [-4.0369e+04,  1.8214e+04,  3.9463e+04,  4.0422e+04, -3.9547e+04,\n",
      "          3.2155e+04, -4.0126e+04, -4.0088e+04, -9.3562e+01, -4.0235e+04,\n",
      "         -3.5075e+04,  3.7164e+04,  4.0170e+04, -3.9818e+04, -3.6362e+04,\n",
      "          3.7665e+04,  3.9094e+04, -3.6254e+04,  3.9952e+04, -3.8916e+04],\n",
      "        [-3.8354e+04,  1.7292e+04,  3.7497e+04,  3.8404e+04, -3.7567e+04,\n",
      "          3.0648e+04, -3.8120e+04, -3.8087e+04,  6.9878e+01, -3.8226e+04,\n",
      "         -3.3233e+04,  3.5289e+04,  3.8163e+04, -3.7824e+04, -3.4536e+04,\n",
      "          3.5763e+04,  3.7136e+04, -3.4445e+04,  3.7955e+04, -3.6962e+04],\n",
      "        [-3.4994e+04,  1.5678e+04,  3.4210e+04,  3.5043e+04, -3.4284e+04,\n",
      "          2.7921e+04, -3.4784e+04, -3.4755e+04, -4.1512e+02, -3.4877e+04,\n",
      "         -3.0350e+04,  3.2219e+04,  3.4831e+04, -3.4521e+04, -3.1505e+04,\n",
      "          3.2647e+04,  3.3876e+04, -3.1443e+04,  3.4642e+04, -3.3743e+04]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-1047341.3750,    39125.5547,    46345.0586,    24557.0332,\n",
      "           21581.5391,    81151.3359,    14596.6670,    25462.8945,\n",
      "           99444.9766,    92598.1250,    24020.4863,    43676.5977,\n",
      "           32451.8867,    78979.9453,    36136.9688,    72959.1797,\n",
      "           33558.6758,    28515.9941,    45332.0156,    70055.0234,\n",
      "           22228.2910,    40664.9609,    38637.3594,    35260.6328],\n",
      "       device='cuda:0')\n",
      "xent mean is: 650.7310180664062\n",
      "kl mean is: 66.2177505493164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 0 | Training Loss: 26265.140625, Training Accuracy: 0.0, Validation Loss: 25201.884765625, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 2554.8943, -2330.4607, -1284.7515,  -888.9233,  1839.8698, -2558.1890,\n",
      "        -1378.3379,   249.5941,   715.7570,   394.7311], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  4371.6147,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -6528.0234,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  3213.5190,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -2618.8049,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  7733.0049,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  7455.3555,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  2643.4097,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -6778.2441,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1959.0134,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1227.0437,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[-2434.4290,  1296.8535, -3227.4006,  4280.9702, -3826.6030, -1699.6154,\n",
      "         -3354.2024,  3265.8721, -2280.8311, -3418.0647],\n",
      "        [ 3522.7771, -2171.8662,  4990.4243, -6205.6289,  5658.9385,  2693.1062,\n",
      "          5009.4956, -5073.6108,  3386.1863,  5280.7485],\n",
      "        [-1739.6044,  1037.7721, -2438.4304,  3068.3354, -2785.9795, -1304.7142,\n",
      "         -2462.3911,  2472.6379, -1664.0530, -2576.3804],\n",
      "        [ 1613.8778,  -749.4642,  1859.0186, -2733.1035,  2385.7917,   988.5390,\n",
      "          2054.9287, -1899.9199,  1445.4961,  2022.1442],\n",
      "        [-4436.7183,  2601.8511, -5828.0786,  7611.6074, -6864.6250, -3186.8303,\n",
      "         -6023.2598,  5969.1060, -4155.2837, -6267.7378],\n",
      "        [-4167.1196,  2720.4741, -5781.4521,  7151.8867, -6556.4116, -3208.9773,\n",
      "         -5803.1025,  5931.2422, -3977.9214, -6202.2041],\n",
      "        [-1516.3965,   826.4667, -1957.2854,  2622.0601, -2343.8137, -1050.8069,\n",
      "         -2049.2446,  1995.2872, -1410.4695, -2096.3325],\n",
      "        [ 4006.0137, -2298.5002,  5078.7422, -6778.1055,  6082.8403,  2790.6931,\n",
      "          5318.0293, -5214.4185,  3702.9365,  5503.0425],\n",
      "        [-1168.4825,   574.9365, -1408.5891,  2007.9491, -1767.7865,  -756.0875,\n",
      "         -1528.8973,  1444.5583, -1065.8817, -1524.2850],\n",
      "        [ -706.5640,   269.1013,  -845.6556,  1253.8761, -1082.4144,  -415.2193,\n",
      "          -933.6832,   841.4459,  -637.5824,  -889.0616]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([ 5404.2886, -7159.3208,  3615.5876, -3765.1934,  9183.4141,  7991.3643,\n",
      "         3295.7917, -8336.6982,  2677.5569,  1820.2556], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([ 5404.2886, -7159.3208,  3615.5876, -3765.1934,  9183.4141,  7991.3643,\n",
      "         3295.7917, -8336.6982,  2677.5569,  1820.2556], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          10221.2490,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           -948.5778,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           7087.2124,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           8869.1172,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -22864.8359,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -5096.3931,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -2680.0376,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           5114.2754,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           2364.5825,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -7870.6279,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[ -7689.5493,   6650.8911,  -4207.8555,  -6360.0830,   1936.9938,\n",
      "           8169.0718,   9802.0166,  -5146.2778,  -6740.4048,   7322.6392],\n",
      "        [  1296.8643,  -1212.9507,    790.9960,   1221.3884,   -391.4066,\n",
      "          -1435.8846,  -1772.2708,    883.0748,   1166.0844,  -1287.7975],\n",
      "        [ -4895.4341,   3893.0657,  -2331.5635,  -3473.1841,   1013.6912,\n",
      "           4957.3794,   5798.2012,  -3226.8293,  -4158.7124,   4462.3672],\n",
      "        [ -6140.6436,   4960.3921,  -2995.3271,  -4483.9854,   1334.0275,\n",
      "           6269.9990,   7376.4854,  -4064.8162,  -5240.9048,   5643.3188],\n",
      "        [ 16941.9141, -14819.1006,   9448.7158,  14295.9297,  -4366.3135,\n",
      "         -18121.6289, -21808.9023,  11359.7178,  14920.2285, -16230.1914],\n",
      "        [  3395.9634,  -2656.1208,   1584.7883,   2339.7502,   -663.0509,\n",
      "          -3414.0684,  -3962.0171,   2227.0713,   2875.2534,  -3069.5715],\n",
      "        [  1664.8003,  -1222.8738,    684.1940,   1008.5793,   -290.5488,\n",
      "          -1611.4392,  -1843.8844,   1087.8293,   1370.6775,  -1459.2208],\n",
      "        [ -3471.9973,   2868.5659,  -1781.4249,  -2652.1770,    765.6231,\n",
      "           3601.6814,   4245.9458,  -2293.9749,  -3003.1887,   3226.9873],\n",
      "        [ -1318.6552,    888.9630,   -476.8270,   -671.1895,    162.8468,\n",
      "           1227.9982,   1353.5592,   -843.1252,  -1064.5862,   1109.0708],\n",
      "        [  5400.6938,  -4368.0459,   2655.9451,   3959.6392,  -1163.3865,\n",
      "          -5527.1836,  -6491.7012,   3570.4871,   4620.7861,  -4966.5337]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([ 10812.2617,  -1648.0133,   7562.3428,   9329.0957, -23490.7344,\n",
      "         -5331.8462,  -2769.2935,   5150.6055,   2349.9707,  -8190.0659],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([ 10812.2617,  -1648.0133,   7562.3428,   9329.0957, -23490.7344,\n",
      "         -5331.8462,  -2769.2935,   5150.6055,   2349.9707,  -8190.0659],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[  -710.7167,    218.9005,  -1545.0421,   1822.0685,  -1342.4315,\n",
      "           -580.2842,  -1435.4504,   1303.5197,   -820.9475,  -1516.0311,\n",
      "          -1406.2279,   1163.4409,   -961.2617,  -1481.2047,    513.1207,\n",
      "           1558.2091,   1738.1060,  -1223.2295,  -1348.1000,   1498.4250],\n",
      "        [  7679.1436,  -5160.4541,  10776.3350, -13131.0322,  12059.7627,\n",
      "           6037.2319,  10959.5986, -11617.4746,   7528.2949,  11360.1074,\n",
      "          10542.7500,  -9020.1592,   5824.8730,   8983.1201,  -2756.5344,\n",
      "         -11235.0713, -13678.2822,   6700.2739,   8905.9766, -10083.9482],\n",
      "        [  1872.5153,  -1252.1924,   2521.0627,  -3502.1528,   2607.9067,\n",
      "           1542.1421,   2774.5520,  -2636.0935,   1761.4188,   3240.2515,\n",
      "           2511.8213,  -2526.3523,   1448.4460,   2055.3325,   -750.4432,\n",
      "          -3094.4055,  -2895.2439,   1803.8051,   2456.6658,  -2603.0105],\n",
      "        [   378.1036,   -178.6777,    749.0887,   -974.6555,    804.3226,\n",
      "            382.1190,    862.7393,   -794.2212,    422.1640,    854.1364,\n",
      "            881.4480,   -693.0647,    475.6100,    678.8004,   -294.3695,\n",
      "           -871.6476,   -984.3444,    582.6562,    784.1160,   -723.0754],\n",
      "        [ -2307.6917,   1572.5590,  -3485.7957,   4276.3032,  -4062.1995,\n",
      "          -1764.9801,  -2911.5752,   3381.0889,  -2124.2683,  -3281.5613,\n",
      "          -2938.6357,   2912.0461,  -1857.0549,  -2935.1323,   1005.1026,\n",
      "           3850.9685,   4589.3188,  -2381.2778,  -2622.3081,   2939.2336],\n",
      "        [ 11352.8105,  -8513.3242,  19046.7168, -18136.5664,  17427.0293,\n",
      "           9367.7939,  18437.6562, -16339.0273,  12190.2490,  19747.6426,\n",
      "          15494.5771, -13692.5479,   9152.6182,  14475.6650,  -4419.6685,\n",
      "         -17899.8594, -21466.9277,  11239.1152,  15157.2461, -14551.3379],\n",
      "        [  -994.2112,    428.9153,  -2094.7969,   2146.4961,  -2374.2129,\n",
      "           -971.0883,  -1909.8717,   1712.0967,  -1134.6361,  -2063.7468,\n",
      "          -1908.2909,   1723.0417,  -1367.7031,  -1887.3850,    692.2177,\n",
      "           2099.5195,   2854.9011,  -1594.1589,  -1866.1381,   1918.3270],\n",
      "        [ -1385.9755,    945.8814,  -2423.4656,   2696.7983,  -2522.5317,\n",
      "          -1284.0890,  -2458.6121,   2543.0874,  -1419.1586,  -2084.9602,\n",
      "          -2490.9629,   1760.0211,  -1348.9823,  -1863.5206,    724.1656,\n",
      "           2202.5662,   2743.7581,  -1603.9930,  -2052.5723,   2384.9004],\n",
      "        [   555.9634,   -250.7033,    444.5838,  -1134.4033,    316.7994,\n",
      "            294.6372,    734.9875,   -512.1833,    497.5321,    741.6561,\n",
      "            437.5694,   -485.4193,    277.7403,    598.6165,   -177.7138,\n",
      "           -747.4401,  -1061.8250,    283.7032,    699.0532,   -778.1894],\n",
      "        [ -1002.1376,    392.5904,  -2000.4082,   2014.7810,  -1886.4926,\n",
      "           -876.5911,  -1883.8143,   1598.3971,  -1029.0630,  -1610.7284,\n",
      "          -1900.1826,   1456.3990,  -1120.7786,  -1761.2285,    641.5936,\n",
      "           2057.6638,   2256.5750,  -1390.3175,  -1702.4375,   1803.9481]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[ -1718.9302,    337.6522,   1616.5885,   1743.6158,  -1619.6078,\n",
      "           1379.4939,  -1574.4116,  -1611.5461,    248.9527,  -1473.7146],\n",
      "        [ 13765.6445,  -3522.4033, -13584.9736, -13788.7959,  13550.9541,\n",
      "         -11992.8359,  13677.0498,  13676.8447,  -3019.7915,  13651.7285],\n",
      "        [  3527.9363,   -912.9688,  -3458.1904,  -3540.2812,   3451.7041,\n",
      "          -3026.1741,   3467.0010,   3478.3752,   -729.4838,   3433.4299],\n",
      "        [   963.9329,   -193.8332,   -911.5391,   -976.5474,    912.7236,\n",
      "           -782.0231,    891.1669,    909.8592,   -132.3682,    840.7630],\n",
      "        [ -4439.7788,   1088.0450,   4353.8076,   4455.7451,  -4342.5928,\n",
      "           3835.9756,  -4358.9062,  -4373.4478,   1073.3629,  -4313.6699],\n",
      "        [ 22086.6387,  -5931.2109, -21773.7383, -22130.3750,  21714.5586,\n",
      "         -19086.2012,  21893.8867,  21908.8535,  -5474.7905,  21824.4707],\n",
      "        [ -2514.6880,    526.6914,   2402.1440,   2542.4751,  -2401.7908,\n",
      "           2077.8906,  -2360.2251,  -2400.6140,    447.7438,  -2256.0232],\n",
      "        [ -3000.6965,    700.9406,   2921.4961,   3017.7205,  -2916.9011,\n",
      "           2565.0063,  -2911.9380,  -2932.2612,    592.0305,  -2856.4102],\n",
      "        [   849.3433,   -205.8150,   -837.7039,   -850.6840,    835.2020,\n",
      "           -742.0071,    842.8015,    842.5439,   -216.7742,    839.9832],\n",
      "        [ -2183.6914,    439.8980,   2072.4890,   2210.1208,  -2074.7971,\n",
      "           1783.6781,  -2031.7880,  -2070.4382,    288.0606,  -1926.9705]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([  2246.6750, -14011.0449,  -3732.0398,  -1229.2595,   4710.5859,\n",
      "        -22648.0449,   3072.9561,   3312.5261,   -868.7606,   2736.7600],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([  2246.6750, -14011.0449,  -3732.0398,  -1229.2595,   4710.5859,\n",
      "        -22648.0449,   3072.9561,   3312.5261,   -868.7606,   2736.7600],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[-11580.9443,   7647.1094, -17114.7324,  19627.5605, -20039.8887,\n",
      "         -10317.9629, -17520.7559,  18658.0430, -12067.5264, -17800.3398,\n",
      "         -17485.7188,  15573.9883,  -8351.7422, -13037.7305,   5101.5576,\n",
      "          16088.0986,  17047.6758, -12084.6660, -14904.9170,  13716.1914],\n",
      "        [ 10069.9873,  -6437.0508,  11991.5400, -15661.5742,  13730.9160,\n",
      "           7505.5469,  14109.0234, -13485.0029,   9039.8105,  15112.8604,\n",
      "          13374.6279, -11272.2139,   6242.6245,   9778.7627,  -3736.0781,\n",
      "         -12075.8516, -15565.4043,   7788.0415,  10565.7666, -10646.5312],\n",
      "        [  1235.8468,   -798.4420,   1725.7657,  -2031.4626,   1928.9255,\n",
      "           1113.2617,   1680.1718,  -1964.3835,   1334.9421,   1775.1510,\n",
      "           1805.0087,  -1519.0658,    711.6630,   1354.8307,   -677.2084,\n",
      "          -1658.3235,  -2440.6541,   1196.8090,   1303.4662,  -1745.1481],\n",
      "        [ -2762.8525,   1866.3439,  -3658.1255,   4229.4946,  -3823.5154,\n",
      "          -2368.6985,  -4486.1367,   4006.3301,  -2629.6072,  -4595.6162,\n",
      "          -3787.8062,   3448.8025,  -2032.4688,  -3551.7437,   1280.9447,\n",
      "           4083.5068,   4468.2188,  -2750.7788,  -3799.1987,   3533.2065],\n",
      "        [-13194.3828,   8008.2471, -19378.7559,  22124.8809, -16029.9219,\n",
      "          -9508.1562, -15385.0693,  17684.8867, -11423.1455, -20954.2344,\n",
      "         -15551.7383,  15544.8535,  -9651.8184, -14533.7617,   4755.8652,\n",
      "          19147.5566,  22581.5215, -11944.7471, -16015.9561,  16796.9941],\n",
      "        [  8147.3589,  -6095.5947,  12998.1143, -15386.6875,  13013.0850,\n",
      "           6918.5283,  12906.6250, -13618.1670,   7965.6353,  12303.9268,\n",
      "          11356.3389, -10136.9053,   5734.3569,   9174.1699,  -3592.5566,\n",
      "         -11230.2959, -13451.7363,   7787.9053,   8550.4414, -10155.9980],\n",
      "        [  1725.6968,   -946.3573,   3700.9531,  -3247.5618,   3284.9275,\n",
      "           1546.2061,   2705.9758,  -2725.0186,   2033.1558,   2999.7571,\n",
      "           2747.6018,  -2682.0916,   1908.2632,   2613.6389,  -1008.5449,\n",
      "          -3504.2581,  -4689.7100,   2445.2351,   2956.4539,  -3178.4810],\n",
      "        [ -7415.7808,   5025.2754, -11585.3535,  12170.4189, -11961.9521,\n",
      "          -6180.1470, -11130.9795,  10428.6895,  -7631.8037, -10736.7803,\n",
      "          -9653.9297,   8752.2695,  -5416.3687,  -8097.5830,   3021.6006,\n",
      "           9609.6377,  14418.2314,  -7323.7998,  -9057.5518,  10285.0684],\n",
      "        [  1507.7163,  -1071.8765,   2514.7939,  -2548.5605,   2415.9573,\n",
      "           1284.7407,   2427.1096,  -1927.7455,   1515.5026,   2049.3147,\n",
      "           1820.8887,  -1537.3367,    903.9813,   1637.0089,   -725.2817,\n",
      "          -2199.2603,  -2195.2151,   1477.1028,   1772.6677,  -2081.7109],\n",
      "        [ -6581.9312,   4336.1724,  -9390.8359,   9230.1377,  -9267.4189,\n",
      "          -5026.5586,  -7321.2412,   9223.2246,  -6241.4868,  -9674.6924,\n",
      "          -7569.7026,   7508.7729,  -3978.0774,  -6186.9683,   2559.4041,\n",
      "           8583.7324,  10526.9941,  -5518.0391,  -6004.5713,   6580.6250]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-20567.0488,  20480.0332,  21832.8242, -20980.2285, -20676.7949,\n",
      "          20891.7227,  21252.9883, -20662.8633,  21301.7578, -21018.3633],\n",
      "        [ 15323.5879, -15157.8174, -16193.0000,  15473.3555,  15345.8105,\n",
      "         -15482.6895, -15725.1064,  15328.0635, -15742.9785,  15544.0713],\n",
      "        [  2021.3196,  -1862.6346,  -2095.4587,   1811.5312,   1994.0238,\n",
      "          -1973.5168,  -1940.0504,   1975.4001,  -1920.2494,   1920.5248],\n",
      "        [ -5011.8428,   4987.9487,   5323.8447,  -5104.0527,  -5041.2866,\n",
      "           5089.4194,   5174.1060,  -5036.1958,   5187.3315,  -5118.2485],\n",
      "        [-21325.9727,  21494.7637,  22707.5215, -22163.6836, -21484.7910,\n",
      "          21798.7363,  22270.2480, -21515.5391,  22373.0684, -22037.5664],\n",
      "        [ 13726.1582, -13429.9502, -14477.3262,  13620.8701,  13723.7998,\n",
      "         -13801.0889, -13956.9072,  13695.7803, -13943.5068,  13783.9316],\n",
      "        [  4136.5967,  -4231.8994,  -4456.9312,   4363.6343,   4221.5371,\n",
      "          -4276.6567,  -4363.2051,   4208.6440,  -4401.1279,   4336.6890],\n",
      "        [-12812.2881,  12879.3545,  13679.4609, -13264.9639, -12958.7021,\n",
      "          13093.7148,  13382.2881, -12955.6738,  13421.0391, -13221.9111],\n",
      "        [  2376.0183,  -2219.5938,  -2473.9270,   2185.1589,   2348.3850,\n",
      "          -2335.0854,  -2313.8403,   2332.0020,  -2295.0291,   2286.0725],\n",
      "        [ -9648.7783,   9319.5254,  10135.7773,  -9385.0752,  -9612.1211,\n",
      "           9634.1133,   9698.1426,  -9576.8379,   9667.7656,  -9576.4404]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([ 23421.4688, -17535.8984,  -2633.4675,   5737.5596,  23691.1816,\n",
      "        -16088.6367,  -4632.9482,  14445.1152,  -3021.9897,  11554.2041],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([ 23421.4688, -17535.8984,  -2633.4675,   5737.5596,  23691.1816,\n",
      "        -16088.6367,  -4632.9482,  14445.1152,  -3021.9897,  11554.2041],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 712617.2500, -186600.1719, -705019.5000, -713454.9375,  702902.0000,\n",
      "         -621637.6250,  710560.1875,  709921.6250, -164611.2344,  711266.4375,\n",
      "          661493.5000, -679505.7500, -711312.3750,  708468.8750,  671770.0625,\n",
      "         -683904.5625, -703225.3750,  673514.0625, -709395.0000,  696364.0625],\n",
      "        [ -26370.9453,    6922.1587,   26089.0391,   26402.6230,  -26012.1621,\n",
      "           22979.6504,  -26295.4863,  -26272.7969,    5976.7651,  -26321.6719,\n",
      "          -24497.7344,   25158.7559,   26326.5977,  -26219.9844,  -24870.1816,\n",
      "           25323.2324,   26025.0430,  -24928.1777,   26256.4609,  -25781.4492],\n",
      "        [ -34054.8359,    9044.5645,   33693.1562,   34092.2227,  -33589.4570,\n",
      "           29720.8730,  -33956.5000,  -33923.1562,    8365.5020,  -33991.5586,\n",
      "          -31658.0098,   32492.5312,   33995.4180,  -33857.3242,  -32116.8965,\n",
      "           32702.6562,   33620.6289,  -32206.8457,   33904.4336,  -33293.2305],\n",
      "        [ -17747.0371,    4577.5449,   17558.5488,   17766.9961,  -17504.9766,\n",
      "           15518.1182,  -17696.9395,  -17678.8281,    4120.6191,  -17713.5801,\n",
      "          -16486.7734,   16925.2969,   17714.4336,  -17643.0430,  -16729.8965,\n",
      "           17036.2715,   17515.7383,  -16774.2988,   17666.7578,  -17343.9453],\n",
      "        [ -14225.4326,    3691.0942,   14072.4111,   14242.6221,  -14032.3389,\n",
      "           12404.6631,  -14185.1357,  -14172.5566,    3129.7781,  -14198.6934,\n",
      "          -13188.4033,   13557.5371,   14198.0869,  -14141.6016,  -13411.4688,\n",
      "           13644.4053,   14037.6201,  -13440.8672,   14159.4238,  -13895.7803],\n",
      "        [ -55162.6250,   14421.6260,   54567.1719,   55227.1445,  -54413.5039,\n",
      "           48080.5234,  -55006.7383,  -54956.5352,   12326.4531,  -55060.1289,\n",
      "          -51247.3164,   52630.3125,   55071.8906,  -54847.2734,  -52029.9766,\n",
      "           52978.9453,   54455.2656,  -52170.7539,   54927.5742,  -53931.4414],\n",
      "        [ -10046.4219,    2666.1008,    9939.9434,   10057.8281,   -9908.8008,\n",
      "            8751.6016,  -10017.1396,  -10007.7334,    2441.3865,  -10027.3193,\n",
      "           -9306.7988,    9566.9658,   10024.0059,   -9985.1602,   -9466.7871,\n",
      "            9629.3184,    9913.4941,   -9488.4746,    9996.2617,   -9807.2217],\n",
      "        [ -16901.9141,    4407.5630,   16721.2168,   16922.7500,  -16672.9375,\n",
      "           14732.9375,  -16853.5293,  -16839.3320,    3770.9792,  -16870.0039,\n",
      "          -15684.5664,   16114.9150,   16872.1562,  -16804.6738,  -15936.4590,\n",
      "           16220.7930,   16678.7930,  -15974.4121,   16827.5371,  -16518.1855],\n",
      "        [ -66966.9297,   17570.5684,   66243.1562,   67042.3438,  -66052.3828,\n",
      "           58450.9375,  -66778.6641,  -66714.2109,   15164.8652,  -66844.1406,\n",
      "          -62161.4258,   63840.0234,   66837.9531,  -66571.1016,  -63126.5312,\n",
      "           64258.3789,   66103.6875,  -63297.0977,   66658.8828,  -65427.9570],\n",
      "        [ -64306.1758,   16866.6445,   63616.1211,   64380.4062,  -63431.4258,\n",
      "           56102.8164,  -64124.2383,  -64064.8906,   14688.4961,  -64187.6250,\n",
      "          -59808.5586,   61368.3594,   64200.1172,  -63939.2891,  -60625.5898,\n",
      "           61765.5742,   63477.0625,  -60818.8008,   64030.8867,  -62874.3438],\n",
      "        [ -17416.4043,    4579.7051,   17232.8262,   17436.1602,  -17178.4492,\n",
      "           15201.4385,  -17365.7598,  -17349.3438,    4251.4229,  -17383.3301,\n",
      "          -16163.6592,   16608.9727,   17384.6641,  -17313.8555,  -16426.4805,\n",
      "           16719.2871,   17187.7148,  -16460.2520,   17337.8145,  -17023.2012],\n",
      "        [ -31306.4805,    8153.5762,   30975.8320,   31342.6309,  -30880.2207,\n",
      "           27303.7500,  -31216.6777,  -31186.8789,    7370.0239,  -31246.8555,\n",
      "          -29083.4922,   29865.3594,   31251.1367,  -31125.8809,  -29517.8516,\n",
      "           30056.0898,   30895.7949,  -29593.2344,   31166.9902,  -30600.6504],\n",
      "        [ -21423.6875,    5638.3813,   21194.2969,   21449.8184,  -21132.0391,\n",
      "           18652.4043,  -21361.8828,  -21344.3672,    4865.7769,  -21383.3848,\n",
      "          -19886.4219,   20432.7266,   21386.4121,  -21299.3301,  -20189.7832,\n",
      "           20567.3535,   21140.4941,  -20250.6094,   21328.4199,  -20937.8027],\n",
      "        [ -58131.6367,   14989.0264,   57515.7188,   58196.0586,  -57340.9453,\n",
      "           50727.9492,  -57967.8086,  -57907.5234,   13575.8154,  -58021.2383,\n",
      "          -54041.3359,   55467.1914,   58031.9336,  -57795.9375,  -54826.8281,\n",
      "           55832.7891,   57380.1914,  -54964.6602,   57877.5000,  -56838.9062],\n",
      "        [ -25560.3379,    6651.6450,   25288.7051,   25589.8105,  -25212.5273,\n",
      "           22290.5469,  -25487.3848,  -25463.3359,    5955.3999,  -25512.1035,\n",
      "          -23741.6289,   24376.7285,   25514.8242,  -25411.5254,  -24092.3516,\n",
      "           24540.5977,   25226.9746,  -24166.6035,   25447.8184,  -24982.9082],\n",
      "        [ -50135.2461,   13277.9717,   49597.3516,   50191.1523,  -49448.3555,\n",
      "           43730.4648,  -49992.3008,  -49944.1914,   11778.9727,  -50042.1758,\n",
      "          -46534.7656,   47800.1406,   50037.4141,  -49838.2930,  -47278.4023,\n",
      "           48109.3672,   49484.6797,  -47380.2148,   49901.0000,  -48989.5820],\n",
      "        [  -8808.4268,    2390.6782,    8734.4180,    8840.7852,   -8687.4990,\n",
      "            7660.6255,   -8754.1650,   -8782.1123,    1897.9868,   -8774.5000,\n",
      "           -7861.1187,    8295.2129,    8783.1807,   -8767.4395,   -8218.3955,\n",
      "            8293.5439,    8571.8037,   -8198.0791,    8743.2061,   -8520.4434],\n",
      "        [ -20945.0430,    5438.4751,   20724.1270,   20967.4180,  -20658.8125,\n",
      "           18305.6270,  -20885.4863,  -20862.7383,    5058.5283,  -20905.1992,\n",
      "          -19427.1562,   19954.6445,   20899.2031,  -20817.1406,  -19733.9922,\n",
      "           20085.4434,   20670.0176,  -19788.8027,   20842.3848,  -20451.8691],\n",
      "        [ -31324.6777,    8213.2432,   30989.4863,   31361.8281,  -30898.4570,\n",
      "           27286.7891,  -31234.9551,  -31207.5332,    7178.1646,  -31265.8887,\n",
      "          -29098.3750,   29883.8789,   31271.6016,  -31146.0820,  -29543.5762,\n",
      "           30078.4746,   30918.7871,  -29618.6621,   31189.3398,  -30618.9297],\n",
      "        [ -46893.9453,   12297.2207,   46388.2539,   46946.6797,  -46252.9102,\n",
      "           40929.5547,  -46762.5742,  -46716.1562,   10650.2510,  -46807.3750,\n",
      "          -43533.6602,   44692.4258,   46797.8945,  -46613.1211,  -44180.6836,\n",
      "           44987.5352,   46281.5508,  -44310.7266,   46672.6602,  -45802.6992],\n",
      "        [ -16290.6641,    4201.6377,   16118.9229,   16309.3145,  -16070.1787,\n",
      "           14220.4033,  -16244.0361,  -16228.0908,    3835.8965,  -16259.3252,\n",
      "          -15134.3438,   15537.0293,   16262.2383,  -16195.9346,  -15355.1729,\n",
      "           15643.2617,   16076.0869,  -15402.4336,   16219.8555,  -15921.5928],\n",
      "        [ -28062.6797,    7390.8647,   27762.6484,   28094.4160,  -27679.5312,\n",
      "           24467.2402,  -27982.5703,  -27955.7227,    6557.3335,  -28010.2461,\n",
      "          -26062.4180,   26757.7031,   28009.0918,  -27897.1504,  -26457.6465,\n",
      "           26935.9199,   27695.8516,  -26520.6367,   27933.7656,  -27421.8594],\n",
      "        [ -26706.9375,    7039.0791,   26422.4414,   26737.3652,  -26340.5645,\n",
      "           23320.7344,  -26629.8809,  -26605.1660,    6326.3774,  -26656.9004,\n",
      "          -24772.8711,   25456.8047,   26655.0137,  -26547.7012,  -25174.3711,\n",
      "           25625.2051,   26355.7617,  -25237.1777,   26583.0352,  -26091.7637],\n",
      "        [ -23827.6680,    6170.6162,   23573.7148,   23856.4336,  -23504.0508,\n",
      "           20797.7148,  -23760.0918,  -23738.8633,    5324.4082,  -23782.6230,\n",
      "          -22112.3301,   22721.5957,   23786.2500,  -23689.6836,  -22460.0254,\n",
      "           22870.5059,   23512.0391,  -22522.7324,   23722.6562,  -23288.2109]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-715642.0000,   26483.0859,   34192.3672,   17820.1992,   14286.6357,\n",
      "          55393.7461,   10088.2793,   16975.5059,   67240.8125,   64572.4219,\n",
      "          17488.7637,   31437.2168,   21515.8438,   58368.4336,   25666.9883,\n",
      "          50339.7539,    8915.0811,   21029.3398,   31457.1055,   47085.6250,\n",
      "          16359.0137,   28178.1074,   26817.6543,   23930.1562],\n",
      "       device='cuda:0')\n",
      "xent mean is: 473.5665588378906\n",
      "kl mean is: 66.23532104492188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 1 | Training Loss: 26928.43359375, Training Accuracy: 0.0, Validation Loss: 25920.05078125, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 1549.7943, -1395.5148,  -802.8329,  -423.8167,  1130.6096, -1443.1223,\n",
      "         -898.5400,   183.9279,   412.1217,   232.1274], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   636.3011,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -3772.4998,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  2010.1031,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -858.3660,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  3265.0630,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  4790.5894,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   406.5920,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -2499.9233,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -996.8677,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   783.6903,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[ -464.2055,   161.5161,  -420.8079,   713.1198,  -599.2777,  -220.0453,\n",
      "          -507.5715,   411.2397,  -399.7724,  -459.8224],\n",
      "        [ 2438.0261, -1686.5398,  3043.9934, -3641.5317,  3363.8420,  1828.8381,\n",
      "          3053.0654, -3055.2068,  2328.3296,  3233.0127],\n",
      "        [-1298.8707,   879.0749, -1611.0280,  1946.8063, -1791.8594,  -961.1833,\n",
      "         -1623.1190,  1613.5358, -1236.8031, -1709.3568],\n",
      "        [  667.1265,  -321.8596,   610.7335,  -965.5073,   833.1604,   361.0352,\n",
      "           714.4415,  -618.9713,   580.2787,   689.3929],\n",
      "        [-2258.7800,  1450.1522, -2567.6255,  3306.8882, -3001.4851, -1558.6042,\n",
      "         -2681.6299,  2596.3716, -2094.7197, -2786.1416],\n",
      "        [-3192.2764,  2299.0232, -3919.6230,  4659.1768, -4325.7637, -2412.7876,\n",
      "         -3930.3650,  3965.4607, -3034.2104, -4208.6982],\n",
      "        [ -314.7063,   132.1877,  -277.7558,   464.1344,  -394.3818,  -158.0145,\n",
      "          -334.5956,   279.0596,  -270.9305,  -312.8449],\n",
      "        [ 1802.8260, -1113.5178,  1939.3549, -2602.2976,  2340.0374,  1184.8990,\n",
      "          2073.2278, -1968.8209,  1642.8411,  2132.6155],\n",
      "        [  580.3152,  -552.8236,   894.6535,  -854.0848,   847.3809,   560.5353,\n",
      "           807.5053,  -901.4843,   597.2959,   929.4135],\n",
      "        [ -533.6611,   274.9115,  -577.4847,   812.1539,  -715.4517,  -326.4203,\n",
      "          -628.2963,   572.3435,  -485.5391,  -619.3650]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([ 1157.9476, -4145.7197,  2255.9639, -1425.4456,  4047.8428,  5149.5972,\n",
      "          722.5479, -3301.4045,  -637.4428,  1127.9463], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([ 1157.9476, -4145.7197,  2255.9639, -1425.4456,  4047.8428,  5149.5972,\n",
      "          722.5479, -3301.4045,  -637.4428,  1127.9463], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           4785.0479,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           -913.6773,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           3386.8914,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           2735.2498,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "         -12861.2529,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -1459.8201,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           -948.8173,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "           4720.1309,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "            983.2488,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000],\n",
      "        [     0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "          -2456.4294,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000,      0.0000,\n",
      "              0.0000,      0.0000,      0.0000,      0.0000]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[ -3774.6201,   3371.0898,  -2313.9143,  -3287.6648,   1098.0936,\n",
      "           3987.6101,   4603.0649,  -2685.3977,  -3306.6287,   3608.1714],\n",
      "        [  1022.8923,   -971.9851,    683.8245,    989.5534,   -343.3230,\n",
      "          -1117.0748,  -1320.2526,    739.7735,    913.9549,  -1014.6407],\n",
      "        [ -2405.7375,   1936.4316,  -1246.5406,  -1727.8521,    547.5173,\n",
      "           2397.5977,   2672.9407,  -1672.0033,  -2031.1348,   2165.0828],\n",
      "        [ -1842.5671,   1412.8116,   -878.1198,  -1201.4026,    372.4448,\n",
      "           1788.5389,   1962.0055,  -1269.1082,  -1529.3832,   1614.0973],\n",
      "        [ 10045.0635,  -9100.8496,   6304.5923,   8976.6084,  -3008.0667,\n",
      "         -10703.2266, -12406.8906,   7167.9756,   8850.4336,  -9684.1152],\n",
      "        [   909.1737,   -639.0042,    377.1644,    494.7138,   -135.4327,\n",
      "           -846.0749,   -894.6777,    612.8117,    737.4860,   -759.4241],\n",
      "        [   558.1937,   -379.0241,    211.9116,    278.6310,    -80.2456,\n",
      "           -508.0980,   -535.0063,    376.4431,    444.6884,   -458.1314],\n",
      "        [ -3518.1104,   3104.4924,  -2124.9937,  -3002.4817,    984.2711,\n",
      "           3694.7708,   4240.2573,  -2490.8528,  -3074.0344,   3338.4348],\n",
      "        [  -513.6277,    304.6588,   -154.6244,   -182.8426,     34.6891,\n",
      "            440.8896,    435.8203,   -335.2290,   -397.5035,    393.3914],\n",
      "        [  1634.8926,  -1255.8026,    787.4702,   1072.3114,   -327.1786,\n",
      "          -1591.3837,  -1742.1995,   1124.7684,   1361.0586,  -1433.6038]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([  5071.2686,  -1262.7748,   3646.4070,   2928.2385, -13241.6885,\n",
      "         -1557.5304,   -982.0513,   4801.2896,    988.9645,  -2592.2202],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([  5071.2686,  -1262.7748,   3646.4070,   2928.2385, -13241.6885,\n",
      "         -1557.5304,   -982.0513,   4801.2896,    988.9645,  -2592.2202],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[  -299.5913,     86.9493,   -645.1141,    718.0356,   -530.4152,\n",
      "           -238.2158,   -586.2415,    519.1932,   -354.6512,   -607.8841,\n",
      "           -599.4743,    507.8897,   -465.4895,   -676.1458,    251.0676,\n",
      "            659.2274,    715.9351,   -562.0045,   -576.6085,    646.1401],\n",
      "        [ 10850.8086,  -7942.1680,  13382.5020, -15676.9805,  14622.6201,\n",
      "           8335.8203,  13610.3691, -14274.4619,  10561.9053,  14104.1113,\n",
      "          13118.1494, -11635.8330,   8183.7432,  11843.4941,  -3950.8613,\n",
      "         -13898.1592, -16287.9326,   8837.7803,  11066.6699, -12639.8037],\n",
      "        [  1021.1646,   -753.0958,   1212.0459,  -1631.3350,   1216.6747,\n",
      "            823.6461,   1342.6854,  -1252.2057,    945.7857,   1574.3809,\n",
      "           1220.2998,  -1285.3350,    786.6744,   1040.4690,   -420.7866,\n",
      "          -1509.9442,  -1348.3003,    933.3149,   1201.8906,  -1278.6848],\n",
      "        [   148.0202,    -73.8542,    291.8568,   -361.0680,    299.2168,\n",
      "            152.2927,    331.9940,   -298.5961,    168.7745,    322.6461,\n",
      "            355.3753,   -286.4140,    212.8163,    283.9487,   -136.5041,\n",
      "           -346.7537,   -381.1219,    246.3380,    315.9481,   -289.2321],\n",
      "        [ -1178.7396,    890.2841,  -1586.1735,   1871.6111,  -1799.1597,\n",
      "           -867.1724,  -1302.1147,   1508.4435,  -1059.9585,  -1472.9541,\n",
      "          -1329.7253,   1378.9236,   -952.6609,  -1425.3340,    534.8691,\n",
      "           1763.3928,   2027.3301,  -1162.3878,  -1188.8658,   1344.2703],\n",
      "        [  7200.0889,  -6085.5244,  10911.3291,  -9759.4824,   9516.0625,\n",
      "           5852.3696,  10543.3330,  -9050.6523,   7881.1777,  11278.6367,\n",
      "           8736.1230,  -7978.2329,   5818.4614,   8730.9785,  -2886.8284,\n",
      "         -10125.9785, -11705.8389,   6801.4268,   8642.3223,  -8226.3740],\n",
      "        [  -401.6251,    173.0743,   -837.9521,    814.3839,   -905.0367,\n",
      "           -397.3614,   -748.4121,    655.3621,   -470.2024,   -797.5225,\n",
      "           -776.1194,    723.7103,   -638.8647,   -817.0154,    327.7331,\n",
      "            848.5127,   1129.2579,   -697.5387,   -762.7768,    784.6681],\n",
      "        [  -606.0785,    462.8260,   -984.9578,   1045.4451,   -986.5969,\n",
      "           -563.4456,   -993.3419,   1010.9315,   -621.7440,   -824.5239,\n",
      "          -1026.4111,    733.2220,   -625.4043,   -799.8729,    344.9650,\n",
      "            890.6441,   1082.5747,   -698.0868,   -842.9937,    987.4688],\n",
      "        [  4254.4116,  -2936.6670,   4992.2354,  -6466.1465,   5171.2344,\n",
      "           3117.6282,   5274.6172,  -5104.4922,   4027.1777,   5634.2065,\n",
      "           4671.8086,  -4416.8081,   3037.4456,   4603.1885,  -1565.6030,\n",
      "          -5386.1763,  -6469.8652,   3355.7458,   4543.3032,  -5011.0913],\n",
      "        [  -421.5283,    161.8763,   -820.6735,    779.0558,   -730.5492,\n",
      "           -364.2348,   -755.4788,    623.5685,   -435.3126,   -629.2759,\n",
      "           -796.2144,    622.3906,   -528.4371,   -786.2272,    312.3665,\n",
      "            858.3488,    912.7233,   -620.6528,   -713.9813,    757.5848]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[  -651.1483,    194.1080,    602.5636,    663.2679,   -605.6256,\n",
      "            528.1348,   -569.9650,   -595.8216,    178.0000,   -524.0955],\n",
      "        [ 16259.1602,  -6552.5117, -16114.8525, -16280.8867,  16100.3760,\n",
      "         -14758.1025,  16169.2344,  16181.5957,  -6783.6226,  16136.2441],\n",
      "        [  1623.2072,   -646.8547,  -1594.2828,  -1629.2036,   1594.0839,\n",
      "          -1447.9086,   1588.2286,   1598.1499,   -641.0452,   1570.1575],\n",
      "        [   343.8995,   -105.4967,   -320.4952,   -349.7459,    321.9107,\n",
      "           -282.4337,    305.0893,    317.4532,    -93.9952,    283.1971],\n",
      "        [ -1918.7681,    741.5807,   1884.5273,   1926.2017,  -1883.6154,\n",
      "           1719.1871,  -1875.2751,  -1887.5847,    812.9995,  -1852.6973],\n",
      "        [ 11950.4746,  -4949.3779, -11826.8271, -11970.6396,  11815.6875,\n",
      "         -10779.2324,  11849.7295,  11869.7686,  -5267.7129,  11807.1699],\n",
      "        [  -922.6896,    297.1620,    871.3723,    935.8325,   -873.9194,\n",
      "            774.9427,   -838.0365,   -865.2941,    294.5186,   -790.7674],\n",
      "        [ -1143.8110,    419.4011,   1111.4124,   1151.5703,  -1112.0985,\n",
      "           1007.6580,  -1096.3320,  -1110.9980,    424.4557,  -1070.5380],\n",
      "        [  6293.3809,  -2524.8252,  -6236.9575,  -6302.2432,   6230.6948,\n",
      "          -5710.6821,   6255.5518,   6261.2979,  -2693.8372,   6241.1792],\n",
      "        [  -812.1941,    251.0952,    759.9587,    825.1588,   -763.1160,\n",
      "            671.7624,   -726.1681,   -753.5111,    221.1107,   -677.6945]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([   921.0974, -16522.5098,  -1736.2845,   -472.9362,   2059.3584,\n",
      "        -12255.3730,   1203.5131,   1300.0078,  -6404.8481,   1098.0863],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([   921.0974, -16522.5098,  -1736.2845,   -472.9362,   2059.3584,\n",
      "        -12255.3730,   1203.5131,   1300.0078,  -6404.8481,   1098.0863],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[-5930.8242,  4250.4556, -7780.0605,  8575.0576, -8946.7539, -5282.3320,\n",
      "         -8012.4575,  8457.9502, -6230.2671, -8077.8218, -8004.5449,  7386.7612,\n",
      "         -4149.1626, -6147.9517,  2727.1260,  7178.0225,  7332.3906, -5895.3076,\n",
      "         -6767.7275,  6171.8838],\n",
      "        [ 5099.5503, -3501.6516,  5206.6172, -6614.5356,  5854.1655,  3664.9382,\n",
      "          6247.8589, -5868.2061,  4469.8535,  6688.4971,  5877.1924, -5100.5625,\n",
      "          2984.4316,  4433.5835, -1916.0242, -5185.8560, -6532.7422,  3552.1353,\n",
      "          4578.0435, -4627.3940],\n",
      "        [  536.9038,  -377.6370,   659.2109,  -750.6229,   720.9370,   479.3211,\n",
      "           639.0705,  -752.9842,   582.3629,   677.5745,   686.3922,  -591.8930,\n",
      "           285.5779,   529.2446,  -302.8542,  -617.0792,  -896.7800,   478.9225,\n",
      "           482.8815,  -665.8950],\n",
      "        [-1209.6470,   902.8505, -1407.5480,  1574.7595, -1436.4578, -1024.2549,\n",
      "         -1755.9299,  1536.6873, -1126.2620, -1795.7119, -1467.5363,  1383.2760,\n",
      "          -868.5925, -1461.8988,   583.3596,  1571.0334,  1663.9501, -1133.6909,\n",
      "         -1478.7061,  1375.7283],\n",
      "        [-6967.1196,  4484.2227, -8991.4277,  9805.6650, -7057.0366, -4791.8638,\n",
      "         -6955.7153,  8027.4277, -5809.8560, -9713.2725, -7083.2715,  7444.3257,\n",
      "         -5011.1362, -7075.6812,  2532.5776,  8810.1152, 10015.0576, -5875.5322,\n",
      "         -7409.9824,  7821.9204],\n",
      "        [ 4033.9524, -3372.4854,  5768.5728, -6548.7930,  5584.5952,  3396.1824,\n",
      "          5728.5254, -5986.0601,  3915.5605,  5396.1514,  4968.0684, -4585.2271,\n",
      "          2754.1025,  4179.5405, -1854.9147, -4828.4956, -5647.5405,  3618.7590,\n",
      "          3661.1587, -4439.5562],\n",
      "        [  673.3594,  -389.3434,  1383.0806, -1142.4772,  1168.3480,   598.7060,\n",
      "           977.8354,  -973.7015,   812.8077,  1080.0337,  1013.2381, -1035.0837,\n",
      "           809.3218,  1021.6754,  -439.9286, -1309.5800, -1705.4911,   987.9565,\n",
      "          1109.8076, -1198.5731],\n",
      "        [-3752.4224,  2782.4829, -5210.2046,  5211.6826, -5213.9326, -3077.8398,\n",
      "         -5001.8828,  4586.6768, -3874.6306, -4756.3257, -4274.3516,  4015.9661,\n",
      "         -2667.8633, -3752.5061,  1571.9628,  4190.7129,  6206.4731, -3492.1284,\n",
      "         -4026.8452,  4642.2656],\n",
      "        [  733.4607,  -580.3495,  1090.0958, -1056.0371,  1014.8911,   618.4263,\n",
      "          1051.2070,  -813.7404,   733.7960,   874.7456,   765.3328,  -660.2557,\n",
      "           414.3397,   720.9240,  -358.3124,  -921.5447,  -895.1777,   664.1302,\n",
      "           742.8157,  -890.5822],\n",
      "        [-3467.5422,  2472.9619, -4318.2769,  4033.5312, -4121.3311, -2563.1790,\n",
      "         -3288.4929,  4190.0957, -3243.4714, -4442.4033, -3399.9980,  3509.9165,\n",
      "         -1954.0195, -2877.1753,  1357.5012,  3823.2161,  4580.8428, -2640.2537,\n",
      "         -2646.1577,  2945.7439]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-9132.2354,  8914.7715,  9377.4873, -8902.2168, -9084.6865,  9098.8086,\n",
      "          9123.3086, -9085.3281,  9073.5264, -9046.4023],\n",
      "        [ 6510.5015, -6305.5044, -6657.6377,  6265.8926,  6453.8647, -6452.7358,\n",
      "         -6453.2622,  6450.2397, -6406.9507,  6397.0410],\n",
      "        [  734.0866,  -644.8965,  -728.8589,   593.9992,   709.3296,  -692.4474,\n",
      "          -662.7989,   701.5176,  -643.2711,   657.8690],\n",
      "        [-1905.6195,  1859.9728,  1958.3032, -1855.7211, -1896.6986,  1898.8036,\n",
      "          1903.0023, -1896.4607,  1892.8213, -1887.3962],\n",
      "        [-9630.8984,  9569.5264,  9940.8584, -9664.8164, -9622.9209,  9686.6260,\n",
      "          9777.8428, -9645.8906,  9765.0459, -9700.9365],\n",
      "        [ 5816.8931, -5542.8457, -5921.7812,  5446.6997,  5742.8882, -5717.9448,\n",
      "         -5680.7402,  5730.7100, -5618.3491,  5628.0215],\n",
      "        [ 1531.9141, -1541.2906, -1596.8209,  1558.1522,  1545.3687, -1555.3275,\n",
      "         -1569.7163,  1545.7053, -1572.6959,  1562.3134],\n",
      "        [-5599.4453,  5524.7729,  5777.0566, -5556.8633, -5595.7642,  5613.0039,\n",
      "          5656.1460, -5601.1826,  5635.7476, -5605.4238],\n",
      "        [  969.1420,  -869.7067,  -968.4005,   816.8576,   940.9520,  -923.8354,\n",
      "          -893.9789,   933.2335,  -872.0386,   886.0624],\n",
      "        [-4198.9624,  3927.1135,  4248.1753, -3809.8528, -4122.4941,  4086.2551,\n",
      "          4029.7222, -4105.7061,  3968.4578, -3991.8599]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([10228.4766, -7359.2070,  -972.0071,  2142.6230, 10427.0898, -6776.2368,\n",
      "        -1666.1707,  6170.0566, -1241.7313,  5037.6436], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([10228.4766, -7359.2070,  -972.0071,  2142.6230, 10427.0898, -6776.2368,\n",
      "        -1666.1707,  6170.0566, -1241.7313,  5037.6436], device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 479712.3438, -196052.0469, -476616.5938, -480102.0938,  476002.9375,\n",
      "         -436308.4688,  479018.8438,  478772.8438, -205206.0625,  479211.9688,\n",
      "          460078.3125, -466930.5000, -479357.4688,  478315.3125,  463223.2188,\n",
      "         -468345.0625, -476724.6875,  465238.2812, -478554.2500,  473010.4688],\n",
      "        [ -18093.9785,    7405.8706,   17976.5078,   18108.6230,  -17954.4570,\n",
      "           16446.3320,  -18068.4121,  -18058.8438,    7678.9434,  -18075.5938,\n",
      "          -17365.6211,   17619.2559,   18082.1895,  -18042.0918,  -17477.3613,\n",
      "           17674.4316,   17983.1680,  -17552.4121,   18052.5117,  -17848.0781],\n",
      "        [ -25574.3242,   10557.4258,   25409.4043,   25592.9668,  -25376.4023,\n",
      "           23270.5273,  -25538.1406,  -25522.4180,   11293.3574,  -25548.8262,\n",
      "          -24554.5039,   24904.0996,   25555.5391,  -25498.7695,  -24703.4414,\n",
      "           24981.4570,   25422.0762,  -24813.4453,   25513.5469,  -25225.9688],\n",
      "        [ -12132.4512,    4919.5986,   12054.3359,   12141.7822,  -12038.8369,\n",
      "           11051.9697,  -12115.4668,  -12108.1924,    5207.0557,  -12119.9980,\n",
      "          -11643.7070,   11811.9209,   12123.5811,  -12096.8271,  -11716.3828,\n",
      "           11848.9980,   12058.2646,  -11768.3691,   12103.4941,  -11965.0449],\n",
      "        [  -9282.2275,    3770.0935,    9221.6943,    9289.8799,   -9210.7764,\n",
      "            8440.3643,   -9269.0889,   -9264.2842,    3881.5625,   -9272.6230,\n",
      "           -8899.2637,    9033.7285,    9275.2451,   -9255.0439,   -8963.8047,\n",
      "            9061.2852,    9224.6406,   -9001.7422,    9259.6719,   -9151.6533],\n",
      "        [ -37538.3516,   15325.5439,   37292.7812,   37568.2656,  -37249.1250,\n",
      "           34124.9844,  -37485.5820,  -37465.2773,   15823.0410,  -37500.2188,\n",
      "          -36028.7656,   36555.4688,   37514.6602,  -37431.0234,  -36261.0039,\n",
      "           36671.5117,   37311.9102,  -36422.0820,   37454.0391,  -37029.1250],\n",
      "        [  -7099.7046,    2924.5254,    7053.9873,    7105.0786,   -7044.6299,\n",
      "            6452.5972,   -7089.5566,   -7085.3599,    3115.8914,   -7092.4399,\n",
      "           -6804.3579,    6906.3706,    7092.8804,   -7077.7988,   -6854.4512,\n",
      "            6927.8882,    7055.8926,   -6883.6040,    7080.7666,   -6996.9854],\n",
      "        [ -11260.3193,    4588.5654,   11187.2402,   11269.7256,  -11173.8213,\n",
      "           10236.8594,  -11244.3281,  -11238.6348,    4741.2632,  -11248.7002,\n",
      "          -10800.9941,   10961.2871,   11252.7109,  -11228.0371,  -10874.9834,\n",
      "           10995.6953,   11190.7529,  -10921.9775,   11234.2676,  -11105.0127],\n",
      "        [ -46163.1289,   18899.9082,   45860.5625,   46198.3711,  -45805.6758,\n",
      "           42001.2734,  -46098.6016,  -46072.1328,   19589.2246,  -46116.9180,\n",
      "          -44285.0352,   44932.6133,   46126.4531,  -46025.7305,  -44577.8281,\n",
      "           45073.3945,   45883.1914,  -44775.6367,   46050.0859,  -45517.8828],\n",
      "        [ -45070.1445,   18451.8613,   44776.6680,   45105.2695,  -44722.5117,\n",
      "           40996.7578,  -45007.0859,  -44981.8320,   19205.9551,  -45024.9883,\n",
      "          -43279.9219,   43894.1797,   45040.9727,  -44940.6367,  -43528.3359,\n",
      "           44030.3516,   44797.9141,  -43730.7734,   44967.8203,  -44459.9219],\n",
      "        [ -12553.9092,    5149.1777,   12473.4980,   12563.3760,  -12456.8447,\n",
      "           11423.8877,  -12536.0107,  -12528.5410,    5523.6880,  -12541.1064,\n",
      "          -12043.7549,   12221.7988,   12544.3906,  -12516.5264,  -12126.3896,\n",
      "           12260.6748,   12477.2051,  -12176.8135,   12523.6182,  -12381.4355],\n",
      "        [ -21604.7344,    8804.8271,   21466.1699,   21621.5742,  -21438.1445,\n",
      "           19649.6719,  -21574.1133,  -21561.6426,    9333.0586,  -21582.3945,\n",
      "          -20735.0156,   21036.8594,   21589.5742,  -21542.0391,  -20866.0352,\n",
      "           21101.2988,   21472.2012,  -20957.5332,   21553.7715,  -21308.7266],\n",
      "        [ -14534.9141,    5954.5181,   14440.4580,   14546.8330,  -14422.7529,\n",
      "           13204.4199,  -14514.1768,  -14506.8379,    6174.2451,  -14520.0020,\n",
      "          -13944.2422,   14151.1377,   14524.9980,  -14492.7529,  -14034.5410,\n",
      "           14195.7471,   14445.1816,  -14098.9834,   14500.8203,  -14334.5791],\n",
      "        [ -39492.3398,   16001.2744,   39238.5938,   39522.2930,  -39188.1055,\n",
      "           35931.6367,  -39437.0898,  -39412.8711,   16989.3945,  -39451.4453,\n",
      "          -37915.9922,   38459.8867,   39465.9297,  -39377.8984,  -38148.0039,\n",
      "           38581.5273,   39252.3281,  -38314.6367,   39401.3867,  -38959.0156],\n",
      "        [ -17434.1289,    7099.5571,   17321.7363,   17447.7773,  -17299.7324,\n",
      "           15855.5469,  -17409.4727,  -17399.5605,    7494.8242,  -17416.2031,\n",
      "          -16730.9375,   16973.6191,   17421.8047,  -17383.2305,  -16835.4648,\n",
      "           17028.2812,   17327.5195,  -16913.1289,   17393.5059,  -17194.8809],\n",
      "        [ -35909.3047,   14788.9238,   35675.3086,   35936.1367,  -35630.5469,\n",
      "           32659.8906,  -35858.9141,  -35837.5625,   15514.0830,  -35873.3086,\n",
      "          -34449.9023,   34954.6094,   35879.9023,  -35801.6523,  -34683.3359,\n",
      "           35062.8945,   35691.4453,  -34828.6875,   35819.8281,  -35410.2188],\n",
      "        [   7942.5649,   -3310.1309,   -7874.7930,   -7929.5449,    7884.8711,\n",
      "           -7262.7500,    7949.4810,    7916.0303,   -3779.9517,    7946.4028,\n",
      "            7874.6650,   -7832.4922,   -7939.8687,    7908.3462,    7736.9907,\n",
      "           -7909.8379,   -7959.0728,    7805.6211,   -7940.1450,    7920.7363],\n",
      "        [ -14627.7080,    5955.8501,   14534.0693,   14638.3496,  -14514.5693,\n",
      "           13321.7842,  -14607.1631,  -14597.6807,    6398.0845,  -14612.6768,\n",
      "          -14027.5791,   14233.1787,   14614.0879,  -14582.6270,  -14121.9717,\n",
      "           14277.8076,   14537.7139,  -14185.0928,   14589.6006,  -14418.6924],\n",
      "        [ -21502.4277,    8791.4863,   21362.8633,   21519.6348,  -21336.6387,\n",
      "           19541.0664,  -21471.9238,  -21460.4277,    9172.5957,  -21480.4258,\n",
      "          -20636.1680,   20937.7227,   21488.1777,  -21440.9121,  -20769.8301,\n",
      "           21003.2168,   21371.8164,  -20860.6660,   21453.2188,  -21207.7812],\n",
      "        [ -32153.9844,   13160.6426,   31943.6484,   32178.5586,  -31904.8730,\n",
      "           29254.5234,  -32109.2812,  -32090.3496,   13661.2656,  -32121.6504,\n",
      "          -30846.3223,   31292.2910,   32126.6797,  -32057.1465,  -31041.9414,\n",
      "           31391.0547,   31956.9961,  -31183.4004,   32073.4570,  -31699.4199],\n",
      "        [ -11091.8740,    4494.8950,   11020.9434,   11100.5156,  -11006.7734,\n",
      "           10094.0977,  -11076.1436,  -11069.6348,    4790.9409,  -11080.2578,\n",
      "          -10645.4609,   10799.0723,   11084.2285,  -11059.5615,  -10711.0576,\n",
      "           10834.5234,   11023.5117,  -10760.5352,   11066.3721,  -10939.1895],\n",
      "        [ -19698.9258,    8083.0693,   19571.0352,   19713.9082,  -19546.4844,\n",
      "           17911.8926,  -19671.1914,  -19659.6328,    8484.5977,  -19678.9824,\n",
      "          -18903.0625,   19176.0098,   19683.3789,  -19640.3242,  -19024.5820,\n",
      "           19237.2246,   19578.6543,  -19106.4453,   19650.7637,  -19425.3789],\n",
      "        [ -18996.1270,    7802.1758,   18873.1953,   19010.5762,  -18848.6367,\n",
      "           17289.3652,  -18969.1895,  -18958.1758,    8238.9082,  -18976.9062,\n",
      "          -18218.1582,   18488.5000,   18980.5918,  -18938.9102,  -18343.6250,\n",
      "           18546.9766,   18879.5859,  -18423.4062,   18948.9883,  -18729.9277],\n",
      "        [ -15839.5068,    6432.1816,   15737.0137,   15852.5332,  -15717.6230,\n",
      "           14411.7354,  -15817.1875,  -15808.7959,    6673.8628,  -15823.1113,\n",
      "          -15193.8184,   15420.1191,   15828.9424,  -15793.8096,  -15295.5479,\n",
      "           15468.3535,   15741.2275,  -15364.3643,   15802.8604,  -15621.7910]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-480894.0000,   18137.7969,   25631.9453,   12161.1035,    9305.2471,\n",
      "          37628.6406,    7116.2705,   11288.3994,   46270.8711,   45176.3555,\n",
      "          12583.0791,   21656.0273,   14570.5186,   39584.6523,   17475.6309,\n",
      "          35991.8164,   -7911.1113,   14661.0596,   21554.2891,   32228.9961,\n",
      "          11118.4248,   19744.7539,   19040.4180,   15878.6182],\n",
      "       device='cuda:0')\n",
      "xent mean is: 360.0035095214844\n",
      "kl mean is: 66.20541381835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 2 | Training Loss: 27591.59765625, Training Accuracy: 0.0, Validation Loss: 26630.046875, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 950.6806, -840.3183, -494.9542, -223.2865,  733.9023, -817.5726,\n",
      "        -555.7754,  117.3071,  228.5037,  119.6683], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   -78.6787,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -2290.5308,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1209.7703,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -292.7594,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1492.3713,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  2958.4617,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     5.4358,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -1030.0668,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -1031.6553,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   430.1560,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[   16.8378,  -108.2605,   122.5773,   -10.3976,    45.4967,    96.9252,\n",
      "            70.1259,  -126.6109,    43.2201,   112.7688],\n",
      "        [ 1647.0349, -1234.3667,  1918.3098, -2235.5408,  2085.4482,  1265.3945,\n",
      "          1921.8054, -1908.1781,  1555.6359,  2034.2620],\n",
      "        [ -869.1707,   636.2650, -1004.1832,  1186.2095, -1101.5228,  -657.2723,\n",
      "         -1011.6865,   996.5554,  -818.3741, -1064.5732],\n",
      "        [  273.5381,  -128.2428,   199.6953,  -361.5035,   305.7264,   130.5191,\n",
      "           254.3018,  -202.1215,   228.5884,   239.1763],\n",
      "        [-1160.9753,   789.1246, -1201.5627,  1552.4932, -1413.0276,  -799.9163,\n",
      "         -1269.6492,  1204.4138, -1059.8773, -1312.8181],\n",
      "        [-2186.0640,  1688.3507, -2509.2358,  2906.8428, -2723.8794, -1690.5934,\n",
      "         -2514.2917,  2513.8733, -2057.1875, -2684.4768],\n",
      "        [  -35.5280,   -23.2891,    25.1263,    47.9807,   -25.0778,    20.7455,\n",
      "            -6.6251,   -24.7682,   -16.6950,    12.8037],\n",
      "        [  844.2385,  -546.0052,   811.7916, -1113.4425,  1000.3621,   545.5748,\n",
      "           886.3327,  -818.2254,   754.9836,   905.4037],\n",
      "        [  699.1448,  -620.6689,   921.9235,  -936.8984,   910.5945,   622.5836,\n",
      "           868.5892,  -920.2911,   687.8419,   958.4748],\n",
      "        [ -328.0851,   186.9570,  -324.9346,   457.3110,  -404.9407,  -202.9094,\n",
      "          -356.3246,   319.6462,  -294.8401,  -353.1443]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([  189.2564, -2524.5728,  1369.3190,  -578.6583,  1939.3313,  3199.2954,\n",
      "          162.3824, -1458.8219,  -853.4437,   638.9075], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([  189.2564, -2524.5728,  1369.3190,  -578.6583,  1939.3313,  3199.2954,\n",
      "          162.3824, -1458.8219,  -853.4437,   638.9075], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  2346.9097,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -701.7056,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1811.6050,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   899.2505,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -7115.1992,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -470.4568,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -384.7701,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  3419.3188,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   497.6570,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -702.4293,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[-1.9199e+03,  1.7534e+03, -1.2756e+03, -1.7143e+03,  6.4062e+02,\n",
      "          2.0106e+03,  2.2489e+03, -1.4517e+03, -1.6985e+03,  1.8292e+03],\n",
      "        [ 7.2957e+02, -7.0132e+02,  5.2090e+02,  7.1189e+02, -2.7368e+02,\n",
      "         -7.8529e+02, -8.9662e+02,  5.6021e+02,  6.5585e+02, -7.1873e+02],\n",
      "        [-1.3243e+03,  1.0839e+03, -7.3716e+02, -9.6036e+02,  3.4258e+02,\n",
      "          1.3049e+03,  1.4037e+03, -9.7332e+02, -1.1277e+03,  1.1766e+03],\n",
      "        [-5.4549e+02,  3.5473e+02, -2.0024e+02, -2.3382e+02,  6.9062e+01,\n",
      "          4.7807e+02,  4.7061e+02, -3.8071e+02, -4.3266e+02,  4.2275e+02],\n",
      "        [ 5.8130e+03, -5.4064e+03,  3.9783e+03,  5.3649e+03, -2.0112e+03,\n",
      "         -6.1537e+03, -6.9218e+03,  4.4153e+03,  5.1798e+03, -5.6041e+03],\n",
      "        [ 2.4416e+02, -1.2064e+02,  4.9017e+01,  3.8293e+01,  1.7447e+00,\n",
      "         -1.9023e+02, -1.6496e+02,  1.6106e+02,  1.8181e+02, -1.6336e+02],\n",
      "        [ 2.0945e+02, -1.2559e+02,  6.4153e+01,  7.0770e+01, -1.9105e+01,\n",
      "         -1.7639e+02, -1.6858e+02,  1.4417e+02,  1.6199e+02, -1.5521e+02],\n",
      "        [-2.7047e+03,  2.4742e+03, -1.8065e+03, -2.4239e+03,  8.9838e+02,\n",
      "          2.8369e+03,  3.1703e+03, -2.0433e+03, -2.3973e+03,  2.5790e+03],\n",
      "        [-2.5757e+02,  1.4568e+02, -7.2156e+01, -7.2757e+01,  1.3003e+01,\n",
      "          2.1249e+02,  1.9575e+02, -1.7408e+02, -1.9801e+02,  1.8478e+02],\n",
      "        [ 3.9424e+02, -2.3149e+02,  1.1880e+02,  1.2595e+02, -2.8752e+01,\n",
      "         -3.3035e+02, -3.1037e+02,  2.6919e+02,  3.0516e+02, -2.8876e+02]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([ 2488.4885,  -879.5411,  1958.2026,   982.0417, -7345.9868,  -512.3436,\n",
      "         -397.4069,  3498.6504,   505.2534,  -756.6721], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([ 2488.4885,  -879.5411,  1958.2026,   982.0417, -7345.9868,  -512.3436,\n",
      "         -397.4069,  3498.6504,   505.2534,  -756.6721], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[  -119.9322,     24.4957,   -281.2180,    299.0807,   -222.2733,\n",
      "            -98.9359,   -249.9552,    217.4946,   -146.8761,   -253.3382,\n",
      "           -270.0052,    232.6498,   -230.4950,   -317.4669,    128.4122,\n",
      "            295.1521,    311.5309,   -270.8211,   -264.1355,    292.0580],\n",
      "        [  9430.1426,  -7309.4780,  10738.5869, -12333.5244,  11598.3926,\n",
      "           7353.9082,  11009.1357, -11528.4268,   9093.1182,  11226.8613,\n",
      "          10768.3721,  -9721.7998,   7292.5059,  10005.8662,  -3691.0845,\n",
      "         -11296.9307, -12841.6016,   7554.1323,   9079.5488, -10374.0479],\n",
      "        [   511.6583,   -405.3223,    571.9859,   -754.3195,    562.2103,\n",
      "            424.1797,    637.5563,   -585.7215,    466.3334,    743.7091,\n",
      "            585.8510,   -640.5942,    408.7855,    509.1427,   -232.7208,\n",
      "           -726.9225,   -624.3286,    476.0363,    588.4474,   -618.0907],\n",
      "        [    57.5868,    -26.9546,    123.1068,   -146.8901,    122.0049,\n",
      "             63.7906,    138.2716,   -121.7981,     67.8131,    131.4246,\n",
      "            156.8618,   -128.9725,    101.1508,    127.2552,    -68.5529,\n",
      "           -151.5975,   -161.3026,    113.0952,    141.1618,   -126.4818],\n",
      "        [  -548.9828,    447.3077,   -701.7479,    807.4504,   -781.6390,\n",
      "           -408.5712,   -569.1494,    659.2865,   -482.9863,   -641.5436,\n",
      "           -591.6893,    635.1645,   -464.1785,   -662.3428,    278.1269,\n",
      "            791.8101,    882.3154,   -557.0618,   -535.4418,    601.7932],\n",
      "        [  4090.5522,  -3799.3396,   5917.8354,  -5066.8696,   4988.6777,\n",
      "           3423.3440,   5712.2534,  -4794.5273,   4521.0347,   6078.3711,\n",
      "           4721.4663,  -4406.6860,   3425.0127,   4907.1177,  -1791.9858,\n",
      "          -5471.2173,  -6134.3345,   3926.3340,   4773.3276,  -4435.5615],\n",
      "        [  -162.9534,     63.4293,   -360.0628,    337.7481,   -375.4899,\n",
      "           -171.6671,   -316.3437,    272.3691,   -195.8825,   -331.8324,\n",
      "           -342.1140,    327.2632,   -313.2990,   -374.4031,    166.4634,\n",
      "            372.8815,    485.8930,   -329.9755,   -343.0410,    347.4735],\n",
      "        [  -259.4074,    213.7791,   -417.6822,    428.4862,   -405.8731,\n",
      "           -253.1428,   -418.7237,    420.8549,   -266.1348,   -340.1776,\n",
      "           -446.0105,    320.0981,   -296.0275,   -353.5477,    172.1426,\n",
      "            381.0406,    451.4892,   -319.2157,   -368.9120,    428.7328],\n",
      "        [  4988.9834,  -3625.3647,   5431.7202,  -6884.6958,   5511.4707,\n",
      "           3701.8105,   5765.1748,  -5521.3369,   4662.0415,   6128.4946,\n",
      "           5129.4849,  -4989.3999,   3634.8860,   5257.2930,  -1997.2102,\n",
      "          -5914.5542,  -6891.2822,   3905.2327,   5088.0869,  -5550.0298],\n",
      "        [  -168.2757,     53.5880,   -353.2094,    320.2952,   -300.1494,\n",
      "           -152.9362,   -317.5691,    257.0334,   -177.0179,   -258.2765,\n",
      "           -355.3283,    281.3630,   -256.5974,   -363.9647,    160.0280,\n",
      "            381.7984,    391.8188,   -293.1433,   -322.5774,    336.8077]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[  -260.1438,    104.5717,    235.3234,    266.2556,   -237.1671,\n",
      "            208.5957,   -213.5607,   -230.8157,     92.9250,   -192.8185],\n",
      "        [ 12686.4814,  -7322.0771, -12599.0039, -12701.4863,  12597.8652,\n",
      "         -11846.7881,  12608.4092,  12629.9795,  -7587.7290,  12580.4346],\n",
      "        [   742.0065,   -415.7971,   -728.2565,   -745.0790,    728.8900,\n",
      "           -678.8552,    720.6166,    728.2148,   -419.4764,    711.4714],\n",
      "        [   134.4039,    -56.0239,   -122.7058,   -137.2875,    123.5643,\n",
      "           -109.4679,    112.5288,    120.6335,    -49.6962,    102.7950],\n",
      "        [  -817.9261,    451.0405,    802.4561,    821.5042,   -803.0143,\n",
      "            749.8541,   -793.1844,   -802.0048,    476.9448,   -782.5898],\n",
      "        [  6234.1763,  -3643.6628,  -6181.2988,  -6243.7852,   6181.0303,\n",
      "          -5795.0513,   6175.6772,   6193.4927,  -3833.5310,   6153.2358],\n",
      "        [  -369.1046,    164.0502,    343.1953,    375.6166,   -344.9901,\n",
      "            309.9105,   -320.7900,   -338.8003,    156.5743,   -299.3878],\n",
      "        [  -460.9340,    239.4041,    445.5432,    464.6835,   -446.4462,\n",
      "            412.6252,   -433.9914,   -443.8958,    240.4750,   -422.1991],\n",
      "        [  6649.0171,  -3826.1191,  -6601.6460,  -6657.5889,   6600.6631,\n",
      "          -6205.6089,   6603.4653,   6616.6094,  -4030.5139,   6586.8423],\n",
      "        [  -319.5269,    134.0344,    292.5442,    326.1642,   -294.5315,\n",
      "            261.5775,   -269.1687,   -287.8189,    118.4024,   -246.7669]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([   401.5353, -12911.1895,   -806.2812,   -200.8006,    892.2962,\n",
      "         -6404.6396,    515.6863,    542.4550,  -6780.5850,    472.3600],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([   401.5353, -12911.1895,   -806.2812,   -200.8006,    892.2962,\n",
      "         -6404.6396,    515.6863,    542.4550,  -6780.5850,    472.3600],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[-2931.0027,  2231.3201, -3625.1594,  3884.3953, -4102.9023, -2714.2275,\n",
      "         -3741.4573,  3922.9419, -3080.8127, -3734.1604, -3758.8401,  3557.9902,\n",
      "         -2055.0200, -2913.2019,  1480.9124,  3303.5457,  3279.5137, -2950.5967,\n",
      "         -3197.3933,  2848.8909],\n",
      "        [ 2434.2935, -1771.5653,  2277.2236, -2843.8318,  2529.0815,  1767.7803,\n",
      "          2775.8760, -2568.1433,  2088.9375,  2953.6033,  2605.1711, -2306.6138,\n",
      "          1392.1898,  1982.0424,  -980.2034, -2254.0198, -2786.6316,  1636.1796,\n",
      "          2025.6621, -2023.6840],\n",
      "        [  240.5107,  -181.2182,   275.0014,  -306.4897,   296.6477,   221.9155,\n",
      "           267.4395,  -313.2013,   260.3237,   281.7360,   286.1423,  -250.8530,\n",
      "           121.6904,   221.6055,  -145.6564,  -252.9015,  -363.1902,   207.9438,\n",
      "           199.3927,  -277.4518],\n",
      "        [ -535.0142,   433.0042,  -583.9894,   640.6612,  -586.3304,  -467.2591,\n",
      "          -738.9963,   635.5689,  -491.1378,  -748.1295,  -616.6927,   593.4036,\n",
      "          -387.7708,  -632.0013,   284.5935,   653.8435,   675.6254,  -503.3564,\n",
      "          -629.7888,   577.2765],\n",
      "        [-3427.5310,  2307.8745, -4135.0029,  4375.9385, -3126.6982, -2362.2747,\n",
      "         -3139.4614,  3628.3005, -2779.1514, -4443.7515, -3237.1326,  3528.4575,\n",
      "         -2513.1882, -3353.3027,  1335.7570,  4045.6162,  4455.5562, -2889.4658,\n",
      "         -3466.2717,  3616.0984],\n",
      "        [ 1938.7324, -1770.7910,  2629.6838, -2897.2778,  2481.0002,  1683.9492,\n",
      "          2617.0664, -2705.2454,  1865.4910,  2428.2812,  2246.4961, -2121.9526,\n",
      "          1319.9607,  1920.3451,  -974.1481, -2151.8301, -2464.4658,  1733.4130,\n",
      "          1640.4891, -1997.1240],\n",
      "        [  270.4885,  -161.3552,   561.8627,  -447.5716,   459.0820,   249.6926,\n",
      "           388.6049,  -382.3508,   334.1753,   426.2671,   413.7131,  -435.6148,\n",
      "           362.9270,   429.4589,  -208.4105,  -537.0367,  -682.6210,   434.7863,\n",
      "           463.1446,  -495.7798],\n",
      "        [-1802.4854,  1432.6428, -2358.0056,  2281.1267, -2306.3235, -1519.8990,\n",
      "         -2258.7966,  2039.1569, -1854.7589, -2119.8901, -1921.8097,  1849.2173,\n",
      "         -1289.1591, -1721.6658,   820.2577,  1857.7930,  2718.0398, -1685.2465,\n",
      "         -1836.1304,  2108.0264],\n",
      "        [  357.3304,  -306.4480,   501.4612,  -470.3271,   455.0041,   311.0647,\n",
      "           484.3327,  -365.4430,   355.0505,   394.1408,   344.3032,  -301.7554,\n",
      "           197.5898,   332.0067,  -184.9127,  -414.1675,  -392.9482,   318.5504,\n",
      "           337.9156,  -404.8896],\n",
      "        [-1706.9258,  1302.2805, -1977.6481,  1784.7122, -1841.8585, -1281.6146,\n",
      "         -1484.2632,  1894.3740, -1580.9266, -2022.8807, -1532.9142,  1629.6409,\n",
      "          -926.8454, -1310.3280,   710.0192,  1707.9717,  2008.5555, -1262.9886,\n",
      "         -1182.9980,  1317.0967]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-4137.9556,  3981.1267,  4170.1670, -3899.5391, -4093.0188,  4073.0381,\n",
      "          4040.7378, -4087.0447,  3993.4128, -4017.7058],\n",
      "        [ 2770.1765, -2635.8013, -2778.7751,  2561.2732,  2729.0852, -2708.9482,\n",
      "         -2676.1589,  2722.3560, -2636.7388,  2660.3687],\n",
      "        [  290.0541,  -242.9154,  -279.6963,   210.8100,   275.8659,  -265.7338,\n",
      "          -247.9373,   271.5728,  -234.9342,   246.7900],\n",
      "        [ -776.6441,   746.5317,   783.0225,  -730.2109,  -768.3953,   764.2173,\n",
      "           757.6284,  -767.0561,   748.5505,  -753.4092],\n",
      "        [-4315.7271,  4250.6235,  4381.6777, -4234.4077, -4297.3501,  4302.1489,\n",
      "          4307.5146, -4302.3530,  4285.5771, -4284.7124],\n",
      "        [ 2526.5046, -2353.0142, -2517.9104,  2248.6333,  2474.4390, -2443.2034,\n",
      "         -2392.2932,  2462.8699, -2342.2661,  2377.3625],\n",
      "        [  617.8925,  -615.0490,  -632.3296,   613.9139,   619.6390,  -620.7417,\n",
      "          -621.8994,   619.8001,  -620.4567,   619.9839],\n",
      "        [-2457.4124,  2393.7087,  2488.3552, -2366.6172, -2441.6267,  2435.3743,\n",
      "          2429.4941, -2440.8174,  2408.6372, -2414.6843],\n",
      "        [  420.4514,  -364.4253,  -409.6045,   327.3240,   403.4273,  -391.8259,\n",
      "          -371.6556,   398.6726,  -356.0808,   369.5363],\n",
      "        [-1812.9568,  1643.8608,  1790.8865, -1537.4242, -1761.5570,  1728.4734,\n",
      "          1673.5685, -1748.5712,  1625.6765, -1663.0813]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([ 4634.6792, -3152.1833,  -398.2888,   873.8219,  4632.3516, -2981.1426,\n",
      "         -663.9221,  2697.1633,  -551.2615,  2227.1655], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([ 4634.6792, -3152.1833,  -398.2888,   873.8219,  4632.3516, -2981.1426,\n",
      "         -663.9221,  2697.1633,  -551.2615,  2227.1655], device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 318404.0000, -185726.5312, -317104.4688, -318579.6562,  316969.1250,\n",
      "         -298284.4062,  318136.8438,  318056.8125, -194046.3281,  318205.6250,\n",
      "          310417.4062, -313383.8125, -318293.3750,  317883.7188,  311652.9375,\n",
      "         -313721.6250, -317341.9688,  312828.8750, -317939.6250,  315606.8438],\n",
      "        [ -12398.3125,    7243.3242,   12347.2744,   12404.8311,  -12342.4980,\n",
      "           11610.3652,  -12388.2725,  -12384.7500,    7535.9312,  -12390.9199,\n",
      "          -12095.6426,   12207.5625,   12394.6113,  -12378.2080,  -12138.8945,\n",
      "           12221.8203,   12358.2148,  -12184.2744,   12381.3057,  -12293.7471],\n",
      "        [ -18956.3516,   11135.2275,   18878.8691,   18965.2246,  -18870.8535,\n",
      "           17765.4668,  -18941.3242,  -18934.6562,   11766.2891,  -18945.5508,\n",
      "          -18498.2207,   18664.9160,   18949.3496,  -18924.2520,  -18560.3262,\n",
      "           18686.6523,   18896.7539,  -18630.7051,   18929.0234,  -18796.0117],\n",
      "        [  -8222.6924,    4783.3223,    8189.1479,    8226.8701,   -8185.6885,\n",
      "            7709.9844,   -8216.0732,   -8213.4805,    5025.3071,   -8217.7451,\n",
      "           -8021.7104,    8095.3906,    8219.9404,   -8209.0957,   -8049.7114,\n",
      "            8104.8560,    8196.0625,   -8080.3574,    8211.0508,   -8152.3887],\n",
      "        [  -6080.6265,    3536.7615,    6055.5308,    6083.9556,   -6053.3003,\n",
      "            5695.0771,   -6075.6621,   -6074.0654,    3666.1782,   -6076.9126,\n",
      "           -5928.7026,    5985.3164,    6078.6016,   -6070.6880,   -5952.5010,\n",
      "            5991.9595,    6060.6543,   -5974.6416,    6071.9307,   -6027.7041],\n",
      "        [ -25414.6250,   14826.5996,   25309.4180,   25427.9336,  -25300.2988,\n",
      "           23800.8105,  -25394.0898,  -25386.8320,   15392.2607,  -25399.3730,\n",
      "          -24794.6875,   25024.6113,   25407.4707,  -25373.7246,  -24883.4570,\n",
      "           25054.4590,   25333.2480,  -24978.1934,   25380.6328,  -25200.7969],\n",
      "        [  -5036.6963,    2953.3420,    5016.1167,    5039.1602,   -5013.9165,\n",
      "            4716.8027,   -5032.6445,   -5030.9663,    3114.7070,   -5033.7329,\n",
      "           -4910.1353,    4956.3818,    5034.2466,   -5027.8564,   -4929.9839,\n",
      "            4962.0933,    5020.3242,   -4948.2588,    5028.6646,   -4991.6035],\n",
      "        [  -7521.9810,    4383.2422,    7491.0288,    7526.0830,   -7488.2114,\n",
      "            7044.3936,   -7515.8257,   -7513.8354,    4552.1045,   -7517.3950,\n",
      "           -7336.0503,    7404.9414,    7519.7114,   -7509.8560,   -7363.9092,\n",
      "            7413.5752,    7497.4209,   -7391.6016,    7511.6436,   -7457.6753],\n",
      "        [ -31807.3887,   18589.7988,   31675.4219,   31823.2773,  -31663.6484,\n",
      "           29803.3418,  -31781.9492,  -31772.0703,   19336.1523,  -31788.6914,\n",
      "          -31023.0039,   31310.4980,   31795.3242,  -31753.8281,  -31137.1367,\n",
      "           31346.9082,   31705.0586,  -31255.3770,   31760.8750,  -31531.8438],\n",
      "        [ -31339.3340,   18317.5742,   31210.1250,   31355.2461,  -31198.1895,\n",
      "           29361.0391,  -31314.2402,  -31304.7031,   19092.8926,  -31320.9180,\n",
      "          -30583.1484,   30859.5605,   31329.9336,  -31288.2793,  -30681.9961,\n",
      "           30895.2734,   31239.1465,  -30801.5332,   31296.5879,  -31076.0312],\n",
      "        [  -8986.7061,    5260.2471,    8950.1748,    8991.0859,   -8946.2090,\n",
      "            8422.1299,   -8979.4883,   -8976.4639,    5565.3716,   -8981.4268,\n",
      "           -8765.8525,    8847.3662,    8983.3320,   -8971.4707,   -8798.7861,\n",
      "            8857.9482,    8957.7900,   -8831.0498,    8973.6230,   -8910.1035],\n",
      "        [ -14793.3633,    8626.2646,   14733.1328,   14800.9170,  -14726.8096,\n",
      "           13858.5898,  -14781.4092,  -14776.7754,    9071.0830,  -14784.4521,\n",
      "          -14432.1650,   14565.3369,   14788.5322,  -14769.0859,  -14482.9854,\n",
      "           14581.8438,   14745.4443,  -14537.7363,   14772.5273,  -14667.6846],\n",
      "        [  -9896.5869,    5783.1460,    9855.8135,    9901.8701,   -9852.0059,\n",
      "            9264.7861,   -9888.4971,   -9885.8223,    6017.5625,   -9890.6504,\n",
      "           -9652.8369,    9743.3643,    9893.4775,   -9880.4004,   -9687.7520,\n",
      "            9754.8242,    9864.3623,   -9725.2852,    9882.7324,   -9812.0029],\n",
      "        [ -26471.2559,   15389.7383,   26363.3848,   26484.6270,  -26352.1504,\n",
      "           24802.9355,  -26449.9297,  -26441.4199,   16193.1426,  -26455.1426,\n",
      "          -25829.7480,   26065.6875,   26463.3535,  -26428.1035,  -25917.9863,\n",
      "           26096.6523,   26385.8789,  -26015.8730,   26435.0605,  -26249.6738],\n",
      "        [ -11820.1953,    6888.7861,   11771.8828,   11826.2686,  -11767.0000,\n",
      "           11072.7129,  -11810.6094,  -11807.0391,    7229.7607,  -11813.0742,\n",
      "          -11531.0391,   11637.2178,   11816.3320,  -11800.7266,  -11571.3203,\n",
      "           11651.3535,   11781.9482,  -11616.2744,   11803.7783,  -11719.6699],\n",
      "        [ -25657.5840,   15045.9062,   25551.6660,   25669.9824,  -25541.4434,\n",
      "           24037.5273,  -25637.1621,  -25628.6270,   15745.6309,  -25642.6328,\n",
      "          -25026.2578,   25257.7402,   25647.1230,  -25613.7695,  -25119.7539,\n",
      "           25286.7754,   25575.3301,  -25212.1094,   25619.1270,  -25436.4102],\n",
      "        [  18205.8535,  -10879.2686,  -18121.8086,  -18200.7969,   18123.7285,\n",
      "          -17073.6367,   18202.7988,   18178.1504,  -11602.1035,   18204.2617,\n",
      "           17947.7344,  -18010.5703,  -18199.7500,   18167.2656,   17886.2031,\n",
      "          -18060.8984,  -18182.5527,   17960.3730,  -18191.3672,   18125.5156],\n",
      "        [ -10136.6338,    5910.9780,   10095.4482,   10141.4785,  -10090.9121,\n",
      "            9503.7998,  -10128.5273,  -10124.9707,    6256.0508,  -10130.5869,\n",
      "           -9884.8887,    9976.5059,   10131.9834,  -10118.9658,   -9921.8242,\n",
      "            9988.1152,   10103.6553,   -9959.6250,   10120.9043,  -10047.0254],\n",
      "        [ -14682.9893,    8572.7100,   14622.6006,   14690.6572,  -14616.8857,\n",
      "           13748.4990,  -14671.0820,  -14666.8281,    8946.7217,  -14674.1709,\n",
      "          -14324.2051,   14456.8447,   14678.5430,  -14659.2490,  -14375.7129,\n",
      "           14473.7568,   14635.7744,  -14429.9521,   14662.9053,  -14558.1582],\n",
      "        [ -22088.2129,   12907.7021,   21996.7090,   22099.2344,  -21988.3223,\n",
      "           20696.1602,  -22070.5742,  -22063.5977,   13435.3936,  -22075.1426,\n",
      "          -21543.3027,   21741.2852,   22079.1406,  -22050.5801,  -21620.0684,\n",
      "           21766.7891,   22016.4863,  -21703.3926,   22055.2188,  -21894.7812],\n",
      "        [  -7469.8975,    4343.3135,    7439.5698,    7473.7397,   -7436.4219,\n",
      "            7000.1548,   -7463.8340,   -7461.5176,    4578.9355,   -7465.3223,\n",
      "           -7287.5137,    7354.3433,    7467.5601,   -7457.6636,   -7312.6367,\n",
      "            7363.5181,    7445.5728,   -7341.1074,    7459.6636,   -7406.1631],\n",
      "        [ -13786.8340,    8068.0688,   13730.1191,   13793.6641,  -13724.5820,\n",
      "           12914.0898,  -13775.7969,  -13771.3232,    8444.9932,  -13778.7148,\n",
      "          -13449.1709,   13572.2969,   13781.5508,  -13763.5547,  -13497.2168,\n",
      "           13588.5771,   13742.3711,  -13547.6602,   13766.6445,  -13668.1221],\n",
      "        [ -13509.0225,    7911.5430,   13453.5957,   13515.6533,  -13447.9102,\n",
      "           12661.2578,  -13498.1484,  -13493.8154,    8305.8955,  -13501.0918,\n",
      "          -13174.3750,   13297.4727,   13503.5352,  -13485.8789,  -13224.3613,\n",
      "           13313.1709,   13465.3115,  -13274.0273,   13488.8652,  -13391.6621],\n",
      "        [ -10532.6455,    6128.0723,   10489.4326,   10538.3447,  -10485.3320,\n",
      "            9868.5752,  -10524.0977,  -10521.2188,    6375.9849,  -10526.2197,\n",
      "          -10272.4531,   10369.2637,   10529.5615,  -10515.5967,  -10310.7617,\n",
      "           10381.2598,   10498.1133,  -10350.3496,   10518.2012,  -10442.9629]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-318896.3125,   12416.5059,   18981.8242,    8234.5820,    6089.9272,\n",
      "          25452.0312,    5043.7212,    7533.4307,   31852.5723,   31384.1562,\n",
      "           8999.1953,   14814.7109,    9911.3105,   26509.2480,   11837.3301,\n",
      "          25692.9238,  -18195.8555,   10150.6494,   14704.5391,   22119.5410,\n",
      "           7480.8267,   13806.2100,   13527.9004,   10548.5723],\n",
      "       device='cuda:0')\n",
      "xent mean is: 286.7535705566406\n",
      "kl mean is: 66.19223022460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 3 | Training Loss: 28286.86328125, Training Accuracy: 0.0, Validation Loss: 27315.478515625, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 597.5424, -519.0981, -300.8261, -134.0474,  497.4421, -469.2529,\n",
      "        -332.1410,   67.5350,  127.4453,   58.2730], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   -87.6111,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -1279.8914,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   641.3433,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -108.4165,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   694.0205,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1649.0963,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,    -4.9846,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -468.8503,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -568.6509,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   208.7789,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[   40.3520,   -84.3270,   106.8296,   -45.9676,    62.8224,    82.2460,\n",
      "            77.0646,  -107.0581,    53.2992,    98.5841],\n",
      "        [  984.7983,  -773.3921,  1095.2936, -1263.5667,  1184.7228,   776.5218,\n",
      "          1099.2983, -1081.7662,   919.0731,  1158.5872],\n",
      "        [ -494.6048,   377.0307,  -541.9047,   638.6970,  -594.9482,  -380.9467,\n",
      "          -548.7094,   533.8642,  -459.2921,  -574.3542],\n",
      "        [  119.6051,   -52.2548,    67.5997,  -149.4004,   123.2587,    48.6554,\n",
      "            97.1752,   -68.8611,    95.8735,    89.2437],\n",
      "        [ -588.9000,   410.0817,  -562.0770,   745.3697,  -676.8787,  -403.5184,\n",
      "          -604.7766,   560.2367,  -527.7482,  -621.1515],\n",
      "        [-1303.7554,  1052.6334, -1429.6603,  1638.9580, -1543.6891, -1035.5762,\n",
      "         -1435.6007,  1422.6772, -1213.0702, -1524.6445],\n",
      "        [  -15.1493,   -15.2233,    20.6815,    19.4985,    -7.7041,    15.9898,\n",
      "             3.3384,   -20.0731,    -4.9819,    12.2732],\n",
      "        [  420.8619,  -278.5621,   370.0395,  -525.8550,   470.8997,   269.0060,\n",
      "           413.2905,  -371.2384,   369.4763,   419.7831],\n",
      "        [  409.5016,  -377.1939,   520.3826,  -518.8069,   507.2502,   375.9160,\n",
      "           490.5198,  -515.6094,   398.3105,   535.3640],\n",
      "        [ -175.3506,   100.1887,  -155.7142,   232.2329,  -203.9825,  -104.3041,\n",
      "          -176.4030,   152.0251,  -153.7467,  -173.3093]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([   55.3783, -1435.9445,   747.9166,  -258.7982,   961.5009,  1815.8557,\n",
      "           78.9488,  -712.2415,  -475.8482,   339.6119], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([   55.3783, -1435.9445,   747.9166,  -258.7982,   961.5009,  1815.8557,\n",
      "           78.9488,  -712.2415,  -475.8482,   339.6119], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1147.5128,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -435.4114,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1032.3582,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   390.5092,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -3780.8162,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -221.9946,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -180.2561,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1989.6074,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   297.7525,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -244.3763,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[-9.6517e+02,  8.9331e+02, -6.7703e+02, -8.6665e+02,  3.6294e+02,\n",
      "          9.9984e+02,  1.0906e+03, -7.7053e+02, -8.6232e+02,  9.1279e+02],\n",
      "        [ 4.4913e+02, -4.3649e+02,  3.3762e+02,  4.3980e+02, -1.8850e+02,\n",
      "         -4.7756e+02, -5.3170e+02,  3.6434e+02,  4.0735e+02, -4.3962e+02],\n",
      "        [-7.7666e+02,  6.4710e+02, -4.5911e+02, -5.6839e+02,  2.3029e+02,\n",
      "          7.5904e+02,  7.9641e+02, -6.0165e+02, -6.6902e+02,  6.8317e+02],\n",
      "        [-2.1122e+02,  1.1047e+02, -4.7526e+01, -3.7169e+01,  6.0178e+00,\n",
      "          1.6529e+02,  1.4238e+02, -1.4683e+02, -1.5961e+02,  1.3902e+02],\n",
      "        [ 3.2067e+03, -3.0376e+03,  2.3361e+03,  3.0059e+03, -1.2622e+03,\n",
      "         -3.3674e+03, -3.7007e+03,  2.5768e+03,  2.8911e+03, -3.0823e+03],\n",
      "        [ 1.0213e+02, -3.5377e+01,  2.6363e+00, -1.4006e+01,  1.3693e+01,\n",
      "         -6.8936e+01, -4.8013e+01,  6.6060e+01,  7.1555e+01, -5.4289e+01],\n",
      "        [ 9.1481e+01, -4.7848e+01,  2.0171e+01,  1.5957e+01, -3.0778e+00,\n",
      "         -7.1442e+01, -6.1853e+01,  6.3738e+01,  6.8893e+01, -6.0239e+01],\n",
      "        [-1.6414e+03,  1.5322e+03, -1.1696e+03, -1.4981e+03,  6.2444e+02,\n",
      "          1.7094e+03,  1.8678e+03, -1.3124e+03, -1.4727e+03,  1.5612e+03],\n",
      "        [-1.6339e+02,  9.9817e+01, -5.4657e+01, -5.4847e+01,  1.6116e+01,\n",
      "          1.3722e+02,  1.2603e+02, -1.1697e+02, -1.2887e+02,  1.1775e+02],\n",
      "        [ 9.4437e+01, -1.0717e+01, -2.3688e+01, -5.2406e+01,  3.1176e+01,\n",
      "         -4.9909e+01, -1.9668e+01,  5.5630e+01,  5.8516e+01, -3.4559e+01]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([ 1220.3756,  -529.1094,  1118.4056,   428.1069, -3921.6177,  -241.0007,\n",
      "         -185.4494,  2051.0330,   303.8090,  -264.2839], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([ 1220.3756,  -529.1094,  1118.4056,   428.1069, -3921.6177,  -241.0007,\n",
      "         -185.4494,  2051.0330,   303.8090,  -264.2839], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[-4.9143e+01,  2.0836e+00, -1.3279e+02,  1.3593e+02, -1.0104e+02,\n",
      "         -4.3127e+01, -1.1508e+02,  9.8827e+01, -6.2928e+01, -1.1383e+02,\n",
      "         -1.3050e+02,  1.1381e+02, -1.1965e+02, -1.5802e+02,  6.9627e+01,\n",
      "          1.4173e+02,  1.4714e+02, -1.4018e+02, -1.3074e+02,  1.4254e+02],\n",
      "        [ 6.2403e+03, -5.0269e+03,  6.7843e+03, -7.6855e+03,  7.2732e+03,\n",
      "          5.0023e+03,  7.0045e+03, -7.3338e+03,  5.9701e+03,  7.0021e+03,\n",
      "          6.9759e+03, -6.3645e+03,  5.0100e+03,  6.5718e+03, -2.6851e+03,\n",
      "         -7.2375e+03, -8.0445e+03,  5.0450e+03,  5.8701e+03, -6.6996e+03],\n",
      "        [ 2.4437e+02, -2.0234e+02,  2.6932e+02, -3.4635e+02,  2.5819e+02,\n",
      "          2.1141e+02,  2.9882e+02, -2.7196e+02,  2.2051e+02,  3.4676e+02,\n",
      "          2.7920e+02, -3.1421e+02,  2.0607e+02,  2.4480e+02, -1.2542e+02,\n",
      "         -3.4714e+02, -2.9008e+02,  2.4027e+02,  2.8617e+02, -2.9517e+02],\n",
      "        [ 2.3303e+01, -8.2510e+00,  5.7562e+01, -6.6040e+01,  5.4781e+01,\n",
      "          2.8319e+01,  6.3039e+01, -5.4723e+01,  2.8746e+01,  5.8551e+01,\n",
      "          7.5809e+01, -6.3174e+01,  5.1127e+01,  6.1479e+01, -3.7200e+01,\n",
      "         -7.2443e+01, -7.5753e+01,  5.6885e+01,  6.9444e+01, -6.0484e+01],\n",
      "        [-2.4909e+02,  2.1261e+02, -3.1542e+02,  3.5360e+02, -3.4405e+02,\n",
      "         -1.8973e+02, -2.5161e+02,  2.9089e+02, -2.1622e+02, -2.8260e+02,\n",
      "         -2.6649e+02,  2.9473e+02, -2.2347e+02, -3.0815e+02,  1.4491e+02,\n",
      "          3.5985e+02,  3.9195e+02, -2.6913e+02, -2.4491e+02,  2.7227e+02],\n",
      "        [ 2.1806e+03, -2.1767e+03,  3.1088e+03, -2.5613e+03,  2.5377e+03,\n",
      "          1.9000e+03,  2.9888e+03, -2.4574e+03,  2.4304e+03,  3.1635e+03,\n",
      "          2.4657e+03, -2.3451e+03,  1.9092e+03,  2.6352e+03, -1.0627e+03,\n",
      "         -2.8657e+03, -3.1384e+03,  2.1919e+03,  2.5571e+03, -2.3111e+03],\n",
      "        [-6.9420e+01,  2.1048e+01, -1.6909e+02,  1.5354e+02, -1.6996e+02,\n",
      "         -7.8724e+01, -1.4514e+02,  1.2388e+02, -8.6020e+01, -1.4969e+02,\n",
      "         -1.6330e+02,  1.5972e+02, -1.6178e+02, -1.8324e+02,  9.0185e+01,\n",
      "          1.7716e+02,  2.2776e+02, -1.6810e+02, -1.6754e+02,  1.6676e+02],\n",
      "        [-1.1354e+02,  9.7002e+01, -1.8937e+02,  1.8815e+02, -1.7869e+02,\n",
      "         -1.1760e+02, -1.8670e+02,  1.8611e+02, -1.1725e+02, -1.4938e+02,\n",
      "         -2.0656e+02,  1.4818e+02, -1.4589e+02, -1.6489e+02,  9.0499e+01,\n",
      "          1.7333e+02,  2.0248e+02, -1.5550e+02, -1.7311e+02,  1.9779e+02],\n",
      "        [ 3.8375e+03, -2.8467e+03,  3.9680e+03, -5.0177e+03,  3.9329e+03,\n",
      "          2.8904e+03,  4.2627e+03, -4.0146e+03,  3.5397e+03,  4.4934e+03,\n",
      "          3.7866e+03, -3.7801e+03,  2.8603e+03,  4.0157e+03, -1.7081e+03,\n",
      "         -4.4052e+03, -5.0305e+03,  3.0294e+03,  3.8785e+03, -4.1720e+03],\n",
      "        [-6.9244e+01,  1.2974e+01, -1.6692e+02,  1.4570e+02, -1.3562e+02,\n",
      "         -6.7724e+01, -1.4571e+02,  1.1635e+02, -7.5319e+01, -1.1604e+02,\n",
      "         -1.7314e+02,  1.3810e+02, -1.3260e+02, -1.8142e+02,  8.8340e+01,\n",
      "          1.8487e+02,  1.8601e+02, -1.5117e+02, -1.6025e+02,  1.6381e+02]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[ -113.2056,    52.4211,    99.7102,   116.4228,  -100.7342,    88.3173,\n",
      "           -85.5900,   -97.0124,    42.9295,   -76.0178],\n",
      "        [ 7869.4878, -5694.8926, -7820.0264, -7879.0464,  7821.9805, -7490.1807,\n",
      "          7806.1221,  7829.6475, -5806.5171,  7786.4106],\n",
      "        [  338.3172,  -234.2705,  -331.1418,  -339.9531,   331.6192,  -314.0364,\n",
      "           325.0475,   330.4514,  -233.4052,   320.5399],\n",
      "        [   57.9609,   -28.0408,   -51.6058,   -59.4772,    52.0859,   -46.0594,\n",
      "            44.9777,    50.3487,   -23.4013,    40.4785],\n",
      "        [ -355.1079,   243.4522,   347.3537,   356.9197,  -347.8294,   329.6976,\n",
      "          -340.5497,  -346.4966,   247.5411,  -335.6025],\n",
      "        [ 3177.2024, -2310.5127, -3152.3994, -3182.1143,  3153.4153, -3013.7869,\n",
      "          3141.3066,  3154.9429, -2374.1890,  3129.8386],\n",
      "        [ -161.8497,    85.8399,   147.8533,   165.2396,  -148.8889,   134.2667,\n",
      "          -133.3030,  -145.1520,    77.2533,  -123.4324],\n",
      "        [ -198.6163,   126.6769,   190.4297,   200.5743,  -191.0093,   178.5140,\n",
      "          -182.4407,  -189.1228,   123.4811,  -176.8801],\n",
      "        [ 4794.2183, -3459.7175, -4762.0161, -4800.7617,  4763.1523, -4559.3525,\n",
      "          4750.4253,  4767.0205, -3556.7195,  4736.6592],\n",
      "        [ -138.4406,    67.0642,   123.3874,   142.0294,  -124.5292,   110.2210,\n",
      "          -107.7013,  -120.4163,    55.7952,   -97.0493]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([  190.4656, -8043.1484,  -375.3013,   -94.2851,   395.6968, -3275.8774,\n",
      "          241.7243,   243.8343, -4914.4971,   224.4419], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([  190.4656, -8043.1484,  -375.3013,   -94.2851,   395.6968, -3275.8774,\n",
      "          241.7243,   243.8343, -4914.4971,   224.4419], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[-1474.4915,  1167.5121, -1772.1901,  1858.9092, -1985.4133, -1436.4591,\n",
      "         -1826.5740,  1908.0596, -1551.0835, -1810.4291, -1853.7078,  1786.8611,\n",
      "         -1042.4257, -1436.3448,   829.7580,  1595.5815,  1556.6361, -1539.2540,\n",
      "         -1585.2477,  1376.5419],\n",
      "        [ 1170.9065,  -886.2574,  1043.2253, -1282.6290,  1145.8101,   873.8905,\n",
      "          1279.6860, -1171.1377,   987.5754,  1353.6163,  1202.8599, -1080.3077,\n",
      "           661.5958,   914.2941,  -514.0873, -1021.0195, -1249.8154,   780.1141,\n",
      "           935.2219,  -921.0328],\n",
      "        [  113.6948,   -88.8496,   126.3719,  -137.5889,   133.5628,   109.9237,\n",
      "           121.7175,  -142.6485,   123.4843,   127.2058,   129.9095,  -115.7433,\n",
      "            55.1328,   100.0790,   -74.6625,  -113.0286,  -162.2041,    98.3125,\n",
      "            90.0134,  -125.1624],\n",
      "        [ -246.6766,   210.8520,  -262.1302,   281.7056,  -257.8362,  -225.3582,\n",
      "          -332.8943,   282.4763,  -224.3830,  -335.5795,  -277.8506,   273.1908,\n",
      "          -181.3376,  -291.0797,   147.1724,   292.6965,   298.3094,  -239.4048,\n",
      "          -288.5267,   259.0252],\n",
      "        [-1678.0126,  1162.6790, -1958.6014,  2016.1895, -1432.4288, -1176.8693,\n",
      "         -1460.4926,  1690.7496, -1327.1971, -2084.4453, -1523.4170,  1708.0219,\n",
      "         -1262.8676, -1612.9025,   715.1956,  1907.1395,  2059.7214, -1453.1836,\n",
      "         -1666.9142,  1713.8276],\n",
      "        [  948.8768,  -924.8748,  1257.4386, -1352.4067,  1162.0709,   862.0258,\n",
      "          1247.7278, -1280.1951,   907.8834,  1144.8802,  1065.3195, -1022.5581,\n",
      "           645.9560,   913.9283,  -528.3698, -1005.5544, -1140.0397,   866.0378,\n",
      "           773.3239,  -940.0328],\n",
      "        [  115.2579,   -67.3196,   249.3079,  -191.6095,   195.5880,   111.5090,\n",
      "           169.7234,  -164.1665,   144.5844,   184.4939,   183.4826,  -198.1545,\n",
      "           172.1382,   194.1583,  -105.2467,  -238.9748,  -299.1422,   207.8250,\n",
      "           210.8468,  -222.1098],\n",
      "        [ -875.4231,   730.5460, -1112.4009,  1046.6589, -1066.8202,  -767.6192,\n",
      "         -1061.3110,   945.3843,  -897.6284,  -984.5575,  -901.4052,   883.0605,\n",
      "          -633.2759,  -813.6906,   439.8936,   859.5402,  1249.5117,  -841.8162,\n",
      "          -872.5140,   993.0873],\n",
      "        [  179.2527,  -162.8724,   245.1091,  -223.6387,   216.5920,   162.5133,\n",
      "           236.1017,  -173.7229,   177.1412,   187.6651,   165.1842,  -145.4693,\n",
      "            98.1400,   160.5499,   -99.7775,  -197.3329,  -185.8204,   161.0461,\n",
      "           163.5363,  -194.7563],\n",
      "        [ -838.0521,   667.5938,  -935.9623,   817.8911,  -849.9628,  -651.5018,\n",
      "          -689.6877,   885.5121,  -766.4535,  -946.1210,  -711.8664,   772.4256,\n",
      "          -438.7270,  -607.3637,   376.5837,   784.3734,   918.0115,  -619.1537,\n",
      "          -545.5667,   605.5853]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-1970.3915,  1872.8019,  1963.0708, -1805.9003, -1941.1497,  1922.7213,\n",
      "          1892.3512, -1934.9572,  1858.5439, -1885.1846],\n",
      "        [ 1230.2976, -1151.6550, -1218.7399,  1096.9381,  1205.9717, -1190.3195,\n",
      "         -1164.3656,  1200.3456, -1137.7498,  1159.7904],\n",
      "        [  125.0972,   -99.9614,  -118.0475,    81.1997,   117.3450,  -111.6728,\n",
      "          -101.7548,   114.9816,   -93.6158,   101.4846],\n",
      "        [ -341.0428,   323.6955,   339.8350,  -311.5760,  -335.9834,   332.5904,\n",
      "           327.0784,  -334.8178,   321.0453,  -325.8510],\n",
      "        [-1991.7618,  1947.1453,  2002.5358, -1919.9271, -1978.5500,  1973.3354,\n",
      "          1964.3850, -1978.0863,  1947.5890, -1957.4475],\n",
      "        [ 1151.3612, -1048.2832, -1130.7576,   974.7040,  1119.7349, -1097.8442,\n",
      "         -1061.4564,  1111.4077, -1027.0524,  1057.0385],\n",
      "        [  271.3190,  -267.8993,  -274.6503,   265.0217,   271.0663,  -270.6959,\n",
      "          -269.8616,   270.9811,  -268.2985,   269.3253],\n",
      "        [-1124.2958,  1083.3461,  1125.6028, -1056.4829, -1112.6515,  1105.2299,\n",
      "          1094.3701, -1110.5430,  1079.5504, -1089.8975],\n",
      "        [  194.1761,  -163.1344,  -185.9243,   140.2616,   184.5452,  -177.6770,\n",
      "          -165.7338,   181.7451,  -155.6314,   165.1638],\n",
      "        [ -804.7188,   704.9282,   780.5208,  -632.5725,  -773.8159,   751.9857,\n",
      "           715.0688,  -765.1567,   682.2582,  -712.1589]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([ 2222.2146, -1419.6372,  -178.8874,   386.4794,  2138.1565, -1388.4020,\n",
      "         -290.4940,  1239.5571,  -261.3164,  1024.8314], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([ 2222.2146, -1419.6372,  -178.8874,   386.4794,  2138.1565, -1388.4020,\n",
      "         -290.4940,  1239.5571,  -261.3164,  1024.8314], device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 209707.2969, -153519.7031, -209153.7188, -209791.1250,  209132.4531,\n",
      "         -200544.9531,  209593.2812,  209572.7969, -157440.0156,  209619.6094,\n",
      "          206231.7812, -207645.9219, -209673.0156,  209500.1719,  206836.9062,\n",
      "         -207672.3594, -209292.9688,  207431.5625, -209510.3750,  208496.5469],\n",
      "        [  -8561.2510,    6275.9326,    8538.4365,    8564.2988,   -8537.7803,\n",
      "            8187.7100,   -8556.8809,   -8555.5918,    6433.0112,   -8557.9326,\n",
      "           -8425.5752,    8480.2588,    8560.0273,   -8552.7305,   -8446.4668,\n",
      "            8482.3330,    8545.0801,   -8470.6240,    8553.8008,   -8514.6592],\n",
      "        [ -13857.4189,   10191.8398,   13820.7061,   13861.7920,  -13819.3945,\n",
      "           13260.0869,  -13850.5869,  -13847.8193,   10525.5186,  -13852.3877,\n",
      "          -13639.9854,   13726.2871,   13854.6074,  -13842.8447,  -13672.0098,\n",
      "           13729.6924,   13831.8574,  -13711.2139,   13844.5674,  -13781.6641],\n",
      "        [  -5616.3428,    4109.6694,    5601.4834,    5618.3096,   -5600.9546,\n",
      "            5374.9912,   -5613.4805,   -5612.5522,    4230.8057,   -5614.1470,\n",
      "           -5527.1812,    5562.8979,    5615.4404,   -5610.6709,   -5540.6812,\n",
      "            5564.2441,    5605.7178,   -5556.7163,    5611.3193,   -5585.3896],\n",
      "        [  -4061.6208,    2970.2263,    4050.7737,    4063.1421,   -4050.4988,\n",
      "            3884.4617,   -4059.5090,   -4058.9766,    3038.7368,   -4059.9895,\n",
      "           -3995.8350,    4022.5422,    4061.0042,   -4057.5957,   -4006.7715,\n",
      "            4023.3489,    4053.8232,   -4018.2148,    4057.9866,   -4038.8706],\n",
      "        [ -17366.0176,   12720.5225,   17319.5195,   17372.1973,  -17318.4082,\n",
      "           16608.4590,  -17357.0820,  -17354.5566,   13024.8906,  -17359.2031,\n",
      "          -17090.9082,   17202.1602,   17363.7539,  -17348.9434,  -17133.4062,\n",
      "           17206.6055,   17333.4297,  -17182.9727,   17351.2910,  -17271.7500],\n",
      "        [  -3578.2646,    2628.6470,    3568.7830,    3579.4417,   -3568.4148,\n",
      "            3422.4507,   -3576.4673,   -3575.8057,    2712.0364,   -3576.9187,\n",
      "           -3520.0916,    3543.2485,    3577.3254,   -3574.3931,   -3529.7432,\n",
      "            3544.0508,    3571.4746,   -3539.7344,    3574.6455,   -3557.6550],\n",
      "        [  -5099.3691,    3732.8411,    5085.7944,    5101.2627,   -5085.4316,\n",
      "            4876.8677,   -5096.7334,   -5096.0444,    3822.5933,   -5097.3262,\n",
      "           -5017.6118,    5050.6426,    5098.6636,   -5094.3682,   -5030.7222,\n",
      "            5051.8379,    5089.6597,   -5045.1758,    5094.9502,   -5071.2881],\n",
      "        [ -22071.4297,   16184.8516,   22012.2148,   22078.9316,  -22010.7129,\n",
      "           21115.4902,  -22060.2773,  -22056.6133,   16588.6152,  -22063.0410,\n",
      "          -21718.3066,   21859.5820,   22067.1699,  -22048.6387,  -21773.5547,\n",
      "           21864.7461,   22029.7969,  -21836.4258,   22051.0254,  -21948.2109],\n",
      "        [ -21844.2363,   16019.5557,   21785.9414,   21851.7793,  -21784.3125,\n",
      "           20896.8594,  -21833.2090,  -21829.6543,   16435.0938,  -21835.9336,\n",
      "          -21501.5527,   21638.4570,   21841.0156,  -21822.3496,  -21550.9219,\n",
      "           21643.6777,   21803.3535,  -21614.1074,   21825.2695,  -21725.7363],\n",
      "        [  -6405.8174,    4702.6641,    6388.8989,    6407.9214,   -6388.2256,\n",
      "            6129.3765,   -6402.6011,   -6401.4077,    4858.6807,   -6403.3984,\n",
      "           -6303.7495,    6344.7363,    6404.5234,   -6399.0913,   -6319.9824,\n",
      "            6346.3940,    6393.7734,   -6337.7563,    6399.8462,   -6370.5630],\n",
      "        [ -10185.2588,    7461.1919,   10158.3506,   10188.7910,  -10157.3477,\n",
      "            9742.9023,  -10180.0762,  -10178.3848,    7685.1855,  -10181.2881,\n",
      "          -10023.7344,   10088.6943,   10183.6240,  -10175.0000,  -10048.3828,\n",
      "           10090.9414,   10165.9707,  -10077.2725,   10176.1523,  -10129.4297],\n",
      "        [  -6811.1948,    4993.1416,    6793.0454,    6813.6616,   -6792.5220,\n",
      "            6512.8892,   -6807.7085,   -6806.7266,    5118.5098,   -6808.5415,\n",
      "           -6702.3604,    6746.3735,    6810.1685,   -6804.3843,   -6719.2183,\n",
      "            6748.0425,    6798.2510,   -6738.8774,    6805.1567,   -6773.6646],\n",
      "        [ -17892.8008,   13088.1143,   17845.4961,   17899.0293,  -17843.7500,\n",
      "           17116.7344,  -17883.7051,  -17880.6934,   13482.7100,  -17885.7305,\n",
      "          -17610.9219,   17724.2441,   17890.3418,  -17874.9961,  -17653.1777,\n",
      "           17728.7559,   17859.0098,  -17703.9453,   17877.3652,  -17796.1797],\n",
      "        [  -8081.6201,    5918.1543,    8060.2036,    8084.4634,   -8059.4565,\n",
      "            7730.3159,   -8077.4956,   -8076.2036,    6090.1372,   -8078.4761,\n",
      "           -7953.2461,    8004.7422,    8080.3447,   -8073.4902,   -7972.7041,\n",
      "            8006.8862,    8066.3286,   -7996.0601,    8074.5205,   -8037.2910],\n",
      "        [ -18296.3984,   13441.3809,   18247.5020,   18302.3535,  -18245.9766,\n",
      "           17503.5332,  -18287.2598,  -18283.8828,   13818.0342,  -18289.5391,\n",
      "          -18004.5078,   18121.1074,   18292.4082,  -18277.0879,  -18050.6504,\n",
      "           18125.3516,   18262.0469,  -18101.5137,   18278.8965,  -18194.6934],\n",
      "        [  23926.9727,  -17764.2070,  -23858.7578,  -23923.6699,   23860.7910,\n",
      "          -22953.8945,   23922.9121,   23904.3281,  -18571.6270,   23924.8594,\n",
      "           23688.0703,  -23763.0508,  -23920.7148,   23895.9746,   23654.4102,\n",
      "          -23793.0469,  -23900.8789,   23724.3105,  -23912.8496,   23849.7129],\n",
      "        [  -7044.8828,    5162.2637,    7026.2891,    7047.1895,   -7025.5259,\n",
      "            6742.1035,   -7041.3394,   -7039.9941,    5333.3301,   -7042.1807,\n",
      "           -6931.4536,    6976.5381,    7043.1895,   -7037.3408,   -6949.3428,\n",
      "            6978.1763,    7031.4575,   -6969.4023,    7037.9702,   -7004.7944],\n",
      "        [ -10097.8945,    7400.1211,   10071.0254,   10101.4902,  -10070.2207,\n",
      "            9656.7295,  -10092.7441,  -10091.2109,    7596.0347,  -10093.9785,\n",
      "           -9937.6934,   10002.2861,   10096.4365,  -10087.9033,   -9962.4912,\n",
      "           10004.7295,   10078.9268,   -9991.1318,   10089.1523,  -10042.5127],\n",
      "        [ -15303.2373,   11221.1641,   15262.2666,   15308.4629,  -15261.1289,\n",
      "           14640.2910,  -15295.5371,  -15292.9590,   11504.7324,  -15297.3818,\n",
      "          -15058.3184,   15155.6436,   15300.0410,  -15287.2881,  -15095.7363,\n",
      "           15159.2656,   15274.1611,  -15139.7412,   15288.8232,  -15216.9609],\n",
      "        [  -5071.8394,    3710.1726,    5058.4731,    5073.6167,   -5057.9795,\n",
      "            4852.2944,   -5069.2339,   -5068.4111,    3825.5706,   -5069.8223,\n",
      "           -4991.3901,    5023.6025,    5071.0869,   -5066.7583,   -5003.4678,\n",
      "            5025.0430,    5062.1836,   -5018.1704,    5067.4326,   -5043.9165],\n",
      "        [  -9672.3799,    7098.0024,    9646.6016,    9675.6094,   -9645.7881,\n",
      "            9252.0518,   -9667.5107,   -9665.8018,    7296.7607,   -9668.6973,\n",
      "           -9518.6797,    9579.8760,    9670.4629,   -9662.3252,   -9542.2500,\n",
      "            9582.3623,    9654.1162,   -9569.4551,    9663.3740,   -9618.6777],\n",
      "        [  -9605.2744,    7052.3667,    9579.7246,    9608.4570,   -9578.8428,\n",
      "            9191.0791,   -9600.4424,   -9598.7373,    7260.3691,   -9601.6455,\n",
      "           -9451.0967,    9512.8340,    9603.1982,   -9595.1230,   -9475.6973,\n",
      "            9515.1865,    9587.1143,   -9502.7852,    9596.1416,   -9551.4814],\n",
      "        [  -7109.8174,    5201.0713,    7090.9238,    7112.4409,   -7090.3594,\n",
      "            6801.3096,   -7106.1582,   -7105.1597,    5330.3101,   -7106.9780,\n",
      "           -6995.8472,    7042.0552,    7108.8667,   -7102.8242,   -7013.9038,\n",
      "            7043.6929,    7096.2095,   -7034.3506,    7103.6665,   -7070.8057]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-209931.5156,    8569.4893,   13869.4395,    5621.7046,    4065.7249,\n",
      "          17382.8535,    3581.5049,    5104.4727,   22091.9863,   21864.7090,\n",
      "           6411.5938,   10194.9072,    6817.8276,   17909.8457,    8089.3457,\n",
      "          18312.7949,  -23921.9941,    7051.2725,   10107.6484,   15317.4893,\n",
      "           5076.7285,    9681.2539,    9613.9971,    7116.9023],\n",
      "       device='cuda:0')\n",
      "xent mean is: 239.36280822753906\n",
      "kl mean is: 66.19217681884766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 4 | Training Loss: 28938.58203125, Training Accuracy: 0.0, Validation Loss: 27961.80078125, Validation Accuracy: 0.020326829268292684\n",
      "layers_enc.0.weight_ih_l0 None\n",
      "layers_enc.0.weight_hh_l0 None\n",
      "layers_enc.0.bias_ih_l0 None\n",
      "layers_enc.0.bias_hh_l0 None\n",
      "layers_enc.0.weight_ih_l0_reverse None\n",
      "layers_enc.0.weight_hh_l0_reverse None\n",
      "layers_enc.0.bias_ih_l0_reverse None\n",
      "layers_enc.0.bias_hh_l0_reverse None\n",
      "layers_enc.0.weight_ih_l1 None\n",
      "layers_enc.0.weight_hh_l1 None\n",
      "layers_enc.0.bias_ih_l1 None\n",
      "layers_enc.0.bias_hh_l1 None\n",
      "layers_enc.0.weight_ih_l1_reverse None\n",
      "layers_enc.0.weight_hh_l1_reverse None\n",
      "layers_enc.0.bias_ih_l1_reverse None\n",
      "layers_enc.0.bias_hh_l1_reverse None\n",
      "layers_ae.0.weight None\n",
      "layers_ae.0.bias None\n",
      "layers_ae.1.weight None\n",
      "layers_ae.1.bias None\n",
      "layers_dec.0.weight tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "layers_dec.0.bias tensor([ 383.9500, -330.7444, -184.3020,  -86.2081,  341.0365, -276.7990,\n",
      "        -199.8035,   37.0823,   71.8040,   28.7418], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0 tensor([[   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  -28.7428,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000, -667.1827,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  313.2200,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  -44.4867,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  323.8225,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  847.2495,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,   14.3817,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000, -225.7825,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000, -246.5729,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,   95.7314,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0 tensor([[   6.7364,  -34.0779,   42.6519,   -5.3262,   15.6514,   34.6072,\n",
      "           26.2213,  -42.4791,   14.7788,   35.1941],\n",
      "        [ 539.8076, -433.0894,  576.3210, -668.7124,  627.5114,  431.3417,\n",
      "          582.1305, -566.2485,  497.8456,  609.9366],\n",
      "        [-255.7387,  197.3769, -265.7243,  318.8370, -296.4619, -197.2398,\n",
      "         -272.1962,  260.4451, -233.9795, -283.0049],\n",
      "        [  56.6121,  -22.8363,   24.3750,  -68.2522,   55.0727,   19.2629,\n",
      "           40.6441,  -25.1361,   43.8672,   36.8331],\n",
      "        [-297.1382,  205.3115, -259.0007,  362.8127, -326.8917, -197.5848,\n",
      "         -287.0355,  257.4745, -260.8357, -292.5168],\n",
      "        [-705.1420,  581.8510, -742.1343,  855.1068, -806.2835, -568.4098,\n",
      "         -750.1751,  735.2789, -648.6581, -791.6551],\n",
      "        [ -23.1181,    3.7134,   -3.2071,   28.5364,  -21.0457,   -2.1990,\n",
      "          -13.0048,    3.4655,  -16.2840,   -9.7585],\n",
      "        [ 218.2081, -144.5641,  176.0647, -263.4362,  234.5009,  136.2852,\n",
      "          202.2211, -176.2204,  188.2679,  204.4526],\n",
      "        [ 181.6393, -176.9999,  232.4110, -221.6803,  219.6176,  176.9921,\n",
      "          216.4577, -229.0634,  176.7390,  235.2296],\n",
      "        [ -89.8232,   47.8346,  -67.5508,  114.8650,  -98.8804,  -47.7646,\n",
      "          -82.0172,   65.6669,  -76.0242,  -79.3272]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0 tensor([  50.8651, -774.2827,  384.1600, -125.7868,  489.8400,  964.7401,\n",
      "          63.2626, -369.9763, -194.4629,  180.8804], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0 tensor([  50.8651, -774.2827,  384.1600, -125.7868,  489.8400,  964.7401,\n",
      "          63.2626, -369.9763, -194.4629,  180.8804], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l0_reverse tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   552.1947,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -234.5759,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   595.7522,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   219.3268,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000, -1931.5997,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -136.6028,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   -93.2638,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  1017.4854,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,   183.5876,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,  -129.6596,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
      "             0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l0_reverse tensor([[-4.7489e+02,  4.4192e+02, -3.4461e+02, -4.2308e+02,  1.9677e+02,\n",
      "          4.8607e+02,  5.1954e+02, -3.9603e+02, -4.2751e+02,  4.4484e+02],\n",
      "        [ 2.4782e+02, -2.4287e+02,  1.9382e+02,  2.4266e+02, -1.1505e+02,\n",
      "         -2.6079e+02, -2.8501e+02,  2.1039e+02,  2.2670e+02, -2.4134e+02],\n",
      "        [-4.5791e+02,  3.8500e+02, -2.8101e+02, -3.3296e+02,  1.5150e+02,\n",
      "          4.4319e+02,  4.5581e+02, -3.7015e+02, -3.9799e+02,  3.9806e+02],\n",
      "        [-1.1494e+02,  5.6601e+01, -2.1109e+01, -1.0760e+01,  6.8732e-01,\n",
      "          8.6639e+01,  7.0387e+01, -8.1505e+01, -8.6238e+01,  6.9906e+01],\n",
      "        [ 1.6913e+03, -1.6213e+03,  1.2887e+03,  1.5941e+03, -7.4277e+02,\n",
      "         -1.7612e+03, -1.9015e+03,  1.4234e+03,  1.5401e+03, -1.6198e+03],\n",
      "        [ 6.4948e+01, -2.5755e+01,  4.7368e+00, -5.0694e+00,  5.9181e+00,\n",
      "         -4.5290e+01, -3.2712e+01,  4.4145e+01,  4.6869e+01, -3.4808e+01],\n",
      "        [ 4.4810e+01, -2.0524e+01,  6.1395e+00,  1.3248e+00,  9.7962e-01,\n",
      "         -3.2769e+01, -2.5826e+01,  3.1387e+01,  3.2994e+01, -2.6107e+01],\n",
      "        [-8.6567e+02,  8.1620e+02, -6.4283e+02, -7.9107e+02,  3.6657e+02,\n",
      "          8.9300e+02,  9.5790e+02, -7.2435e+02, -7.8382e+02,  8.1868e+02],\n",
      "        [-1.0579e+02,  6.8574e+01, -4.0262e+01, -4.0050e+01,  1.5536e+01,\n",
      "          9.0035e+01,  8.2707e+01, -7.9588e+01, -8.5255e+01,  7.6676e+01],\n",
      "        [ 4.0289e+01,  8.5137e+00, -2.7242e+01, -4.7653e+01,  2.6378e+01,\n",
      "         -1.3084e+01,  6.5875e+00,  2.0461e+01,  2.0713e+01, -3.2708e+00]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.bias_ih_l0_reverse tensor([  592.0906,  -286.3557,   648.2797,   237.7238, -2018.9321,  -145.8995,\n",
      "          -95.5981,  1059.4058,   187.8293,  -136.2672], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l0_reverse tensor([  592.0906,  -286.3557,   648.2797,   237.7238, -2018.9321,  -145.8995,\n",
      "          -95.5981,  1059.4058,   187.8293,  -136.2672], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1 tensor([[-2.1202e+01, -3.3339e+00, -6.7236e+01,  6.6520e+01, -5.0005e+01,\n",
      "         -1.9958e+01, -5.6779e+01,  4.8440e+01, -2.8514e+01, -5.5504e+01,\n",
      "         -6.7763e+01,  5.9738e+01, -6.5336e+01, -8.3187e+01,  3.9813e+01,\n",
      "          7.3122e+01,  7.4875e+01, -7.5875e+01, -6.8151e+01,  7.4404e+01],\n",
      "        [ 3.5571e+03, -2.9365e+03,  3.7647e+03, -4.2220e+03,  4.0163e+03,\n",
      "          2.9291e+03,  3.9071e+03, -4.0946e+03,  3.3868e+03,  3.8382e+03,\n",
      "          3.9597e+03, -3.6380e+03,  2.9748e+03,  3.7602e+03, -1.6856e+03,\n",
      "         -4.0691e+03, -4.4475e+03,  2.9344e+03,  3.3306e+03, -3.7928e+03],\n",
      "        [ 1.1528e+02, -9.7739e+01,  1.2744e+02, -1.6177e+02,  1.2089e+02,\n",
      "          1.0376e+02,  1.4176e+02, -1.2767e+02,  1.0348e+02,  1.6225e+02,\n",
      "          1.3484e+02, -1.5506e+02,  1.0400e+02,  1.1898e+02, -6.7605e+01,\n",
      "         -1.6779e+02, -1.3651e+02,  1.2145e+02,  1.4003e+02, -1.4300e+02],\n",
      "        [ 1.0079e+01, -2.0010e+00,  2.9239e+01, -3.2499e+01,  2.7091e+01,\n",
      "          1.3488e+01,  3.1273e+01, -2.6848e+01,  1.3096e+01,  2.8634e+01,\n",
      "          3.9858e+01, -3.3522e+01,  2.7904e+01,  3.2291e+01, -2.1514e+01,\n",
      "         -3.7534e+01, -3.8693e+01,  3.0393e+01,  3.6649e+01, -3.1695e+01],\n",
      "        [-1.1481e+02,  1.0080e+02, -1.4652e+02,  1.6110e+02, -1.5776e+02,\n",
      "         -8.9562e+01, -1.1545e+02,  1.3377e+02, -9.8728e+01, -1.2899e+02,\n",
      "         -1.2498e+02,  1.4099e+02, -1.1080e+02, -1.4715e+02,  7.7161e+01,\n",
      "          1.6964e+02,  1.8129e+02, -1.3348e+02, -1.1540e+02,  1.2798e+02],\n",
      "        [ 1.1195e+03, -1.1740e+03,  1.5891e+03, -1.2718e+03,  1.2740e+03,\n",
      "          1.0120e+03,  1.5238e+03, -1.2370e+03,  1.2556e+03,  1.5990e+03,\n",
      "          1.2605e+03, -1.2124e+03,  1.0246e+03,  1.3709e+03, -6.0415e+02,\n",
      "         -1.4615e+03, -1.5730e+03,  1.1804e+03,  1.3318e+03, -1.1753e+03],\n",
      "        [-3.1442e+01,  6.0669e+00, -8.5244e+01,  7.5442e+01, -8.3469e+01,\n",
      "         -3.8250e+01, -7.1879e+01,  6.1017e+01, -4.0014e+01, -7.3252e+01,\n",
      "         -8.4241e+01,  8.3697e+01, -8.8690e+01, -9.5809e+01,  5.1732e+01,\n",
      "          9.0560e+01,  1.1556e+02, -9.0554e+01, -8.7120e+01,  8.6102e+01],\n",
      "        [-5.2297e+01,  4.5022e+01, -9.1604e+01,  8.8789e+01, -8.4699e+01,\n",
      "         -5.7121e+01, -8.9256e+01,  8.8094e+01, -5.4377e+01, -7.0471e+01,\n",
      "         -1.0273e+02,  7.3574e+01, -7.6057e+01, -8.1946e+01,  5.0125e+01,\n",
      "          8.4885e+01,  9.8181e+01, -7.9994e+01, -8.6453e+01,  9.7953e+01],\n",
      "        [ 2.4311e+03, -1.8124e+03,  2.4316e+03, -3.0830e+03,  2.3655e+03,\n",
      "          1.8559e+03,  2.6423e+03, -2.4546e+03,  2.2213e+03,  2.7626e+03,\n",
      "          2.3460e+03, -2.3923e+03,  1.8611e+03,  2.5555e+03, -1.2055e+03,\n",
      "         -2.7532e+03, -3.0991e+03,  1.9532e+03,  2.4789e+03, -2.6308e+03],\n",
      "        [-3.0228e+01,  2.6856e-01, -8.5947e+01,  7.2479e+01, -6.7681e+01,\n",
      "         -3.1943e+01, -7.3049e+01,  5.7938e+01, -3.4367e+01, -5.7618e+01,\n",
      "         -9.2133e+01,  7.4064e+01, -7.3861e+01, -9.7440e+01,  5.2073e+01,\n",
      "          9.7464e+01,  9.6388e+01, -8.2922e+01, -8.5523e+01,  8.6949e+01]],\n",
      "       device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1 tensor([[  -53.3940,    25.4879,    45.7845,    55.1341,   -46.3571,    40.2156,\n",
      "           -36.7288,   -44.2161,    18.7161,   -32.2799],\n",
      "        [ 4311.1860, -3547.5349, -4282.1328, -4317.2061,  4283.9668, -4152.5215,\n",
      "          4262.2305,  4283.3228, -3571.4890,  4249.8350],\n",
      "        [  156.2781,  -121.1493,  -152.2477,  -157.1869,   152.5378,  -145.8313,\n",
      "           147.9021,   151.6445,  -118.8149,   145.6900],\n",
      "        [   27.5085,   -13.8354,   -23.8771,   -28.3395,    24.1498,   -21.1625,\n",
      "            19.5619,    23.1325,   -10.5542,    17.4408],\n",
      "        [ -160.2342,   123.4140,   156.0000,   161.2038,  -156.2962,   149.4623,\n",
      "          -151.3740,  -155.3411,   122.0456,  -149.0333],\n",
      "        [ 1587.8328, -1307.8793, -1575.0607, -1590.4827,  1575.8492, -1525.5291,\n",
      "          1565.1527,  1574.9636, -1321.1418,  1559.3777],\n",
      "        [  -77.0154,    43.7983,    69.0492,    78.8579,   -69.6426,    62.6229,\n",
      "           -59.5991,   -67.4400,    36.9837,   -54.9549],\n",
      "        [  -92.0226,    64.9083,    87.3447,    93.1005,   -87.6885,    82.3522,\n",
      "           -81.9582,   -86.4813,    61.4251,   -79.2824],\n",
      "        [ 2918.3120, -2393.4309, -2896.7244, -2922.9292,  2898.0386, -2807.5977,\n",
      "          2880.5659,  2896.9861, -2418.1475,  2870.9617],\n",
      "        [  -66.0187,    32.8127,    57.1607,    68.0451,   -57.8268,    50.5762,\n",
      "           -46.6322,   -55.3429,    24.7485,   -41.4575]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1 tensor([   96.8102, -4437.7329,  -178.0694,   -48.2115,   183.2918, -1646.4402,\n",
      "          122.3980,   118.2405, -3015.9785,   116.5250], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1 tensor([   96.8102, -4437.7329,  -178.0694,   -48.2115,   183.2918, -1646.4402,\n",
      "          122.3980,   118.2405, -3015.9785,   116.5250], device='cuda:0')\n",
      "layers_dec.1.weight_ih_l1_reverse tensor([[ -774.6676,   629.9821,  -910.9155,   942.0145, -1018.4793,  -789.0654,\n",
      "          -940.4687,   982.0708,  -812.7454,  -926.0311,  -960.1630,   942.4818,\n",
      "          -553.2794,  -740.6578,   480.3295,   816.1648,   784.5674,  -834.9533,\n",
      "          -827.0606,   703.9910],\n",
      "        [  588.3099,  -459.2385,   503.8685,  -615.4901,   553.9750,   450.6241,\n",
      "           624.8154,  -569.4147,   489.2393,   658.0675,   587.2991,  -534.2330,\n",
      "           329.4391,   443.2820,  -279.2443,  -491.2962,  -597.0488,   389.6205,\n",
      "           456.5865,  -444.9533],\n",
      "        [   57.7410,   -47.0598,    61.9857,   -67.7291,    66.5775,    58.5850,\n",
      "            60.0460,   -70.9182,    62.6958,    62.8385,    64.4137,   -57.8496,\n",
      "            27.2562,    48.9109,   -40.7178,   -55.3047,   -78.9532,    49.9304,\n",
      "            44.0908,   -61.6691],\n",
      "        [ -120.5468,   106.2850,  -124.1792,   132.0985,  -122.3986,  -114.2479,\n",
      "          -159.0504,   133.8543,  -107.8309,  -159.0836,  -132.9516,   133.1060,\n",
      "           -89.6836,  -139.9859,    79.2098,   139.5459,   139.1781,  -119.2938,\n",
      "          -138.8958,   124.0148],\n",
      "        [ -842.0402,   595.2693,  -961.3538,   972.9860,  -692.0072,  -601.4836,\n",
      "          -706.0676,   821.0707,  -655.0439, -1015.2909,  -746.8531,   856.7090,\n",
      "          -652.7629,  -801.3461,   390.9756,   934.5452,   993.1756,  -751.0936,\n",
      "          -831.4490,   844.2062],\n",
      "        [  486.3235,  -495.6478,   631.5596,  -667.8297,   579.5591,   458.2193,\n",
      "           624.6451,  -641.9346,   460.7887,   569.4221,   531.6733,  -517.8730,\n",
      "           329.6878,   454.6470,  -294.7219,  -496.4426,  -558.6311,   449.2494,\n",
      "           383.3506,  -466.3169],\n",
      "        [   52.5959,   -29.7128,   119.4794,   -89.5053,    92.2357,    53.2203,\n",
      "            79.0676,   -76.6403,    67.0037,    85.6608,    88.7845,   -97.8024,\n",
      "            88.4382,    94.8183,   -57.0634,  -115.9718,  -142.9982,   106.0611,\n",
      "           103.1724,  -108.4977],\n",
      "        [ -442.2529,   382.9434,  -551.4783,   509.5988,  -524.7344,  -402.6967,\n",
      "          -524.5903,   465.1925,  -452.1662,  -482.3291,  -446.0108,   443.9300,\n",
      "          -325.3189,  -403.8258,   243.4676,   421.3200,   608.3906,  -438.3078,\n",
      "          -436.2177,   493.4734],\n",
      "        [   93.6360,   -89.2708,   125.7191,  -112.9173,   110.6895,    88.6065,\n",
      "           120.4762,   -87.7891,    92.0083,    95.2902,    83.8636,   -74.8066,\n",
      "            51.3777,    81.8679,   -55.5599,  -100.2093,   -92.8729,    85.1758,\n",
      "            83.8675,   -99.2207],\n",
      "        [ -427.7482,   353.7118,  -463.2122,   399.9325,  -419.9783,  -341.9597,\n",
      "          -337.6696,   438.9703,  -387.3455,  -466.9760,  -349.4119,   384.3648,\n",
      "          -215.8231,  -294.6546,   204.5548,   379.4984,   442.0880,  -315.1598,\n",
      "          -264.9638,   293.9046]], device='cuda:0')\n",
      "layers_dec.1.weight_hh_l1_reverse tensor([[-991.8279,  931.5492,  980.0986, -885.1748, -973.5283,  960.5378,\n",
      "          939.1271, -968.8235,  915.9522, -936.6988],\n",
      "        [ 580.6294, -534.4470, -569.7123,  498.7943,  566.3168, -556.1445,\n",
      "         -539.2444,  562.4812, -521.7947,  537.8193],\n",
      "        [  59.0267,  -44.9538,  -54.6951,   33.7847,   54.6438,  -51.3902,\n",
      "          -45.7499,   53.2997,  -40.5901,   45.6861],\n",
      "        [-158.8417,  148.5226,  156.8326, -140.5013, -155.7627,  153.4829,\n",
      "          149.7727, -154.9296,  145.7946, -149.3813],\n",
      "        [-959.1227,  931.2534,  958.1121, -910.4600, -950.8231,  945.6608,\n",
      "          937.2490, -949.4187,  925.9085, -934.9954],\n",
      "        [ 554.1329, -492.4709, -537.7936,  444.4580,  535.0648, -521.1132,\n",
      "         -497.8022,  529.6193, -474.7501,  496.4057],\n",
      "        [ 129.5360, -127.0987, -130.2113,  124.9060,  129.0747, -128.6018,\n",
      "         -127.7683,  128.9352, -126.6426,  127.5978],\n",
      "        [-544.2875,  519.3546,  540.7822, -500.4511, -536.9403,  531.6094,\n",
      "          523.3427, -535.1542,  513.4979, -521.8655],\n",
      "        [  95.6067,  -78.1419,  -90.4034,   64.3748,   90.1546,  -86.1601,\n",
      "          -79.2689,   88.5294,  -72.8410,   79.1037],\n",
      "        [-377.2311,  318.1805,  360.1361, -271.9211, -358.8478,  345.3125,\n",
      "          322.4143, -353.4156,  300.5716, -321.5564]], device='cuda:0')\n",
      "layers_dec.1.bias_ih_l1_reverse tensor([1131.0165, -682.4766,  -87.9595,  182.8159, 1033.6934, -686.2895,\n",
      "        -138.6364,  604.9086, -131.8069,  500.5008], device='cuda:0')\n",
      "layers_dec.1.bias_hh_l1_reverse tensor([1131.0165, -682.4766,  -87.9595,  182.8159, 1033.6934, -686.2895,\n",
      "        -138.6364,  604.9086, -131.8069,  500.5008], device='cuda:0')\n",
      "layers_dec_post_rec.0.weight tensor([[ 137631.8594, -114834.1016, -137390.4531, -137673.4531,  137390.3125,\n",
      "         -133529.8594,  137578.0156,  137575.9375, -116389.2188,  137587.5781,\n",
      "          136022.0625, -136729.3750, -137620.0469,  137544.5469,  136356.9531,\n",
      "         -136693.9688, -137454.0625,  136645.1719, -137543.0625,  137074.9219],\n",
      "        [  -6015.7749,    5030.9951,    6005.0742,    6017.2720,   -6005.2383,\n",
      "            5837.1592,   -6013.6812,   -6013.1865,    5098.4800,   -6014.0996,\n",
      "           -5950.2104,    5978.7617,    6015.3022,   -6011.7954,   -5962.0479,\n",
      "            5978.0015,    6008.5801,   -5974.3535,    6012.2744,   -5993.7725],\n",
      "        [ -10068.9463,    8437.2158,   10051.0869,   10071.1533,  -10051.2998,\n",
      "            9773.4531,  -10065.5791,  -10064.4199,    8580.5762,  -10066.3311,\n",
      "           -9960.2148,   10006.9082,   10067.6924,  -10061.8311,   -9979.1953,\n",
      "           10005.7109,   10057.1191,   -9999.7480,   10062.6211,  -10031.8936],\n",
      "        [  -3914.5527,    3271.2712,    3907.6160,    3915.5100,   -3907.6902,\n",
      "            3799.6362,   -3913.1855,   -3912.8418,    3321.2888,   -3913.4512,\n",
      "           -3871.8062,    3890.3376,    3914.2126,   -3911.9365,   -3879.4421,\n",
      "            3889.8306,    3909.8577,   -3887.5269,    3912.2178,   -3900.0596],\n",
      "        [  -2797.3481,    2336.3022,    2792.3662,    2798.0776,   -2792.4509,\n",
      "            2714.1963,   -2796.3503,   -2796.1699,    2365.3162,   -2796.5420,\n",
      "           -2766.2490,    2779.8813,    2797.1313,   -2795.5220,   -2772.1904,\n",
      "            2779.4441,    2793.9563,   -2777.9255,    2795.6968,   -2786.8533],\n",
      "        [ -12117.5195,   10129.5576,   12095.8799,   12120.5264,  -12096.2812,\n",
      "           11757.6270,  -12113.2598,  -12112.3271,   10260.0469,  -12114.0928,\n",
      "          -11985.4551,   12043.1543,   12116.6865,  -12109.6191,  -12009.3594,\n",
      "           12041.7305,   12103.1016,  -12034.4043,   12110.6338,  -12073.2930],\n",
      "        [  -2559.4941,    2143.1543,    2554.9514,    2560.0779,   -2554.9897,\n",
      "            2483.6873,   -2558.6191,   -2558.3516,    2178.4590,   -2558.8049,\n",
      "           -2530.9597,    2543.2327,    2559.0911,   -2557.6536,   -2536.3953,\n",
      "            2542.8826,    2556.4138,   -2541.5808,    2557.7708,   -2549.6216],\n",
      "        [  -3544.2180,    2961.6609,    3537.9148,    3545.1270,   -3538.0203,\n",
      "            3438.8889,   -3542.9534,   -3542.7097,    2999.8730,   -3543.1934,\n",
      "           -3505.1704,    3522.2109,    3543.9563,   -3541.9080,   -3512.4324,\n",
      "            3521.7427,    3539.9473,   -3519.7244,    3542.1682,   -3531.1106],\n",
      "        [ -15562.6631,   13017.9639,   15534.8057,   15566.3770,  -15535.2920,\n",
      "           15103.3887,  -15557.2842,  -15555.8779,   13192.3887,  -15558.3994,\n",
      "          -15391.5889,   15465.5957,   15560.9551,  -15552.0225,  -15422.8486,\n",
      "           15463.5654,   15544.0488,  -15454.8916,   15553.0850,  -15504.3721],\n",
      "        [ -15426.8701,   12905.1768,   15399.3799,   15430.5771,  -15399.8359,\n",
      "           14971.1533,  -15421.5547,  -15420.1885,   13083.7725,  -15422.6514,\n",
      "          -15260.2432,   15332.3066,   15425.5947,  -15416.5791,  -15288.9277,\n",
      "           15330.3516,   15408.5479,  -15321.1055,   15417.8740,  -15370.5898],\n",
      "        [  -4584.6963,    3837.9937,    4576.5850,    4585.7393,   -4576.6514,\n",
      "            4449.8804,   -4583.1279,   -4582.6411,    3903.5884,   -4583.4614,\n",
      "           -4534.5000,    4556.2549,    4584.1396,   -4581.4844,   -4543.7568,\n",
      "            4555.7290,    4579.2251,   -4553.0015,    4581.8267,   -4567.7163],\n",
      "        [  -7133.0605,    5964.0171,    7120.4448,    7134.7954,   -7120.5557,\n",
      "            6921.9688,   -7130.5693,   -7129.9395,    6056.9316,   -7131.0532,\n",
      "           -7055.2427,    7089.0728,    7132.4268,   -7128.2871,   -7069.2104,\n",
      "            7088.0859,    7124.5049,   -7083.8691,    7128.7939,   -7106.7681],\n",
      "        [  -4778.9189,    3996.4722,    4770.4097,    4780.1162,   -4770.5435,\n",
      "            4636.5693,   -4777.2393,   -4776.8721,    4050.1960,   -4777.5762,\n",
      "           -4726.4287,    4749.3481,    4778.5112,   -4775.7368,   -4735.9629,\n",
      "            4748.7471,    4773.1748,   -4745.9092,    4776.0869,   -4761.2207],\n",
      "        [ -12377.0410,   10340.9727,   12355.1162,   12380.0488,  -12355.3262,\n",
      "           12010.8457,  -12372.6816,  -12371.5957,   10501.8721,  -12373.5049,\n",
      "          -12242.7754,   12301.1689,   12376.1309,  -12368.8682,  -12266.5537,\n",
      "           12299.6924,   12362.1963,  -12292.0322,   12369.8936,  -12332.0566],\n",
      "        [  -5637.1484,    4712.3071,    5627.1621,    5638.5371,   -5627.2773,\n",
      "            5470.1699,   -5635.1836,   -5634.7051,    4783.6328,   -5635.5703,\n",
      "           -5575.5605,    5602.2734,    5636.6641,   -5633.3931,   -5586.5757,\n",
      "            5601.6436,    5630.3979,   -5598.3271,    5633.8442,   -5616.3770],\n",
      "        [ -13114.8203,   10981.5889,   13091.3867,   13117.7607,  -13091.7051,\n",
      "           12727.9854,  -13110.3428,  -13108.9375,   11144.6807,  -13111.2686,\n",
      "          -12971.0479,   13033.1221,   13113.1074,  -13105.5674,  -12997.4482,\n",
      "           13031.3955,   13099.1484,  -13023.9307,   13106.4092,  -13065.8525],\n",
      "        [  26999.7832,  -22868.3184,  -26948.1270,  -26997.6914,   26952.2871,\n",
      "          -26236.4883,   26996.6074,   26983.6387,  -23318.5957,   26998.4062,\n",
      "           26809.7402,  -26882.3184,  -26995.2578,   26975.3008,   26799.5625,\n",
      "          -26897.1172,  -26980.4922,   26850.2227,  -26989.0137,   26946.5566],\n",
      "        [  -4962.4258,    4150.3999,    4953.6431,    4963.5610,   -4953.7061,\n",
      "            4816.8247,   -4960.7202,   -4960.1924,    4220.8745,   -4961.0640,\n",
      "           -4907.5493,    4931.1543,    4961.7280,   -4958.9077,   -4917.6440,\n",
      "            4930.4946,    4956.4341,   -4927.8564,    4959.1909,   -4943.4951],\n",
      "        [  -7071.8584,    5913.3081,    7059.2700,    7073.6094,   -7059.4570,\n",
      "            6861.5806,   -7069.3755,   -7068.8076,    5996.2559,   -7069.8735,\n",
      "           -6994.6782,    7028.2847,    7071.2900,   -7067.1929,   -7008.6597,\n",
      "            7027.4150,    7063.4238,   -7023.2100,    7067.7456,   -7045.7910],\n",
      "        [ -10782.9326,    9019.5986,   10763.6602,   10785.4990,  -10763.9785,\n",
      "           10464.6426,  -10779.2246,  -10778.2227,    9141.7051,  -10779.9502,\n",
      "          -10664.3477,   10715.3770,   10781.6670,  -10775.4824,  -10685.6465,\n",
      "           10713.9883,   10769.9521,  -10708.0400,   10776.2012,  -10742.1973],\n",
      "        [  -3519.2488,    2940.4375,    3513.0334,    3520.1172,   -3513.0989,\n",
      "            3415.3101,   -3518.0110,   -3517.7080,    2987.4465,   -3518.2439,\n",
      "           -3480.8491,    3497.4846,    3518.9719,   -3516.9165,   -3487.6672,\n",
      "            3497.1304,    3515.0117,   -3495.0251,    3517.2134,   -3506.2319],\n",
      "        [  -6861.4424,    5742.0151,    6849.2192,    6863.0444,   -6849.3784,\n",
      "            6658.4702,   -6859.0835,   -6858.4043,    5826.8896,   -6859.5649,\n",
      "           -6786.4829,    6818.7671,    6860.6504,   -6856.6938,   -6799.9673,\n",
      "            6817.9907,    6853.2334,   -6813.9629,    6857.1680,   -6835.8613],\n",
      "        [  -6873.9565,    5754.3447,    6861.7275,    6875.5303,   -6861.8726,\n",
      "            6671.9570,   -6871.5986,   -6870.8965,    5843.3833,   -6872.0928,\n",
      "           -6798.1860,    6830.9531,    6873.0635,   -6869.1084,   -6812.2168,\n",
      "            6830.1055,    6865.7075,   -6826.2393,    6869.5747,   -6848.1045],\n",
      "        [  -4926.5537,    4115.6216,    4917.8003,    4927.8081,   -4917.9312,\n",
      "            4780.7583,   -4924.8091,   -4924.4512,    4170.0718,   -4925.1250,\n",
      "           -4872.2803,    4896.0293,    4926.2114,   -4923.3389,   -4882.3013,\n",
      "            4895.3652,    4920.5840,   -4892.5425,    4923.7090,   -4908.3896]],\n",
      "       device='cuda:0')\n",
      "layers_dec_post_rec.0.bias tensor([-137741.5469,    6019.7803,   10074.9736,    3917.1465,    2799.3145,\n",
      "          12125.6660,    2561.0886,    3546.6699,   15572.7305,   15436.8916,\n",
      "           4587.5444,    7137.7471,    4782.1411,   12385.2393,    5640.8921,\n",
      "          13122.9258,  -26996.7930,    4965.5498,    7076.5830,   10789.9072,\n",
      "           3521.6072,    6865.8013,    6878.2627,    4929.9517],\n",
      "       device='cuda:0')\n",
      "xent mean is: 208.517578125\n",
      "kl mean is: 66.17803955078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')\n",
      "Epoch 5 | Training Loss: 29563.0546875, Training Accuracy: 0.0, Validation Loss: 28560.79296875, Validation Accuracy: 0.020326829268292684\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_test = []\n",
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "xents_train = []\n",
    "xents_test = []\n",
    "kls_train = []\n",
    "kls_test = []\n",
    "\n",
    "if vae_type == 'full':\n",
    "    print (\"training full\")\n",
    "    model = VAE(layers_enc,layers_ae,layers_dec)\n",
    "\n",
    "    prams = list(model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(prams, lr = 0.001)\n",
    "\n",
    "    x_train_data, x_val_data = train_test_split(x_train, test_size = 0.1)\n",
    "\n",
    "    ins_train = x_train_data.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_train = torch.Tensor(ins_train)\n",
    "    ins_train = torch.argmax(ins_train,1)\n",
    "\n",
    "    ins_val = x_val_data.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_val = torch.Tensor(ins_val)\n",
    "    ins_val = torch.argmax(ins_val,1)\n",
    "\n",
    "    for epoch in range(nb_epoch):\n",
    "        model.train()\n",
    "\n",
    "        train = np.random.permutation(x_train_data)\n",
    "        train = train.reshape(-1,batch_size,1968)\n",
    "\n",
    "        train = torch.Tensor(train)\n",
    "\n",
    "        \n",
    "        \n",
    "        for batch in train:\n",
    "            out = model(batch)\n",
    "\n",
    "            batch = batch.reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "            batch = torch.argmax(batch,1)\n",
    "            out = out.reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "            loss,_,_ = vae_loss(batch,out,model.z_mean,model.z_log_var)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  \n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        out_train = model(torch.Tensor(x_train_data))\n",
    "        out_train = torch.Tensor(out_train)\n",
    "        out_train = out_train.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_train = torch.argmax(out_train,dim=1)\n",
    "        bool_train = (classpreds_train==ins_train)\n",
    "        class_acc_train = bool_train.sum().item()/bool_train.shape[0]\n",
    "\n",
    "        out_val = model(torch.Tensor(x_val_data))\n",
    "        out_val = torch.Tensor(out_val)\n",
    "        out_val = out_val.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_val = torch.argmax(out_val,dim=1)\n",
    "        bool_val = (classpreds_val==ins_val)\n",
    "        class_acc_val = bool_val.sum().item()/bool_val.shape[0]\n",
    "\n",
    "        loss_train,_,_ = vae_loss(ins_train,out_train,model.z_mean,model.z_log_var)\n",
    "        loss_val,_,_ = vae_loss(ins_val,out_val,model.z_mean,model.z_log_var)\n",
    "        \n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_val)\n",
    "        accuracies_train.append(class_acc_train)\n",
    "        accuracies_test.append(class_acc_val)\n",
    "        \n",
    "        print('Epoch %s | Training Loss: %s, Training Accuracy: %s, Validation Loss: %s, Validation Accuracy: %s'\n",
    "              %( epoch, loss_train.item(), class_acc_train, loss_val.item(), class_acc_val ) )\n",
    "\n",
    "elif vae_type == 'conv':\n",
    "    print (\"conv\")\n",
    "    model = VAE_conv(layers_enc_pre_view,enc_view,layers_enc_post_view,layers_ae,layers_dec)\n",
    "        \n",
    "    prams = list(model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(prams, lr = 0.001)\n",
    "\n",
    "    x_train_data, x_val_data = train_test_split(x_train, test_size = 0.1)\n",
    "\n",
    "    ins_train = x_train_data.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_train = torch.Tensor(ins_train)\n",
    "    ins_train = torch.argmax(ins_train,1)\n",
    "\n",
    "    ins_val = x_val_data.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_val = torch.Tensor(ins_val)\n",
    "    ins_val = torch.argmax(ins_val,1)\n",
    "\n",
    "    for epoch in range(nb_epoch):\n",
    "        model.train()\n",
    "\n",
    "        train = np.random.permutation(x_train_data)\n",
    "        train = train.reshape(-1,batch_size,PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        train = torch.Tensor(train)\n",
    "        train = train.transpose(-2,-1)\n",
    "\n",
    "        for batch in train:\n",
    "            out = model(batch)\n",
    "\n",
    "            batch = batch.transpose(-2,-1).reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "            batch = torch.argmax(batch,1)\n",
    "            out = out.reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "            loss,_,_ = vae_loss(batch,out,model.z_mean,model.z_log_var)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        out_train = model(torch.Tensor(x_train_data).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)).transpose(-2,-1))\n",
    "        out_train = torch.Tensor(out_train)\n",
    "        out_train = out_train.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_train = torch.argmax(out_train,dim=1)\n",
    "        bool_train = (classpreds_train==ins_train)\n",
    "        class_acc_train = bool_train.sum().item()/bool_train.shape[0]\n",
    "\n",
    "        out_val = model(torch.Tensor(x_val_data).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)).transpose(-2,-1))\n",
    "        out_val = torch.Tensor(out_val)\n",
    "        out_val = out_val.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_val = torch.argmax(out_val,dim=1)\n",
    "        bool_val = (classpreds_val==ins_val)\n",
    "        class_acc_val = bool_val.sum().item()/bool_val.shape[0]\n",
    "\n",
    "        loss_train,_,_ = vae_loss(ins_train,out_train,model.z_mean,model.z_log_var)\n",
    "        loss_val,_,_ = vae_loss(ins_val,out_val,model.z_mean,model.z_log_var)\n",
    "\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_val)\n",
    "        accuracies_train.append(class_acc_train)\n",
    "        accuracies_test.append(class_acc_val)\n",
    "        \n",
    "        print('Epoch %s | Training Loss: %s, Training Accuracy: %s, Validation Loss: %s, Validation Accuracy: %s'\n",
    "              %( epoch, loss_train.item(), class_acc_train, loss_val.item(), class_acc_val ) )\n",
    "        \n",
    "elif vae_type == 'rec':\n",
    "    print (\"rec\")\n",
    "    if lang_mod:\n",
    "        print(\"language model training\")\n",
    "    else:\n",
    "        print(\"vae training\")\n",
    "    model = VAE_rec(layers_enc,layers_post_rec_enc,layers_ae,0,layers_dec,layers_dec_post_rec)\n",
    "    \n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    prams = list(model.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(prams, lr = 0.01)\n",
    "\n",
    "    x_train_data, x_val_data = train_test_split(x_train, test_size = 0.1)\n",
    "    \n",
    "    print('FAKE TRAINING SET TO ASSESS REC VALIDITY')\n",
    "    x_train_data = np.array([[0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]]*3690000).reshape(45000,1968)\n",
    "\n",
    "    ins_train = x_train_data.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_train = torch.Tensor(ins_train)\n",
    "    ins_train = torch.argmax(ins_train,1)\n",
    "\n",
    "    ins_val = x_val_data.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "    ins_val = torch.Tensor(ins_val)\n",
    "    ins_val = torch.argmax(ins_val,1)\n",
    "    \n",
    "    ins_train = create_tensor(ins_train,gpu=cuda)\n",
    "    ins_val = create_tensor(ins_val,gpu=cuda)\n",
    "    \n",
    "    \n",
    "    ## Printing model perf before\n",
    "    model.eval()\n",
    "        \n",
    "    out_train = model(create_tensor(torch.Tensor(x_train_data),gpu=cuda).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)),False,lang_mod)\n",
    "    out_train = out_train.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "    classpreds_train = torch.argmax(out_train,dim=1)\n",
    "    bool_train = (classpreds_train==ins_train)\n",
    "    class_acc_train = bool_train.sum().item()/bool_train.shape[0]\n",
    "\n",
    "    out_val = model(create_tensor(torch.Tensor(x_val_data),gpu=cuda).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)),False,lang_mod)\n",
    "    out_val = out_val.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "    classpreds_val = torch.argmax(out_val,dim=1)\n",
    "    bool_val = (classpreds_val==ins_val)\n",
    "    class_acc_val = bool_val.sum().item()/bool_val.shape[0]\n",
    "\n",
    "    loss_train,xent_train,kl_train = vae_loss(ins_train,out_train,model.z_mean,model.z_log_var)\n",
    "    loss_val,xent_test,kl_test = vae_loss(ins_val,out_val,model.z_mean,model.z_log_var)\n",
    "\n",
    "    losses_train.append(loss_train.item())\n",
    "    losses_test.append(loss_val.item())\n",
    "    accuracies_train.append(class_acc_train)\n",
    "    accuracies_test.append(class_acc_val)\n",
    "    xents_train.append(xent_train.item())\n",
    "    xents_test.append(xent_test.item())\n",
    "    kls_train.append(kl_train.item())\n",
    "    kls_test.append(kl_test.item())\n",
    "\n",
    "    print(classpreds_train)\n",
    "    print(classpreds_val)\n",
    "\n",
    "    print('Pre-training | Training Loss: %s, Training Accuracy: %s, Validation Loss: %s, Validation Accuracy: %s'\n",
    "          %( loss_train.item(), class_acc_train, loss_val.item(), class_acc_val ) )\n",
    "    \n",
    "    for epoch in range(nb_epoch):\n",
    "        model.train()\n",
    "\n",
    "        train = np.random.permutation(x_train_data)\n",
    "        train = train.reshape(-1,batch_size,PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        train = create_tensor(torch.Tensor(train),gpu=cuda)\n",
    "\n",
    "        xents = []\n",
    "        kls = []\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch in train:\n",
    "            out = model(batch,True,lang_mod)\n",
    "\n",
    "            batch = batch.transpose(-2,-1).reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "            batch = torch.argmax(batch,1)\n",
    "            \n",
    "            out = out.reshape(batch_size*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "            loss,xent,kl = vae_loss(batch,out,model.z_mean,model.z_log_var)\n",
    "            xents.append(xent)\n",
    "            kls.append(kl)\n",
    "\n",
    "            if lang_mod:\n",
    "                xent.backward()\n",
    "            else:\n",
    "                loss.backward()   \n",
    "            \n",
    "        for layer, paramval in model.named_parameters():\n",
    "            print(layer,paramval.grad)\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "#         import pdb; pdb.set_trace()\n",
    "        print('xent mean is:',torch.stack(xents).mean().item())\n",
    "        print('kl mean is:',torch.stack(kls).mean().item())\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        out_train = model(create_tensor(torch.Tensor(x_train_data),gpu=cuda).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)),False,lang_mod)\n",
    "        out_train = out_train.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_train = torch.argmax(out_train,dim=1)\n",
    "        bool_train = (classpreds_train==ins_train)\n",
    "        class_acc_train = bool_train.sum().item()/bool_train.shape[0]\n",
    "\n",
    "        out_val = model(create_tensor(torch.Tensor(x_val_data),gpu=cuda).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)),False,lang_mod)\n",
    "        out_val = out_val.reshape(len(x_val_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "\n",
    "        classpreds_val = torch.argmax(out_val,dim=1)\n",
    "        bool_val = (classpreds_val==ins_val)\n",
    "        class_acc_val = bool_val.sum().item()/bool_val.shape[0]\n",
    "\n",
    "        loss_train,xent_train,kl_train = vae_loss(ins_train,out_train,model.z_mean,model.z_log_var)\n",
    "        loss_val,xent_test,kl_test = vae_loss(ins_val,out_val,model.z_mean,model.z_log_var)\n",
    "        \n",
    "        losses_train.append(loss_train.item())\n",
    "        losses_test.append(loss_val.item())\n",
    "        accuracies_train.append(class_acc_train)\n",
    "        accuracies_test.append(class_acc_val)\n",
    "        xents_train.append(xent_train.item())\n",
    "        xents_test.append(xent_test.item())\n",
    "        kls_train.append(kl_train.item())\n",
    "        kls_test.append(kl_test.item())\n",
    "        \n",
    "        print(classpreds_train)\n",
    "        print(classpreds_val)\n",
    "        \n",
    "        print('Epoch %s | Training Loss: %s, Training Accuracy: %s, Validation Loss: %s, Validation Accuracy: %s'\n",
    "              %( epoch, loss_train.item(), class_acc_train, loss_val.item(), class_acc_val ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = model(create_tensor(torch.Tensor(x_train_data),gpu=cuda).reshape(-1,PRUNED_SEQ_LENGTH,len(ORDER_LIST)),False)\n",
    "out_train = out_train.reshape(len(x_train_data)*PRUNED_SEQ_LENGTH,len(ORDER_LIST))\n",
    "classpreds_train = torch.argmax(out_train,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classpreds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74041 3690000\n"
     ]
    }
   ],
   "source": [
    "bool_train = (classpreds_train==ins_train)\n",
    "print(bool_train.sum().item(),bool_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1555377790673,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "r-7N7Ysvd7Fq",
    "outputId": "27f070b1-e7de-4531-8fb3-3c9d479fab3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full\n"
     ]
    }
   ],
   "source": [
    "if vae_type == 'full':\n",
    "  run_id = vae_type+'_{}'.format(intermediate_dim)\n",
    "  for i in dropout_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in dropout_dec:\n",
    "    run_id += '_{}'.format(i)\n",
    "elif vae_type == 'conv':\n",
    "  run_id = vae_type+'_enc'\n",
    "  for i in out_conv_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in kernels_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in dilations_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in maxpools_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in paddings_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in out_lin_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in dropout_enc:\n",
    "    run_id += '_{}'.format(i)\n",
    "  \n",
    "  run_id += '_dec'\n",
    "  \n",
    "  for i in out_lin_dec:\n",
    "    run_id += '_{}'.format(i)\n",
    "  for i in dropout_dec:\n",
    "    run_id += '_{}'.format(i)\n",
    "elif vae_type == 'rec':\n",
    "  run_id = vae_type+'_enc'\n",
    "  \n",
    "\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dx0qRaei5TjN"
   },
   "source": [
    "Let's explore the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WrajPV75TjN"
   },
   "outputs": [],
   "source": [
    "if vae_type == \"conv\":\n",
    "    fit_xtrain = model(torch.Tensor(train.view(-1,24,82))).detach()\n",
    "\n",
    "if vae_type == \"full\":\n",
    "    fit_xtrain = model(torch.Tensor(train.view(-1,1968))).detach()\n",
    "    \n",
    "if vae_type == \"rec\":\n",
    "    fit_xtrain = model(train.view(-1,82,24)).detach()\n",
    "    \n",
    "z_means = model.z_mean.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1555377801937,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "zRUn9xRO5TjZ",
    "outputId": "ac0b9978-fbb6-4fa8-e71e-eddaa5e3ec69"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztvX+MXdt1HvZtUqQYtmxHshRGMak3bxAx0LQgaIhxk7JN1MqJ6EySRyBKq6AplMSFmpT9owgCvBkYIgICAWmlQFpABJzASOKkCOzEARjhTTSGJEdBHpG4plyathg86pmhS6oKo9ialAlNkxqe/HHvd+531ux9zj7nnvtr7vqAwdx7zj5777PvOWuvvda31g5FUcDhcDgcy4VDs+6Aw+FwOKYPF/4Oh8OxhHDh73A4HEsIF/4Oh8OxhHDh73A4HEsIF/4Oh8OxhHDh73A4HEsIF/4Oh8OxhHDh73A4HEuI98y6Ayl84AMfKFZXV2fdDYfD4VgofP3rX/83RVF8sKnc3Ar/1dVV3L59e9bdcDgcjoVCCOHXcsq52cfhcDiWEC78HQ6HYwnhwt/hcDiWEC78HQ6HYwnhwt/hcDiWEC78HQ6HYwnhwt/hcDiWEC78HQ6HYwnhwt/hcDiWEC78HY4OuHTj1qy74HCMBRf+DkcH3Lx8YdZdcDjGggt/h8PhWEK48Hc4HI4lhAt/h8PhWEK48Hc4HI4lhAt/h8PhWEK48Hc4HI4lhAt/x0LD+fYORze48HcsNJxv73B0gwt/h8PhWEK48Hc4HI4lhAt/h8PhWEK48HfMNabl0HXHsWPZEIqimHUfojh//nxx+/btWXfD4XA4FgohhK8XRXG+qZxr/g6Hw7GEcOHvOJBwM47DUQ8X/o4DCef/Oxz1cOHvcDgcSwgX/o65hJttHI7JohfhH0K4GEJ4J4TwbghhM3L+94cQfjGE8L0Qwqf6aNNxsNHWbOOThcPRDmML/xDCYQA3APwwgHUAfzKEsG6K/b8A/jSAvztuew5HDHayWN3cnlFPHI7FwHt6qOMHAbxbFMUDAAgh/BSANwDcY4GiKB4Oz73qoT2HoxEPr2/MugsOx1yjD7PP9wN4JN8fD485HA6HY04xVw7fEMJnQwi3Qwi3v/Od78y6O44FgJt3HI5u6EP4fwvAafl+anisNYqi+OtFUZwviuL8Bz/4wR665jjo6NO8405jxzKhD+H/CwA+EkJ4PYRwFMCnAXyxh3odc4b1Kzuz7kI2ugjyvgPDfDJxzDPGFv5FUXwPwP8K4GcB/AsAf68oim+EEK6GEP4YAIQQfk8I4TGAPwHgr4UQvjFuu47p497Vi1jb6sfMMmnBOA8RvrPqg086jhx4Vk/HgcPq5nZnc9ClG7fmYuJwOLoiN6unC3+H44CAGr9PXssNT+nscCwZbl6+sE/w00zXxhTkZqPlwIEW/ovkoJwk5p0OeenGrX2/lQugfvDg2sD8xUkhZ1x1AunLx+OYP7jZx+FIIGX/78svkFNPW1PO2tZ2KfC7tOdYfLjZx9E7Lt24NZFVxCS1/EnU3VYzjvVh/cpOliCOmXLqQMEfa9MFv6OCoijm8u9jH/tY4egPH/3cl2bdhV7xxhfezjo2iXbGLf/am2+17kfdNXW/bey6SYyTY34A4HaRIWPd7OOYS4xD12yCmlJsO+tXdnDm5ImF0pLXr+zg3tWLlWNcoTWNYW45x+LAzT6OVoiZCfowmVy6catTPX2nbVBzlTo/bTv3rl7MtsNPEikTmx7jZyv4iYfXN7LMdC74lxQ5y4NZ/LnZpx7TWLofNPOAvZ83vvB28frmwCxC04mWid1/yvyix1Pj9tHPfak8p59TZev67nCkgEyzz8yFfOrPhf98YB59BXWC9o0vvJ1lU6fQT9Vj27HjkBoXW89rb75VOdbF3h/rm+2/lnvtzbei5x3LARf+C4SuWl3TdbPSFtu021cfx61HNXJb3+ubb5XHX3vzreQqITYhWGHfJPzf+MLbjffigt1RBxf+M0RbQcSXuYtW2IS2mvs8avpN4LjFBGfX+6kTwjm/L69nWTs5dJmspsVwciw2XPgvINoIrpRwmqQm3SRIm+zl04SaQCyaNOeUFq/ncqmXOg6pOmn/19XF65tvZf++0xjrSSgmjsnAhb+jgjqBl6vJKnJt4E11qWacgtYda6fOXv/am29VVgax61XoxvrE/69vDoSymn6a7ks/Wx9C28lSzU2sa9ortVlP6o5muPA/wKAgb9L+J62t5Qju2PmYzVzPxZy4TbATRO4kkToXc/bGtH46dFPlFToB69jppDQJYT6vArtpBdU35nUc+kau8Hee/wTQdwoE5ZVfunELx44cBjDgq995tLuvLHnqffC369IXsJ0mXrw9T1763ce7WNvariR1e/5yryzPcdR75DEdk7WtbZw5eaI8fu/qRTx/uYfVze0KX/7Oo12sbm5jbWsbq5vb5XnWxXtd3dzGudMr5efnL/cq98FzAHAoDI6fO71S1sP7u3n5QqWfD65tVL6zX/efPC37H+Psc3x0nNrEZfSVh6hv3Lt6caoxBosUuDcNuPCfAPp+oPWhvXn5QjKox5btA6kkYSmsbVUFap3QeHBtAw+ubeDMyRNlubOnVkohR4Gq4/nw+gbWtrZx8/KFstyrYjRBUOBygrzzaBeHwuDaQ2FQ56uaoHa2d+fRLi7duIXjRw/jVYFysnj2Yq8SJMbxYfv23u8+Hh1n/7T8zcsX8PzlHu4/eRrtj0bv8r9N0NZXHv+63E2+xeUBRM7yYBZ/bvaJY1x6p+WdW3ShEdYt3a0JKmZeSZ2rq8/WrfUQei96ztrtabbhf5oj6HRtGjPW0ea3qTN98XjbQLCmNh3LAbjZZ/Joq730sb9AKi87NbYmDe3h9Y19ZhWFavpsoynXvl3psF6t/+H1jWhK4YfXN8rrU31n+7zemj+OHz2Mc6dXyjZifdG6uCq4d/VipU83L1/AoQA8e7FXrhDuPh6tHGgy0v0H1q/sZI39+pUd3H28WzEN2fFY3RyYwHhcNf0YUitANWNZ01OqLNt3DLAUe4HkzBCz+Dvomn/bTIxFMZ4Wl6vRp5gsudfUce5z+5i6/xj90dIk2bZ1rlp2jK4WbP9jTCG9Nsbft+PGMnbcc1ZJTRiX7jmvqSNyU1846gHP6jkeZr3xRZusltRS6nwBdeC92v9rW9s4e2qlHAduFKJ9u3TjFu4/eYpnL/Zw/Ohh3Lt6cd/5LuNILZR18j6pMdNGzvbohLUrIy3HY3ce7eL40cNlW6yD7Z05eQJ3Hu2W96Djy7a0nftPnuL5y71ybHie927H486j3X39Td3/udMrZfnccezj2dVnQcdiWpj1+7fI8A3cZ4B5eGBjfdDdncZNlawCrU4grV/ZwbMXe1GTELNNxvqh/V/d3C6FsQpR7QMwYt9QSKpwpwnn7uNdvCoGwp0T1bMXAxaP/Uxo/7XfrO/BtY2KQ5YO3rOnRuf1flKpl+04pMaubqxysbq5jUMB+/pWVz7Vj9j9jIt5eIcWHZ7SeQbIeWgtrbAJKdsj6yFdkfVZGzKQZuzk+CxYB/tBlgoZMbR5sx/EvasXS0FKOzmvy2334fWNMsUyWTi0YwMDQa3+gofXN0p2zfOXexUq4atiMElQyPM/bfyHwqBvZ06ewJmTJ6I0xPUrOxWfCe+TuP/kKY4dOVwK/rWtbdx/8rQcG9I5AVTGQ78THDt7XL832fNjeHh9Y9++vhZaZ1dFoevev30J/qWw2Y8JF/5TgH1hgXwKZUqzslx+yyHXcyqY7cuccvhRoLO89kPro2ZtQb49MKBvKpWRgprQF1UFuzqO7bitbg64/aub2yV/n+cPBZR0ztXNgQDmxEDnMIXr2VODvrP83ce7pcDWceAKhPd29tRKeY6C7t7Vi7h39WK5InhVjCYZxermdoUCqvfHcbt39WJlq0frVE7RO3Ocu00OfF2p1JXTZ8Kea0sR7ht9r0gOIlz4TwFtVgRNx4D9L2VTsBXNM2SS6HUMtNLgKTJoyHJR8LhOPirM1dbOVYEyVyyj5NKNW3j2Ym/f8TMnT5STysPrG5UVBo9RgBI8f/bUCs6dXsHa1mj1QIFNnj6FMu3ZLP+qQBnUxdUGVwy85/tPnpb3fCiMJhBl69C0xOv1fs+dXqnEB+hqqM5EpKDWnxLcqT18OWkSKQ3dts8VV5c4gDot3Pn+s4ML/xZo2llpHMSCdmKBPww00rL8s9GyVpNXEwqvfXBtoxSW7AeFNYObrDmHwo5QkxC18Gcv9vD85V4puGMrj7Wt7VL4sRzL3nm0u08b1nLAQOiq6cdSRo8dGa0e7j7eLU1WOoEcCiMa5rEjo7roO2B9x48eLicgCs/1Kzs4e2pQbm2rujpQR/XNyxfKCePOo8HKYm1ruyJ47e+vqwyam9imrgJSQjd1XM1W/P1jz57V6jlZaSRzDLH3wU5iLvDnAy78WyBm/6yziebYHSm0dUnOl1FfmvUrO6Umq22q1v/g2kZlYlBhqNqnCjVtz97Lw+sbOH70cBntqterQGI/tQ3aljnpsA4VDmdPrZROVB6n0OOqgxOBXqfjQtPPoVD1g5CBxPrZFvt57vTAbEOh9uDaBp6/3NsnEPU3ZH06wXEFcPbUSumYvvNod5+5h6uPc6cH5R5c26iNNNZVhmrq6ovIEaJ1ZXLt61pOJ+UYYu+DnRBUAZnX1BPLABf+E0RTkA4wEtp8IVJLfdqSiZjjj2aKVDuEllHtNIYzJ0+U2mJsMqOpQ+ugYNU+WhMShbr2R4WnmkWs2YSgIHp4fb8gpaDmmHDCio0HzU7HjhzG/SdPy0mKmjw19ONHD5cTMPtNkxn7sn5lp/QnqJZuYVdoSgRgH4DB72PNW7xGJ8WYCcjeZwxKFtDyqX7H/Du2vpTvScvEzFNdhbizg7phqame06KVqf2UQiUW4dm2LnLMLdWwztlmaZJ1fPz1Kzslf101NY0DeFXsZ/BQCOo96jW2f5bTv35lB2dOnijpmedOr5QaMDX6QwGVSF3SI+09Kf2TUBMXx+zYkcN4/nKvbM8mzGO9SpXU+AIAZd/YX+0nf6PYmDH/EI/Hfi/+HpwgY+dzYa+x32O/UaoeIC58rc/AKZzTg1M9MzCth1FfAjogtf0mZ6+1uRO0zavmTYphCnrPtN/STELQ7q6rDRWGLPvg2kaFfql1P3+5VzEp0S5OlgzNWLFxojmF2vydR7tl4BUF/INrG/tWVmr/p1DXWASae2j/p0DiJML/dx+PnMA2dcS50ys4e2qUcI62fGC0Srr7eLfCgqI5aXVzIPgPhZHmTibSsSOHKxOJXTnZsdeJg1Czl0K/K1UWGJkZtS2OA8/rf4X+5rrKUSaWlnXMF5Za+PcNfdjbOMViL4Yei3H3ibWt7UqEKpCm2dlcO2rrp11Z0yOn+vTsxV45QajdHBgJo7OnVkrWDNvQ7Jt0HF+6cQuHwkggAiOOu6VlWq14bWu7TAHNScFSJ62/gFAe/bMXezhz8kSpqdPRffPyhbL+9Ss7pUObpikApVmIxyngOV607XMFQkEPjFYgnGhp3qE/RQU7NWlmQCUdVYPSODEcP3q48vzRWc3fhW2qdq6rQVUwNKNonVKhvgj7XwkGjvmBC/8eEXPEKsZ5+C1bht9pkqEWn3rR1EGrUJMM6yM7Rdsm757cf5pg1GFLbZjmFgpB9oUpldkPClMAFYcsBRwnIZqAiJjdORZJbM0o1LRphrn/5GnJNuL98xr6LV4VowmC90Bzz/qVHdx/8rTiuLbgeNGUtLa1Xa4K7jzaLRlY61d2yt9SU2Twd6BfQtlYjEzmhMc+qmmI9cU4+da/lLLn67OWQop+3HRsmvDJp4qltvk3YZJ2yly2Q52dN5ZrR7VBmyJA7f2aqsBOANq2De4ir5318PpYnbavvM46/PQzhTzvC0ClDYJ2d+0byyp3PmW71/G3Kw4dO03noLZ6tsExSfkKNHWEBp9pSgr2iW3xGH0H6i+gb0J/A73eBs9Zrn5dtHeMhZP6rZowjzb+eezTJOA2/x4wyQclVbfVTmLcfNXS7bXnTq/g3tWLydwwlq1C0DyidEw1/6jg1QhYNR0p3588dvb9wbWRGYOTAe3NXDlwtUC/gPaNq4pDYSBQdXJQcPKwfgwGp7GfNN3oeD97sVeagICRNsvVAuu1NNA6yua9qxdLQa8rEfZTaae8V/aX19Pcwo1v+LvzOJlUysGnOZBQ81zKl8Bxsb4oy+Yi+qCRTgN90koPElzz7wl1WoU652ICue7aJuaFvZYMHSYx04yYXEHwZbbmAbu6ICjAabcmkwUY0R1VAFoGkdatqxWrMav2qoJd29SkanrPNJVY7ZpJ3ugwVtjVktr8VWPXflgmExFbYQCosHjsZGVXBceO7O+nZvXUdrgC03Oc7FiHzbJq/+tvRNjVQy7sb6wMp2XDrFcYrvn3jCZ7Yd2PTW0qh4Nv27TamgoQZdNo2uGzp1aiqRkoHJ6/3KtkymQf6ATUyUqFt9rk6YxVByft+TGmCtuhMxUYaLJqaqF2HxNAmo6BTksNjmP7FNx0mMYC3Ri8xnGjEOZ5nTQJBrvRZm9XJoSyhIBq6odzp0e/i7bBfnOVp2Dddx7tVlYLx44cLu/p5uVBmg6uWNSpHhtTnfSY3kP73wXab2VijYOc5HBt7fip8n1F6gOLs8JwzT8DbWfyVHnVUOteMl5fx5WO2ftjbVPjB0b2bbUv87iWZ14aoLrjlF0NWB9A05jU2d8tz5xara4sVHuOaeZaZ12QkubHV/t9rB2bttra2IGqLZ9mJZsuWlcAnKi1DFdr/K4MLhXWeo86XrEYAe2z1fStn4j9YlxHCjpe+pzqKrILxz+2SmzCJFJKHwRMVfMPIVwMIbwTQng3hLAZOf/eEMJPD8//fAhhtY92p4W2Mzlt2jH7PZCvXdWZFqhRqvNSVwLK6NDNxSm0jh89HBVqZJGQVgiMVgzUPJU3T1s+bfhkHFHAsV5m1LT3T2HCSFnapsm1t9x4ri6s4Fdt3kKPU3DRb6G8eZsQjtexXyowVStVXrxOTkwXDVSzjKpfARispLQOMni0fTtuyrDSczpBsIwywdSZrGMHoJyA6tKS6ApQ4xtiVNE2tnaWafOu5Qj+HI2+T61/kTC28A8hHAZwA8APA1gH8CdDCOum2I8A+G5RFL8LwF8F8GPjtjvvSGlPmkETiKcA0MlDXwYV9Ep9VM45UF05ACPtVScPCiW+tKRyWu43y9IExX1oOdGwDc0RT4G6fmWnNHOcPbVSTizk9tM0Q7MHQYHPulWztqsH3V+XKxbVyJlLBxhNAowPOBRGJiyOp6aWULMM24k5eWN5fM6cPFEmnrOgCYmI5QLiMZrX+NyoOY91q2kwNgnQLKSmLzq7lUjAemJxHilziU4UbNs6iPm9a47/GJrMPXx/mhQtPovLiD40/x8E8G5RFA+KongB4KcAvGHKvAHgJ4effwbAJ0IIB37IY1oMo2IJZYPErucLqg97KphMtSflyfM4g300EhYYvbQqKO2ERGFFTZ/lVMvTYxSAsZw7ZM8oO4iCjGOhWnMqHQRQNbccCgPtlXUxuIr3z6Azatla77Ejh8s0C+pf4aTJdjhppKBsHkb62vP8zbUeWye/P3uxV7av6aMZa6G+mtjzxhUT4xWoWGhsA7A/RTfBZy/G9Fnd3I76KaxGrvs6tEFupHoMNlspYZ/rpnQoBxl9CP/vB/BIvj8eHouWKYriewD+LYDv66HthQOX4gq1pyutDxgJL2roNIdoFCmv01B8anoK7lBlKYCrm9v7Ygm4ylAzhtrB2SbppUxzwP6TuglU7bk02Tx7sVcKQY0w5jUq1Oug2vyrAhW7uUJXD89e7JWOZ2rYap5RCqjVtgGUDnUrwNkP1kMmk46p0lqPHRntmcCy+v340cPlffC4pqXmeOvKgaY3jpuOI58hbjajikPMQc/nTSmzNDWpM73JbKLmnzqN3So54wjl1L2p72N1c7v1hHSQMFdsnxDCZ0MIt0MIt7/zne/Muju9Qu221nYLxHf44jk+qEwNYF822t41IvbsqZXSpMQ8OtbGT3OP5fvry0r2iobvaz3Mk8McQ4xSVWYJhZT6CtgHZrBU7Z+wwoXfNd+OCm3a6XVi4fXUnAGU6SQUjB9g/7gKOH70cEVAcIXAScKubGiu0qheIuUD0rHnJKCBeqxX8/9whcLysZgNhQ1kY/6ku4+r2U45eenkoUkJaarT+8khLxA5rLhxYP0MqRURn+1FYeZMAmOzfUIIvw/AXyqK4pPD71sAUBTFNSnzs8My/yyE8B4A/wrAB4uaxueJ7TNtxJbZGv1ps0pSuFKjA1CyRSwHXFklACrXW0EWY4goCygWL8C+sm7lnhOWGpnD4LFIMXosU4fjBQyEp826GYPGBaT6lDquKwz7nVz+nHvUGAUtbycFGweg+xgAo8yvvG/+TvY34jXKDiOUFQXUZx5NoalMF258HWuOz6qy1SybadZ8/ElhmmyfXwDwkRDC6yGEowA+DeCLpswXAXxm+PlTAH6uTvAfBFgtrw1n2S6PqUlTGPDhVa2a2qnu/6qaOqN+dUcmFQb0RagGrH2KLddp11fHr044alKh7Vwd1er8BeL73RLsL6HtaH9j/Hs19xB1bSlDKlXObjpD6AqD32m+ie3pq7x/1fhpVjlz8kTlN6HpDqhuZck8SZrgjuY/zSWksIIfGCgHNBnyTzV8JR0AI/MTV6f2Ocll/ORMHhY6wcXOxYgLuoo7iIK/DXrh+YcQ/jCA/wPAYQB/oyiKvxxCuArgdlEUXwwhHAPwdwD8AIDfAPDpoige1NW5zJq/heXZ5z601IxUw9FjBCcVakVcRajGpFx1RtnaCFObO5/ceY1cZdvUyFkHkNZ2Y9DApdiqxQq71ArDRg+nVkF6XV3/1GbP77GIYdtvFWQ6fhpNrONrVxj23oHR76E0TF25Wa6/RjBr/qAmjBvX0lTvQdXQJ4Vczd+DvOYAdoMWoCpIY4FQdS+UdchZRpC1scdedE3EptRJzaljBQf7re0yIZoGMlHwKVQIagAb26W5hH3QQCpgf0ZKFfzatjWP8LwVzNakZAOx2Kbm4qkzIzVNGgo7Uanw53jopGmvBaqTvP7WmitIE+2lTDopqGDW5HNEKgCrTpCnJqN5wCJNQC785xT6EKUodDYCM1UWiO/sFcvZk1o92P4QauOPRfnyvGa8BOoFE4WfjTC2dDsr6Ajdbctqzuwnj8Vs48Bo1ywgrjk3adSx/EAcp9RkYs+ldgyLRR/rWLOMjnfsWt1FjO2kFIcmJSL27Fhhn3pWYxOQPqt9O3eJNjuRWVprlz51XdlMCi78Z4w2D1JO2dgD1uU6fZGttmdfUr5EMYGqqwCb5tnCOqg5QdF0pMKsTktOOXltOgTbNu28aqqKgYIZiK8u7L1of3Ogwj41ydiViU6Ysf7E7j2WLiKW7C8Gu/2mjWq2zlI9r2keeDz2/AH795Gw0Gdu1ttBLloaCU/sNmO0eUBt2Vh4fUyzUP41sD8Fbwx3Ho2ofaQjUnBQc2REL7nwDIpiWoJUSmUrgDRFwqUbt8pI2ucvR4FLNy9fqDjhKHzpULVOXnXuqqM45VB/VYzoqMeOHK44nC0YlMaU0s9f7pXce6U+6n2eOXkiGaRnj2vblhFEkH7JCVZXSrGJiL+V0j3p2KSTnPel8RSrm9tlnAbTbwMjXwApuCrYY4Jf+64BX9avpNBgxDrQaRsLZFTYmIW+sUiCvw1c858ScjWWttq8amPUUGzaZF0CqzavydMUVvuzG9BbJ2Iq+ZlNDmft8Jq+WM041mSj11AbjqVoJmJatR5LbbCSgnUQq1PUpo+2/glgv/M3BV2lpFZRNhmftY0rJVj7ZR3FMd9JzEkOVIW9jgcFPFdu2gZXdHWbx+h9NKEvjb9LPYtk7wfc7LMw6Ns8BOxn+ehkoc5gvri6OUiK4RGz/6pt2SYmA0ZsFZoCbCbSVGRonZBU4QVUy8WC53Jh7fKxySXWr5QfgH2IMZhi19jrdSKMmb9iJiC9F5vJNcVk0vqteYbPifUraQBVLHtpLNusNQ8RmqIipogodDxjJIhcsN+2ra6m1XmDm33mAG2XoimOND/rQ8jo3VSg09rWdiX/OyN9aSpixK0mUNPMk/zPP12q80XVF4hxBOSlMwKVe/2yfq2bYAZMwnLlbYoDCmFqmzR51I1JHayAZT4dWyYWBMcxVnMU+6FxCCpgOZZ6jyyreyTQVk/znBWebJMxAvxMs5pt59KNW5WUEkA1eZ3WrXmOrNmD43vm5IkyNoTmLyoB/D10/HgtY1HUTMSI27p35lCIZxXtAmY61WclNvEsmuBvA9f8Fxwx9pCydGwZaj2WpmhpjDyuGrCNUFWzDkHtiRo/TQ4ppy6vb1oF2CjdOi08x5RjVx4xbTzmfG7S2u31On7W3ERBacc+dW82VoOfbZvPX+7to8Yy3kLvR01I9jmidq2rBrtSBPbvvRDT4C01mG2kmDZ2pWpNj31hEbX6HLjZZ86RYkMQuXS1GFIPtX0J65g/PA9gXyqEHFqjFZKpa3QZrzZlFWCKFLfemldyJgBgZPu2PgmbYC3mbNXJLLVNpArnmMkq5Yuw1yqllBHcGphWt9qJmdjsBjQA9pmS1FRkBbxOAjFTojUftqVDdhXMB1Wgt4GbfeYANLXEwBdB2TbEpRu3srINXrpxa98y2SayIpuDbdkALDI7GLqv+wvcvHxhH6ebbCAyYFSDBEbL/7uPd0umjBVqvF4TqekYMOumDbw6frSaBz+mjWsiNGDEOrKguYKmKMsC0gyhth88z8yeNBOp+er40f0J+DiR6KSkDm4ynYD9Gr1+p+kEiCdxYzoJZSnZMVBHNdMy6KqEdZP1dfPyhTIdB82FyhKjqVFZXPpsphhRvB+W5bNqr8/FJAT/JJlEs4QL/0zU0SdTaOJU05ZuX8ybl9PZDdkPZUso1c22d//J01JDUwHH48zYqVQ/anBrWyMhQ7sz62B+eGBEEWV/gCrDSIXa2tYoLoDaMq+n0OLY6X9LdVQbe8wxSzx7sRfViq3j02b9TEFoDtCrAAAgAElEQVTpjczsqW2RUlmXE0gnCGD0m9kU0Jrrh981dXMKWp+m2GabnOip2evq6/jRw5U9A1SjpxPZ7nTGPuvmNbZ/qedZ6aSso+matmjrA7I4qCsJF/6Z6GJvzLkmpVWkjttkVTyW0pSUJ63cfNrZYxHGPE5zCPngdx7tlgngVGAoVjdHefyt2QcYpSa+eXm0sxjrev5yr9ztS3nuMaQybT68vlEKEBXiKc03dky1cb1WtX69J1vWChttm5qxrmxoVz9+dLS15qFQ3eHLTkip4xoTEHNSP3+5VzrFuSub3sOzF3vlSpDg83Dn0SC3vyYR5D3xd6cTOLUpEFBNcpjL+R8Hk4y+7aIUzgtc+E8RurwF0lkJNfd93ZIzd2msL5a+CCq0NJHbudMrla34uIxXrVTTJPA7BRhNAA+vD/YfUEcjUE3vTO0SGJlgaEpKBULZ7yq41HavQrruJaUJSKGTS4yhxDI6QZ07vZIMNrMavd6DFfQAyiAtwq4y9Dg1eQDlTm0ETTX2HjlJsk5VVPj76DaazBKrWUP5X58pPmvc3If3oONS58uyK1lCTZzjmmHqVgKxc3XtLXIAmAv/CSFFwbRRuWri4TXUrKhxNaWDrns4687pS75+ZaeiwdFUw+hRXSWwn8eOjFIyU2ulmYWbyTCls02/zDpS/bOCjiYqQicGFZoxAck+xBDT1G36aCBuCnp4faPSjzuPdpO+Gk2uZicnsn6U7nn/ydNkfiS2Tei+0M9e7JWmu2cv9krap93pTPuhikbsGeUqhW0w5bNVZDjp0yehvhSOiz7LMV8XYVcCMVNo10mgbiWwTHRPF/4TQswcAlTNMArVoDREHtivKdmXpO3SmdfzxSTnWp2jZ0+tRFcp/KwaL1M268tMTZZ2fH63e75yBVFnomGf67JY5kJt1UB8Q3amg7ATgF6n1xLU4PU7gIpgjvWH40TWDifRWFny/nWs1XlLLZ2mJPL6uUKwPgTy3VUonzl5Amtb26UJR8fBRiFrupBLN26VZkEr2Hlv6ihmOY1BUWd2E+ZRKC+Sc9ipnnOIFF3NcrGB6guQw4W2PHxNdma542pKiGUK1UAmUv9Yv6YNVjaIZuTkpBFLHUCoSUd557GJoI6DbymVllvfJkEbkaJyppCij+Yki1MBznI2uZ6mkbCpGyyVNpYOOxYjQujxGG9fnxEbJ2CpxfbzrBOntaWhzjuc6jkn6IOupoJeqXC2XNMLpKsLauAPrm3sYxxZ+/CzF3sVAU0KICMugarwYhIwSydU4cVzmn5C2SzASEipnZnadYw6WGcm0fQT3OOY4CpGzT1Wy4+B90uNO8YY0nosfZRjrn3judgOYUB1taFatGUuKTtKBT/ZXQR9LjqZax4nPnuqGMTos3xG1G+k42CFvSUs6Jg2OVFjFOdxMCnBP++rABf+E0YfS1Otg45ZoL2tv8lBTKYPl+/64vOlX7+yU1I+VYByYtEgJFIJyRG3AvXZiz2sX9nBoTDazD0FCn0KOY06TglqTiY2opisFUKTkrFepafWtcH6lClk20/dD8c3RkdNObtj9Wk8BVDd4J4TL8tw4raUTKXtAiNtmBu80xzD35fPCp2zKVMdlQJLidXn0DrUU6ZRgs/UvAvXHNLGLOFmnwVGXTRj0zlg/yYqqTpiedyBaiI4XZ2kIod1Wz+gmj6B2l8qYVlTlG1dub4Ry5qZW74N6iJ+m8rHvrMeFUaxjWFi1+smPfZ+rNlIU1HEnheWtectxkntsMyRvm72mRN0nfX1ulQddQ+3PadLaWpOdY5krUNNDvrS0mRk2UgUAkoH1SAhaqV0KFIT1ElBWS2W70/zTExoTVrwA1Xzko3sJSwNNoY6RpE1w61tbTeW5/jyWjUtqf/m5uULJWMnlhWV40vWEFB1KlvaL1dRpIMSOlnxeQGwT1ngedZB8JwV/PY5VXOjvXZSWvelG7cWmuMPuOa/EJiG5hNrQzX42H+WoTZol+Kx7f6s1gygohlaIUNzU0yDZftAWsCmYB2nOXmAFNb5OQ5I7Ww7cdFxrauvmPNcHa8UWOrwtqsK+101eU2HrGWY38g6cdX5q7DPp363q4c6HEQNf9x78sRuBwhND0NfbAUKBn2JWX/dy2iZHjYeQDeUibF1YpOAbgtp2TCsM5b5UtNPWAZMSsB3NctY2ARvFLaaaC6VeC6VkdQmYEuNH80jlj1l640lzeP4pyYgva9UBLMmf9Nr7LPAsvo8pPwFOaYhew3QLwVUlYtYP8dJwDgpuNlnwVEX8GKhS/omc5EGBMWgwkPZHnUONjI9gOrWktZkQEF+8/KFMika+e28juYBTVhG9godmRRkNB/R/HH/ydMKo8YyYOgAtS9xV8Gv8QmHwsA8oWYg5azT1h1Lu6AJ66wZhv2O9ZemMf42TM1g4y0YzUsTGym2bOPOo13cfbxbibzmOV6jGj3vl2XWr+zsi0COpXfgJKP+Ix5LmRztZyJmcokx4OrQtGpjpD3ZcLHy8yb428A1/ylCl+WxILAu2nudVtSmzpSmZTUb8vOVkUHHsTp0lWuuW0Lqbl4Wap7QOABq88xLr9z2Y0fiexFYTMI5rMyXul2/NK5Atfa6uIXYSsGOm83xr/0CsG8VYvn5NthKYfdgUDObEgUohG16aTUxqe+hq6Y8qViASWruszJJudlnCdDXw6XBVwRfYNtGakKxm8TwWmr0z1/ulcKbQlBXE3cf75aTigrEOpOMBoDVwe4x0MXMk8OkIbpsJhMzB+keBak9i5WFw2uA0QosZ68FazYC9q+GdKKyCozdW0Bht4AERvl7Uo7cVJyLnrOBh20w66CyScOF/wIi5RwbBzFhbR23dXZXReylVGGqdM4YTc++dJYKStMOMDI/6WShbcU0aKB5c3Z7rC9qaJ2/YVzo2MQmCLXxszxt8HS6k5EViy62iff4G1mlQO3fsfuzNFJVAoC0+XLadvOuStO4yta0Jh23+c85rGClfTEnsrQOMWcc2+M5a5+3gl8zet68fKGMALX1a2IwKzg4uah5SCN1eYx2fNZ75uQJ3Lt6sQwuslGz9588Lf0EpISq/VeDszhp6JjS9s22uwp+G7RG2qQma7P0T0vVtN/t70CfAk1cpMZqH2jj141b1q/slKsBBpA9uLZRZgrV2IGzp0aUTu7DzN+GeXqAkeDTiG4dR/U/8JnRsakTmk2TZNtgRnu+DSOrKbhsHMzbasM1/xlj1jlO6gJpUv2hxq6OV2A0mVhmiNqYUzxwNRPFtFKgP1ZOV9j+xPoU62ObSaYumIvnNEeQtqH+h5hJzDKHrDnFUkXtvbBd9b0A1echxurS4zG6cAwHkcI5LbjmvyDQB7yr4NcgqzrNJSZUNJDm0o1bFTaQNdHoNQ+vD3L1P7y+UWp2zA6q7AibE0brI5tEnYNsU3eUAqr2b2X45JjI6nbmagtdWQBV5o32Ue+1zepChTvHhvXxd3lwbWPfyuLMyRP7zIb2edKVkGryTPmgsIKfqwROHurwvf/kaTkxakoQ/tdUIXze6qJ6eV77VMdS47MT2wfA1guMv7NXW3h6h5ZYFs2/D8Rs9jl21JR2lbLNW6efbds6agmbRiLmcwBG2qhljFjhqUwU+xnAPqFV97LHtNuUGYLCtkn717QVtu9Nq4DY+ZwVjzKf1LFu/SHAyPmu425ZOxxTy/G3sJlDu2jsqYAvDfpTPwafnVgQYd+MuUWEO3wdncBVhJ04+OJp6mYg/sLXvYRKAwUGwoNRqkA1Y6cNGNJIYhtURLBsXc4aoFmg2sAl1epjppTYJJNiI2nbakqyE6c60W2OH2035jiPRQ2rqUfjMepYN3acrQ8AqNJ/tQ5O4jaIz14PYB9RwPan7vmyq4W+SRN1mMeJI1f4v2canXHkISU0czWaLpqPCmMGWFGI6YOtNlugyhW3Jp6YXZ9CmQLIcsxT+XJsFk8N/rr7eCSMSXnkRiwpNDGAqN3T2UwGj1Ih7aSk90lYnryC98dJj4JY75FQGmXsXvS8BnhpXiX2h2N29tRKNEVDbN8G3Y+Bvz21bmViWbAO9kt3htPzPBbbm5rQiYrmwth48nOsP5PCvAn+NnDNfwKYR22ASPH3U4iVs9ztWHbQlAanS3XLGwfqc+ZQkDBldJN5xFIagSpFEqiP7m3aNMauSGLpEeyKQfsUW0Xo5Kt1pkxIdZvSaNvAKJ+PZlG1wlgJAGoesvEb9rfVjJ0pE6FFKrBQzT42b1EKqefQ1p06N+47O0/vvJt9Fhh0jirfOpcH3WTH1/9dg2RS7aTatgFB2g97ryxfh5jgzmUC2chZPQ6MVg91eYDuP3m6z7SUmkxUCOr5WIrkuhiFmKBXzd2akmJ8fqAadWy1fPaFAXc2cMvmcAKqmm8sJ5OuDGKCPoYcgaxKjO1HHYNtGeDCfw4wT9oA0bVPVtu3vP+mLIyatgEY5bpReiiFqnXYsu2Yw8/awtWcAqBW67ZtNUXSaloJ3kMsklb7HhP8HK9YkJptU4W1tqO2fivY6Ufgveh9qdC3Udmx/P0pGrIGfOmKDKhmZ40lhdO+x2z3rNfGjhBWgZiUsJ/H9zcHLvwdAPrLdJh6sazwj238Yl9S6yfQNMFsywrhmGO0iTmTs6dubr4fa06h0LcmKhWaqv3W9TU2OdRNRLr6qJuoVEBbH41q11zBWN+NpnuIOWcVWp8tn4I6plOMolhsyLScuYsK5/kfEIzLEW6KrmwCudOxDb35IpLrD2Bf4A8w0lY5EVi+PwX66uZ2ORGwvofXB5k4H1zbqNAQH17fKJ2IVhjwO4Wu3RzGXkOOvsJGWqvgv/t4t9xbmIKfEbavitFmNsBgwmJULetk5K6aQ1TTBararh7Xsb55+cK+GAby8Vc3t8t6WUYFp+XRsw0dB67G7j4eOdkv3RhtysM6OYHEoG3Q0c94EvaDK5Fzp1cqTmWbIE43i5lX7vwiYWmE/yI8LLE+Krsl59q+75NBW1q/3VVK0ytwv1cGelmcOXkCZ06eKO/p/pOneHh9o8LSoBDg9ToGdx6NMohSW9X+WDs400VQ+PIc+6zpDnTHMJ1Y9F7vPBrtXXzu9GjDd3XUUrBxgrKTIDe3v/t4cM9rW6O8PaqhU7jrikHTYAAoUzawn9TA+dlOarFUyA+ubexbwXDyvnf1Ih5cGwXVUZmgEGd/da9epuZgectAYioMfT5Y/v6Tp6VJzwarqdmRgWl1gV2KRXj/p42xqJ4hhPcD+GkAqwAeAvjviqL4bqTcDoDfC+Dtoij+yDhtdkVftrtp2AHVFqvZL61ZRdGGsRBzqOVcqxuT6HV2VVCnRbNda1O298m6KQRUoNOmzHqpbfMamoiAgTDR6GHWY7Ndci8Ammko4Ml6oVN6batKS6Vw0yhrOjmVJQPsp8fyM/um48dr1HbOPuv1XDGoqcyO4bEjhyv3zevtb89xVns8haZN6cD8SsCIDpsy2djVViry2zqPFXpP475/bjoaYCybfwjh8wB+oyiK6yGETQDvK4rizUi5TwA4DuB/zhX+y2LzzwlKUVu5Zca0gXWU2fqb+lk3wTS9UDYorC4zpDoMFTZPvZ1AUu3R9k4HrbJmbEpktsOVAk0fnDis0AaqNnudqGKMntgOXrofgrZn70XploSypmJ+FJuCWSN/U05cjZiOPZvWvp+iWBKWBcRnwN6nfca6RPHGzi+bsJ+KwzeE8A6AjxdF8e0QwocAfK0oit+dKPtxAH/Rhf8A1KKahL8KsTbIYT6ocy4mCOiIs9GVsURd2i4wMktYOqRGYVq+vBUomr5BmTUpx7Nlk1g2kPZP9w1Qxk6M6kihTQZNTCBZh65l8Oh5nZAA7JuIAFTGR/n+mkRPUzVb4RuLArbCl7DxCTGhXzfp855smRgLxz7Pek3OPtJN/ekDi04RnZbw3y2KYmX4OQD4Lr9Hyn4cDcI/hPBZAJ8FgA9/+MMf+7Vf+7XOfZtHxDSQmBmnjtc/zoOZemFSm7kA6fQNlv+d6lNTgA2AyiQTSyMRS2GgfY8FRlG7V5YQhaHm6OF+tgQ1frKTVKuNsZXYJu+DdnsVrpb5o/dg2VK2Tkt5jI2H3rfdl9fSYi0lV89xNcAJDti/n7NeF/s9NF0H+27TNqhJxyZ9yxHqTSvgmIl0UWmbXdCb8A8hfAXA74ic+lEAP6nCPoTw3aIo3peo5+NwzT8K+2LwhWir6aReyLr2gKrWrFqfRtE28fi13pgvwbappgCWA+L5fGKpgokY5VKpjlaDtumOdZWg+xfbKFieJ7Q+jo9l7Cifnu0pdAJRAa9jlqK7WthNcHRC1sndbp5uYVc+djWldev1ep5txPL1WFOOmqlUmKt/SRWhOsrxpE07izKB9Eb1LIrih4qi+M8jf/8QwJOhuQfD//96/K4vF+wD9aqIP2Sxh84yN+i0q3sJ1q/s7KP5aQphy7KwVM46xIQ8P2t+lvUrO3hwbaMsr31hymHSPY8fHW12rsLmzqPdkiZIUACePbVS2Wic9SjrRgUIWS3MjfPw+kZl4xlg5Ge4/+RpWR/vjZ+5ifu9qxcr2jOPcYzJzNH7IStp/coO7j7e3edY5qrh3tWLJbuHvxMnKhWoOi6aRE/TLpNQQKorJ3jm/lF2FIW2Uj15jdJHuZHM6uZ2OUFwQlWns7KtOEb6fPDZU8HPOmNoeub7wCII/lYoiqLzH4C/AmBz+HkTwOdryn4cwFu5dX/sYx8rDhpee/OtoiiK4o0vvF1b7qOf+1JjmTZ4ffOtyv9cvPbmW2WfU3jjC283limKwT3Fro0dY1m9JqcNC3vNG194u+zv65uDe9Ox5vdY/974wtvl+Gl5+6dts65YefYl1t/Yveq42H6yPnttrB69X/ubaN+KYvC82DIf/dyX9j1HdszsuGmdvA/2zY5pDE3PberaLs9MH9fOGgBuFznyO6dQ8mLg+wB8FcA3AXwFwPuHx88D+Akp908BfAfAbwJ4DOCTTXUfROFPxISSfrYvS+zF7gNtJ4OYANe6UoIz1e/US9tm4tM2U4JEx9L+pzDkuOvY275reQowe11T39lHnXhSQjrVh9c336oI8NTkoveaI7Bj46YTkhXY2j8V6Fq/7XtsLGybsWOx+6gba73/RRbkXZAr/D29Q8+IpcUFmpeMam+3uWhynMSpOpvsoDa1QqpfMUZQXXIt20drg1ZnblP/7HjmxjLoWCotVKmmZNfocctESbFQYmkJeBxAhRlknZpKHwWqbKTU70FfBJPKWZqrtqF+i5hfITZeRIrlxX7a3zbG4GE/mvwDWt7mdGI7427wvujsnbbw3D5zhDo6W8wRSmcZ0N+mz9N4AVSopfLCWCdsjPKXgqYZ1vI6iaS44laYpZg86mRNObktXdSyXWwAmaZ20H6kqKDAfqpoW2emZVLFiAAxxzbRFMsRm7jt5JmideYqRONg2bj9Chf+C44mYTiuNtQGMUZGiqXBc0TqBYwJgLqYhth4WBaKrRtAdKKpSxamLBNSToFqRk9lsvAadbCm+qIKAIAKm8XGCujEkxrPGLtLJ9fYKoPC3q4wYvEadbEMqd/I3meK36/tp36/OorwgXO+9ohc4T+WzX+Sf4tq8+/qqLX29Lb2+EkgZreNIWVXtlB7t23DOkdTbaTqtDbrmP1e24nVr32xduaUL6bOD6LlUs5nOyY6hq9vvlU7pk1tx/pi/RQK6wBPtaU+hpR/Ra+L3b8lP2h/1H+Q6kufhIiDBkzD4TvJv0UV/m2Q4xTNubYJ1uk2rRenSZArCykm6OqcfymmTOraJieurdsKOLKDYmOpzt86xPqkjlsrKK0T1V5j62UfiZQDvq5vuc9GnaPbMpNy6iJyGGax6/osu+hw4T9l2Ad42pp7H8yZputzBIQKs64vcRPVM0VVtMiloKYEjm1TtdE6IdzE3rH3F9N0Y/TRurZj/aZgjo1TSvPPWc3UrZ4sKyj3t6qja7Z5n5ZJyKfgwn8KsMIwxoG2GJe73haTeBmaKJ+2bErTrqtbBUcMMWHcF7TPqtVb84UVdlrGat2xCc3eR46Ay5nkU/XE2tb/TeNZN+E1rT5yn/W2pizHfrjwnxGsgGiyZbe1X+YGS03iJeprddFkJ+5SZ+71qUAuFYAqqOuCvpq02piJKBWEpWafmHavfbHcfk6UOWMWi4XQGI0mIV33vGof9B7qgtdi16d8EbasIw4X/hNGjnC1gr+NkK4TXrHrYsv4VN25Eb+52lquQzj1PaeelCbdBN6jFeRNfcmpv074WwGujt1cc1TTOfU31PUxpbGrOSWlrVuBrse1Ll4XG099zmIBaLH77Xs1pzjoQV8u/OccdZpMjk099QA3MSVYJiVcYuH8dWjyA+SaIHJQl5qASNnu69qzQiimfbaNYI6NR9tI5xRipsNUJK32uy7K114X+111HFMrmBjsRGPrjZW3n9Wk2jRe88CUmyVc+HdE7os4CVNFW/TZh3l/YXJXF00aZErbjJkpeNzSL3P6Zc0ftm2LJvNgXRt6vf3cdJ09nvIDpK7VyYeKR5uJfdbmm4PoY3Dhn4k+Hr6+NLq6Ouo03jpqYE6dWoctE7vWXp+zRM8RgDltKWIrnJR2ac0vRI7PpUnjzNFiU2aXNs7epv6l6o4xcFKrm7bUTCJljsytI9auva8Y3daxHy78O6DJpNBXIFbsxatDiimSqtu2kXNd6l7qNOW6sk1mkRw0TRg5E0qdXTnnmrpjTdDJqc55rGWbxqqNw7iOaZXrH4r11R7vS8mZ9SrgoMCFf0/I0YBtuRxnXZ+w/UjZoHNS46oQidWfg5T9uQldX/5c4V8UcS57bAWRGsPcFY7WEUuN3BapSSHmsI2V6eKUV9NP7HmI2fF1lZTDHNI+1vmiumIZJxUX/j0j9lBayh3RpDku0rI110lXh3Foem20S9V060xxqYCrGM0z1Xdrfqj7TZtMO1Ybz1l5NJnp1IfQZDKrq5uCX00ulg2kAWltTaB1k8w4yDErHVS48J8wYtS4VLmY1p3zkFtB1eX6HIxr5piU06xJa2ujhadgVzk2qKttfba8rd8Gr7URSn0IMOsPsJNrbFKxTt1Yn1JjE5tom8a1zX2mfDmpupcBLvznEDkCJWaOoAaa65RtalOv7UtDSvVlGhpXW63Ylk1RIFMas45pLBI5x2RmHbJtoO3HHNZN9nud4Ajr1K67p1yfGJGi3/aBpud7GeHCf0Koc/qOYye3sNphTFuz55v6EdM4Y0Ko64TQlolRN15tnNyptnMmPZ1Um5g/TQKS/YgJzpSJUNFWkNfB9iOHCaZcesIK19xI4KZ2HJODC/8Jwr7842jgtk79nhLGag6apS1TBWhRjJx2uRNfk8lAy+SWbwPV+i1bJibEVcuPTaQ5/RpH29d+2GNNz2KTySVm54+tiuraSKHJDzCurX/eY1SmDRf+U0bMOWi1LUv105csZhu12nxKk03xxesEQp0DrovjblzUrQKakry1bSMm6PhbUeDFWEFavm27Mdol/5oib21dbdpv+u3tb62ToT3WBqoQqCJjo42X2TwzKbjwb4lxl7F8uVVQKTtC6XYp7csKIQt9iYg2gqMNUhNTLvp4qZsEbZc27G/T9Gev4femti0rpiuaiAW5q84+ytiybcyBfbQ7zjXLBBf+LRCzL7fhHKsWX6clqXYe03RtAqyU4FXhk7OMt+VyBeokbbMpmuQkHYG2HR3jOuFv69X/fSF3rFOpJvo0/+UEfM0DdXIe+jCPcOE/YVhhGrPDq6Cw9vG6iE8rlNr0JXVNrrCywq3LC9Y0CVp0mWTsWOe2p9fF/nQisJO6HQu7ouuC3HvXfnfN3ZML16wXGy78ZwSrxVvHrTXdWEGpgiim1ee+mDHKaB1iwrMvzb9tPanVTNdruUUkBaf6NWJ/GpEbM7Vp3bkTrbbb9p76wCzNQ+PU4WgPF/5jIqZpxV72mMmG1+uLTmFuHX3KMGE9XQT8OKhbxRTF5JfXbQVKFwGUcvYWRVFr9qlLV6CruJg5JuY3sP2qQ5dVX1vkriL6Ethd6sldATsGcOGfibYBKzn1qIDndzUrxMxB/J6DNteNK3RSL12KBlgU9UFSbV/+NoFDObmL6uz8PK+pDFSA2xQQ7IddyXW5z0kipaA0lS+Kfpy6junChX9PiGlvMUony/K/MndSduGUeSAXTRpRjiBKndN7GQez0BhjAVWx/yr09U+1fp1o68w/TagTvF1+n0nAmTcHAy78J4g6W6Z1KLY1U3RpUzErm3Ib1EXv5txnTsBSLNhM7f52Urbsqljkr3X8amBY7n1MCpNob56fIUcaucL/EBxZuHTjVvn55uUL0XM3L1/A85d7+8ra8ql69dj6lZ3KMVuHPc96Hl7fSLal18Taze1fm/Mx3Lt6sfIfAFY3twGM7tPWq/d/5uSJ8vPa1nalzOrmNh5e36jUfenGLaxubuPBtcHYPHuxh3OnV3DsyOGy7XOnV8pz/K9jeffxLu4/eYoH1zbw8PoGVje3ce/qRZw5eSL6+/KY/Z0mhbpnbJ7qdMwPwmCimD+cP3++uH379qy7sQ+XbtyqfSkofNa2tnHsyGE8e7GHQwE4e2oF9588rQgl1nfn0S7OnV7BzcsXsH5lZ18ZCy1DoVkn9Ne2RoIv594osNjGpRu3yr7z/maBprGvK8970N+D43738S5eFaj8TpwEzp0elDt+9DDuXb1Yjj3rtn3i78Hfs0u/p42cZ86xOAghfL0oivONBXOWB7P4mxezj2XijOsAy7Xzx8rFKKAps09df2IOvVks8cehdNry+hvF0mpY563a9ZV1FXPysjz/x+z3NBnZ37Lpnty04ugbcJt/N+S8rDGhFaOAWiaIRZNzN4dFUueYbWJ3WO75pKE2+dTnLn4Sew/W4Zti5/A6ZfGov6ArldYyvXKvcTj6QK7wd7NPJtQGfefRLoCRqUWX+jyvpjmuWqIAAA5RSURBVBGaSnT5z6X2+pWdpN24D6hZSNtXO7e2zX7F+jxp0ATWxawU6+f6lZ195pvYPVtTjZp0CFt+VqYvoo/fJdcc6Fgs5Jp93OGbCX3RHl7fKF9+OhwfXt/A/SdPcffxbmVSoGBZ3dyu2IDptFS7OkEHpaKr4/Xc6ZWKENd61RnN6ykseQ/TtFXfvHyhIlRznaVrW9vlhKw4c/JEef/3nzwFMJiYeU9rW4Pf51CI+0xi925/F6KL45t9UKxf2amtS8kF46JvwT8t57ajJ+QsD2bxNy82/xQ07W+MH66wJoCUOcFyyidtkmlj+x/HLNFnvvWYScUG2KUS9fF/LHNqju8jZsrz4CbHvAFO9Zwszp5awaEw0CT5xxXBsSOH92nY+t9SHJXmSA314fUN3Lx8oTRT5CJX+2I5tq+afkzzvPt4t7N227eGyTG6efkC1ra2K9TPu493o8yV5y/3yv/3rl7E8aMDmqelyN59vH8FQVhz39rWfvPPpRu3Oo9TW0yrHccBRc4MMYu/WWj+bbXbWKCQZXzo6qCtlmg18yYmSdeAslSG0Zy6puWoTGVAtbBsKJa3429XaPPicG3r6HY4LDANtg+A9wP4MoBvDv+/L1LmHIB/BuAbAO4C+O9z6p53s4+iLneKFaQ5eVPqBH0fL3yb1A19RSh3QVumTCzyN5dOmpt+I3VtqqxvMeiYNqYl/D8PYHP4eRPAj0XKnAHwkeHn3wng2wBWmuqehvDvookzxN8K8ab9aykEmjZ7YZ9iMQU5W9/lbMTBturON13f5XwOxs0vM+00BzZR3KTh2r6jCdMS/u8A+NDw84cAvJNxzS9xMqj7m7TwT2nSfb1cbXYCs+3rn/apTsPsA11XFzmrmb4Ry7CpfYn1qalv9nzXbRi7/kZ9/baTfk4c841pCf9d+Rz0e6L8DwL4FwAONdXdp/DPZbC0icpNBXs1aYn8b+3POX0c94WOZSPt4udog3H63JV5VJc4rq6O2ESiE28TmsqknqO68w5HW/Qm/AF8BcCvRP7esMIewHdr6vnQcKXwe2vKfBbAbQC3P/zhD09+lMZA04rBfqcZx27wXkf77LN/TajrS58Yl0pq7fqx61KmtS5Ruzbit831aiZ0bdwxLcyV2QfAfwLgFwF8KrfueXf4xlYNTRxxLTsNQUvktNVGME2q703O0ZhmrWNp76FNnp3U75diGeU4lZVhNElzmDuVHYppCf+/Yhy+n4+UOQrgqwD+tzZ1z1r4t3lZ1dkXS75WFHHhb5OIjYs6bdiapGLU01zttE8Ndpy66sZNt1Icty5FTn/522rZaU72juXGtIT/9w0F+zeH5qH3D4+fB/ATw89/CsBLAHfk71xT3bMW/m2hgpeZIuuQk31zEohFsjYJ/UXVLK3jvO21sc99Inf/XIejDaYi/Cf5t0jCXwV2agP2SQnQtjZo7Ucs/UFOHbNC3d7AMTQ5WHOvifXBHov5E2bN+nEsJ1z4j4lFEIZdkWLDjLPqmMY4tG0jNhF37afGX6TO5a4yFvGZcSwOXPjPGbowTXLqzIFt16Y1yAke6wtd2ujLHp9aldXVpeVzVm8s7zZ+x6zgwr8n9OGQbUtvnLTdf9o00j7qmKSTOVW3Ujs5ScaEe1u6bptJyOFoCxf+PWMSE8AsHKkHWeh0/Y3GZeWkzD05dVkHvKeIdowLF/49o0s0bG6942KeBMYkJrRx7Oi51MxYOogY4yf1Pwe5cSAOxzhw4d8T+n4p+0oANk8C/yChrxQaNo6iblJ0we/oEy78J4x5cOg5s6QebaihTWa5LrRRWzYWFe5w9A0X/hPEuIJ/mgI5x7k5zxNEThSvlm0rUNuktlYBPm6Oorrr5vn3cMw/XPhPCX3ZfOcZi9rvHLQR4sr0ca3dMa/IFf5hUHb+cP78+eL27duz7sbS4NKNW+Uew22wfmUnumfupKH7Ds9TfV3H0eHoCyGErxdFcb6pnG/g3gF24+yDsJF2V4F17+rFyqbxa1vbNaX7Q5+Cv019Tb91H4I/53k6CM+cY7Zw4d8BfMH5Ai67pkfN/9KNW3g1nwvJ3tD0W/OZGEc45zxPy/7MOcaHC/8x0PUFXBStbXVzu7avMS2/b418XFy6cWtfPye5OuEzEXs2FuV3dywH3ObvWGi4jd3hqMJt/o5e0EZb7cPk0RZ1gv+gaNqrm9PxoziWC675LxFcS3Y4Dj5c8+8BB0VzXGQsstbrz49jnuGav2OhMMnVi6+MHAcBrvk7DgxUg56kcJ5nwe+rCEffcM3f4XA4DhBc83csLTTieN7RJebAVwGOPuDC/wBjWYXELHINtQV/mwfX2gfFzbN5yrE4cOG/AOgakZoSEm0nhWlOIl3aWiRNn3AB7pg13ObvWGgcBIbOpRu3cOfR7tylxnAsJnJt/i78HQ6H4wDBHb4OAItpEjlI6DuJ3LL6cRz9wzV/h8PhOEBwzd/hcDgcSbjwdzhmBDfJOWYJF/4Ox4ywCPEIjoMLF/6OhYM7PR2O8eHC37FwWHRev8MxD3Dh73A4HEsIF/4Oh8OxhHDh73A4HEsIF/4Oh8OxhHDh73A4HEuIsYR/COH9IYQvhxC+Ofz/vkiZ10IIvxhCuBNC+EYI4c+N06bD4XA4xse4mv8mgK8WRfERAF8dfrf4NoDfVxTFOQD/BYDNEMLvHLNdh8PhcIyBcYX/GwB+cvj5JwFcsgWKonhRFMVvDb++t4c2HQ6HwzEmxhXEJ4ui+Pbw878CcDJWKIRwOoRwF8AjAD9WFMX/N2a7DofD4RgD72kqEEL4CoDfETn1o/qlKIoihBDND10UxSMAZ4fmnpshhJ8piuJJpK3PAvjs8Ou/CyG809S/BnwAwL8Zs45pwvs7WSxaf4HF67P3d7LI6e9rORWNlc9/KJw/XhTFt0MIHwLwtaIofnfDNX8DwD8qiuJnOjec37/bOXmt5wXe38li0foLLF6fvb+TRZ/9Hdfs80UAnxl+/gyAf2gLhBBOhRB+2/Dz+wD8VwDG1egdDofDMQbGFf7XAfzBEMI3AfzQ8DtCCOdDCD8xLPNRAD8fQvglAP8EwP9eFMUvj9muw+FwOMZAo82/DkVR/DqAT0SO3wbwPw0/fxnA2XHaGQN/fUbtdoX3d7JYtP4Ci9dn7+9k0Vt/53YPX4fD4XBMDs65dzgcjiXEQgv/nPQSw3I7IYTdEMJb5vjfCiH8y2HqiTshhHML0OfXQwg/H0J4N4Tw0yGEo3PS388My3wzhPAZOf61EMI7Msa/fUL9vDhs590Qwr5I8xDCe4fj9e5w/Fbl3Nbw+DshhE9Oon999TeEsBpC+E0Zzx+fk/7+/mEal++FED5lzkWfjTnv856M8RfnpL9/IYRwL4RwN4Tw1RDCa3Ku/RgXRbGwfwA+D2Bz+HkTgwCyWLlPAPijAN4yx/8WgE8tWJ//HoBPDz//OIA/P+v+Ang/gAfD/+8bfn7f8NzXAJyfcB8PA/hVAGsAjgL4JQDrpsz/AuDHh58/DeCnh5/Xh+XfC+D1YT2H57i/qwB+ZcrPbE5/VzHw7f1tfafqno157fPw3L+bwzH+bwAcH37+8/JMdBrjhdb8kZFeAgCKovgqgKfT6lQDOvc5hBAA/LcAGCORvL5H5PT3kwC+XBTFbxRF8V0AXwYwzd3JfxDAu0VRPCiK4gWAn8Kg3wq9j58B8InheL4B4KeKovitoij+JYB3h/XNa39ngcb+FkXxsCiKuwBemWtn9WyM0+dZIKe//7goimfDr/8cwKnh505jvOjCPyu9RAP+8nAZ9VdDCO/tsW8pjNPn7wOwWxTF94bfHwP4/j47F0FOf78fg9QdhO3X3xwunz83IQHW1H6lzHD8/i0G45lzbd8Yp78A8HoI4f8JIfyTEMJ/PeG+VvoyRJsxmsX49tHusRDC7RDCPw8hTFrBAtr390cAfKnjtQDGpHpOA6GH9BI12MJAoB3FgEL1JoCrXfqpmHCfe8eE+/s/FEXxrRDCCQD/AMD/iMEy29EN3wbw4aIofj2E8DEM0qX8Z0VR/P+z7tgBw2vD53YNwM+FEH65KIpfnXWnACCE8KcAnAfwB8apZ+6Ff1EUP5Q6F0J4EkL4UDFKL/GvW9ZNjfa3Qgh/E8BfHKOrWu+k+vzrAFZCCO8ZaoOnAHxrzO720d9vAfi4fD+Fga0fRVF8a/j/aQjh72KwvO1b+H8LwGnTvh0XlnkcQngPgP8Ug/HMubZvdO5vMTDy/hYAFEXx9RDCrwI4A+D2jPtbd+3HzbVf66VXze12/l3luX0QQvgagB/AwCY/KWT1N4TwQxgoZX+gGGVL7jTGi272aUwvUYehMKMt/RKAX+m1d3F07vPwxf/HAMhMaH3PHZDT358F8IdCCO8LAzbQHwLwsyGE94QQPgAAIYQjAP4IJjPGvwDgI2HAhDqKgYPUMjT0Pj4F4OeG4/lFAJ8esmteB/ARAP/3BPrYS39DCB8MIRwGgKFW+hEMHHyz7m8K0WdjQv1UdO7zsK/vHX7+AIALAO5NrKcDNPY3hPADAP4agD9WFIUqYd3GeJoe7b7/MLCBfhXANwF8BcD7h8fPA/gJKfdPAXwHwG9iYA/75PD4zwH4ZQwE0v8F4D9egD6vYSCc3gXw9wG8d076+2eHfXoXwJ8ZHvuPAHwdwF0A3wDwf2JCTBoAfxjAfQy0sx8dHrs6fFEA4NhwvN4djt+aXPujw+veAfDDU3p2O/UXwB8fjuUdAL8I4I/OSX9/z/A5/fcYrKi+UfdszHOfAfyXQ7nwS8P/PzIn/f0KgCfD3/4OgC+OM8Ye4etwOBxLiEU3+zgcDoejA1z4OxwOxxLChb/D4XAsIVz4OxwOxxLChb/D4XAsIVz4OxwOxxLChb/D4XAsIVz4OxwOxxLiPwDlqUPRqMSz7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transposed_zmeans = np.array(z_means.cpu()).transpose()\n",
    "\n",
    "plt.scatter(transposed_zmeans[0], transposed_zmeans[1], s = 1, linewidths = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gm3MVzsS5Tjc"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-6ea8166f523f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz_means_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_means_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    972\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 311\u001b[0;31m                     order=order, copy=copy_x)\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 570\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "z_means_np = np.array(z_means.cpu())\n",
    "kmeans = KMeans(n_clusters=12, random_state=1).fit(z_means_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4342,
     "status": "ok",
     "timestamp": 1555377807921,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "HWkA0VaP5Tjf",
    "outputId": "3473289e-4a25-4bf7-bc78-31295678e944"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-23b7a89fe6ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mz2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kmeans' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_points=len(z_means_np)\n",
    "\n",
    "latent_dim = 2\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "counter=0\n",
    "cmap=kmeans.labels_\n",
    "for z1 in range(latent_dim):\n",
    "    for z2 in range(z1+1,latent_dim):\n",
    "        counter+=1\n",
    "        fig.add_subplot(latent_dim,latent_dim,counter)\n",
    "        plt.title(str(z1)+\"_\"+str(z2))\n",
    "        plt.scatter(z_means_np[:, z1][::-1], z_means_np[:, z2][::-1],c=cmap[::-1], s = 15, alpha=0.01,marker=\"o\")\n",
    "#         plt.scatter(z_means_np[:, z1][::-1], z_means_np[:, z2][::-1],c=\"y\" ,alpha=0.3,marker=\"o\")\n",
    "        plt.scatter(z_means_np[0][z1], z_means_np[0][z2],c=\"r\" ,alpha=1,s=40,marker=\"s\")\n",
    "        plt.xlabel(\"Latent dim\"+str(z1+1))\n",
    "        plt.ylabel(\"Latent dim\"+str(z2+1));\n",
    "plt.savefig(\"Try2_originalDropout.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3915,
     "status": "ok",
     "timestamp": 1555377808121,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "I_6Xsa9t5Tjh",
    "outputId": "2e4e8f84-49b2-47ac-a687-67a827c1c5de"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(x_train[0].reshape(82, 24).transpose(1, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1563,
     "status": "ok",
     "timestamp": 1555377812660,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "hIIJ8Jc75Tjj",
    "outputId": "86fb6fd4-d5d4-4f5a-f78c-ba1c1a71c1b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "if vae_type == 'full':\n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "    reshaped_fit_xtrain = m(fit_xtrain.reshape(45000 * 82, 24)).reshape(45000, 82, 24).transpose(2, 1)\n",
    "\n",
    "\n",
    "  # reshaped_fit_xtrain = torch.stack(list(map(m, fit_xtrain))).reshape(50000, 82, 24).transpose(2, 1)\n",
    "elif vae_type == 'conv':\n",
    "    m = torch.nn.Softmax()\n",
    "\n",
    "\n",
    "    reshaped_fit_xtrain = m(fit_xtrain.reshape(45000, 82, 24)).transpose(2, 1)\n",
    "\n",
    "\n",
    "  # reshaped_fit_xtrain = torch.stack(list(map(m, fit_xtrain))).reshape(50000, 82, 24).transpose(2, 1)\n",
    "\n",
    "elif vae_type == 'rec':\n",
    "    m = torch.nn.Softmax()\n",
    "    \n",
    "    reshaped_fit_xtrain = m(fit_xtrain.reshape(45000,82,24).transpose(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1555377814818,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "cej8TrWn5Tjm",
    "outputId": "9820c811-fcca-411d-f038-1996ca9e3c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7f52300657b8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(reshaped_fit_xtrain[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1555377818459,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "RxRyEmS75Tjt",
    "outputId": "036488db-3b5d-4f9b-cef4-19cdee96493a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman SpearmanrResult(correlation=-0.003898952400759738, pvalue=0.8936120091084995)\n",
      "Pearson (0.02768872903297902, 0.3421581862442896)\n",
      "------------------------------\n",
      "Spearman SpearmanrResult(correlation=0.05497584720845208, pvalue=0.059145950198686716)\n",
      "Pearson (0.08388370189141227, 0.003947780859156703)\n",
      "------------------------------\n",
      "Spearman SpearmanrResult(correlation=0.0006085460243089096, pvalue=0.9833467891391192)\n",
      "Pearson (0.012191676470954744, 0.6758068610037712)\n"
     ]
    }
   ],
   "source": [
    "sample_size=batch_size*int(len(test_data_plus)/batch_size)\n",
    "sample_for_averging_size=100\n",
    "sequence_size=PRUNED_SEQ_LENGTH\n",
    "digit_size = len(ORDER_LIST)\n",
    "x_decoded=reshaped_fit_xtrain\n",
    "\n",
    "digit = reshaped_fit_xtrain[0]#fit_xtrain_softmax_reshaped[0]\n",
    "digit_wt = digit\n",
    "digit_wt = normalize(digit.cpu(),axis=0, norm='l1')\n",
    "# print (digit_wt)\n",
    "\n",
    "\n",
    "wt_prob=compute_log_probability(test_data_plus[0].reshape(digit_size, sequence_size),digit_wt)\n",
    "#print (\"wt_log_prob: \", wt_prob)\n",
    "\n",
    "wt_probs=[]\n",
    "digit_avg=np.zeros((digit_size, sequence_size))\n",
    "\n",
    "\n",
    "sample_indices=random.sample(range(sample_size),sample_for_averging_size)\n",
    "\n",
    "counter=0\n",
    "for sample in sample_indices:\n",
    "    digit = x_decoded[sample]\n",
    "#     print (digit)\n",
    "#     print (digit_avg)\n",
    "#     digit_wt_i = normalize(digit,axis=0, norm='l1')\n",
    "    digit_wt_i = digit\n",
    "    \n",
    "#     print (digit_wt_i)\n",
    "    \n",
    "    digit_avg+=np.array(digit_wt_i.cpu()) * 1. / sample_for_averging_size\n",
    "    \n",
    "    wt_p=compute_log_probability(test_data_plus[sample].reshape(digit_size, sequence_size),digit_wt_i.cpu())\n",
    "    wt_probs.append(wt_p)\n",
    "    counter+=1\n",
    "    \n",
    "average_wt_p=np.mean(wt_probs)\n",
    "\n",
    "fitnesses_vs_wt=[]\n",
    "fitnesses=[] #first plug in just the sequences\n",
    "fitnesses_vs_avg=[] \n",
    "\n",
    "for sample in range(1,sample_size):\n",
    "    digit = x_decoded[sample]\n",
    "#     digit = normalize(digit,axis=0, norm='l1')\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size, sequence_size),digit.cpu())-wt_prob\n",
    "    fitnesses.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size, sequence_size),digit_wt)-wt_prob\n",
    "    fitnesses_vs_wt.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size, sequence_size),digit_avg)-average_wt_p\n",
    "    fitnesses_vs_avg.append(fitness)\n",
    "    \n",
    "    \n",
    "print (\"Spearman\",spearmanr(fitnesses_vs_avg,target_values_singles[:sample_size-1]))\n",
    "print (\"Pearson\", pearsonr(fitnesses_vs_avg,target_values_singles[:sample_size-1]))\n",
    "print ('------------------------------')\n",
    "print (\"Spearman\",spearmanr(fitnesses_vs_wt,target_values_singles[:sample_size-1]))\n",
    "print (\"Pearson\", pearsonr(fitnesses_vs_wt,target_values_singles[:sample_size-1]))\n",
    "print ('------------------------------')\n",
    "print (\"Spearman\",spearmanr(fitnesses,target_values_singles[:sample_size-1]))\n",
    "print (\"Pearson\", pearsonr(fitnesses,target_values_singles[:sample_size-1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qKBXcPQiP-r"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-cd168394d947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mperf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitnesses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_values_singles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results_{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_id' is not defined"
     ]
    }
   ],
   "source": [
    "perf = np.array([losses_train,accuracies_train,losses_test,accuracies_test,[0 for i in losses_test]])\n",
    "perf = np.transpose(perf)\n",
    "\n",
    "perf[-6,-1] = spearmanr(fitnesses_vs_avg,target_values_singles[:sample_size-1]).correlation\n",
    "perf[-5,-1] = pearsonr(fitnesses_vs_avg,target_values_singles[:sample_size-1])[0]\n",
    "perf[-4,-1] = spearmanr(fitnesses_vs_wt,target_values_singles[:sample_size-1]).correlation\n",
    "perf[-3,-1] = pearsonr(fitnesses_vs_wt,target_values_singles[:sample_size-1])[0]\n",
    "perf[-2,-1] = spearmanr(fitnesses,target_values_singles[:sample_size-1]).correlation\n",
    "perf[-1,-1] = pearsonr(fitnesses,target_values_singles[:sample_size-1])[0]\n",
    "    \n",
    "np.save('results_{}.txt'.format(run_id),perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1555377867613,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "L4RDCn1SvqoG",
    "outputId": "42112982-d092-434b-ece5-97c3d653e9e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[tensor(46209.6914, grad_fn=<AddBackward0>), 0.683179674796748,\n",
       "        tensor(46217.6641, grad_fn=<AddBackward0>), 0.6841414634146341,\n",
       "        0.4578265313051431],\n",
       "       [tensor(50862.2422, grad_fn=<AddBackward0>), 0.7036788617886179,\n",
       "        tensor(50878.2852, grad_fn=<AddBackward0>), 0.7027926829268293,\n",
       "        0.5079178720488651],\n",
       "       [tensor(59320.0781, grad_fn=<AddBackward0>), 0.7018785907859079,\n",
       "        tensor(59321.6133, grad_fn=<AddBackward0>), 0.7032731707317074,\n",
       "        0.4436030731647449],\n",
       "       [tensor(54943.0195, grad_fn=<AddBackward0>), 0.7167444444444444,\n",
       "        tensor(54955.0117, grad_fn=<AddBackward0>), 0.7172926829268292,\n",
       "        0.48681000381760786],\n",
       "       [tensor(57309.3906, grad_fn=<AddBackward0>), 0.7249265582655826,\n",
       "        tensor(57324.5859, grad_fn=<AddBackward0>), 0.7245121951219512,\n",
       "        0.02598174839699039],\n",
       "       [tensor(55233.3086, grad_fn=<AddBackward0>), 0.7319241192411924,\n",
       "        tensor(55250.3008, grad_fn=<AddBackward0>), 0.7321658536585366,\n",
       "        0.017550713661802993]], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1125,
     "status": "ok",
     "timestamp": 1555273584799,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "Uv9CVwsL5Tjp",
    "outputId": "a4a22c9b-2464-4772-9d8f-4a8ce2e0b0a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fe83f248898>"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+QHsH+0Kb8bgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(np.matmul(test_data_plus[0].reshape(digit_size, sequence_size).T, digit_wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1555273585826,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "oCTzkmWt5Tjw",
    "outputId": "70ad7123-0504-4140-fea6-a7df1dd1bc4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fe83a0d0b00>"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(test_data_plus[0].reshape(digit_size, sequence_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1555273586877,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "8GtV15V35Tjy",
    "outputId": "c04362e4-ce2c-4a10-dafe-7af1e168286d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fe839f54470>"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolor(digit_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1555273588311,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "I0MrIy-I5Tj0",
    "outputId": "b7a1c650-b8d1-42d6-c2f3-6130b0b30109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.009824433997652364, pvalue=0.7361261981734875)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(fitnesses_vs_avg, fitnesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1555277682382,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "d8L-i5nZ5Tj2",
    "outputId": "9d033724-6604-43a2-9535-d16d70b91073",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAEYCAYAAABP+LzuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(fitnesses_vs_wt, target_values_singles[:sample_size-1], alpha = 0.5, s = 10)\n",
    "plt.ylabel(\"Fitness vs. wt\")\n",
    "plt.xlabel(\"p(x)\")\n",
    "plt.title(spearmanr(fitnesses_vs_wt, target_values_singles[:sample_size-1]))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Correlation.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1555273605145,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "-4TOANTD5Tj3",
    "outputId": "724b7efc-b13a-4e89-c8b5-7519b4c99d0d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRqPRbEL+H22PxRA3vL3RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(fitnesses_vs_wt, target_values_singles[:sample_size-1], alpha = 0.5, s = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1555273606918,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "P0Fnmdpj5Tj4",
    "outputId": "d0426e7d-8ea4-4433-f7ed-4115cd1b9ffb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(fitnesses,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1555273608021,
     "user": {
      "displayName": "Anirudh Suresh",
      "photoUrl": "https://lh4.googleusercontent.com/-C1cjzADetbU/AAAAAAAAAAI/AAAAAAAABMU/EVQSgSzpL1U/s64/photo.jpg",
      "userId": "10961260883155163173"
     },
     "user_tz": 240
    },
    "id": "LkE73rY65Tj7",
    "outputId": "07329914-166f-4e7f-d76c-7436a2ace0f2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfUlqiKEvSQ35P+aX/42pCnAOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target_values_singles[:sample_size-1], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppGBWTVe5Tj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKc9fixn5Tj-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_4gbNeo5Tj_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dX-B_yHo5TkG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmdEQNQW5TkJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzFrRh7-5TkL"
   },
   "source": [
    "We have kept track of some performance metrics, so we can follow whether the network was still improving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "au8vScro5TkL"
   },
   "outputs": [],
   "source": [
    "#print performance measures over time\n",
    "keys=[\"categorical_accuracy\",\"top_k_categorical_accuracy\"]\n",
    "for key in keys:\n",
    "    if not key.startswith(\"val_\"):\n",
    "        plt.plot(hist.history[\"val_\"+key],label=\"val_\"+key)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(bbox_to_anchor=(1.6,0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLB3-7j25TkM"
   },
   "source": [
    "Our metric of interest of course is the prediction power (here spearman's rank correlation) between protein sequence probability and the actual fitness measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C40YJSOo5TkN"
   },
   "outputs": [],
   "source": [
    "plt.plot(spearman_measure.scores)\n",
    "plt.title(\"Spearman corr over epochs\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Spearman corr\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zGZieRrf5TkP"
   },
   "source": [
    "## 3. Exploring the Latent Space\n",
    "It is helpful to visualize the latent space to see if the network is separating any mutants (and how the variance is captured). The wildtype sequence is highlighted in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUDZaDF85TkQ"
   },
   "outputs": [],
   "source": [
    "encoder = Model(x, z_mean)\n",
    "sample_points=data_set_size\n",
    "\n",
    "sample_size=batch_size*int(len(test_data_plus)/batch_size)\n",
    "sample_for_averging_size=100\n",
    "sequence_size=PRUNED_SEQ_LENGTH\n",
    "digit_size = len(ORDER_LIST)\n",
    "\n",
    "#wildtype in red\n",
    "x_train_encoded = encoder.predict(training_data[:sample_points], batch_size=batch_size)\n",
    "x_test_encoded=encoder.predict(all_test_data[:15060], batch_size=batch_size)\n",
    "wt_encoding=x_train_encoded[0].reshape(1,latent_dim)\n",
    "\n",
    "x_decoded=vae.predict(test_data_plus[0:sample_size],batch_size=batch_size)\n",
    "digit = x_decoded[0].reshape(digit_size,sequence_size)\n",
    "digit_wt = normalize(digit,axis=0, norm='l1')\n",
    "wt_prob=compute_log_probability(test_data_plus[0].reshape(digit_size,sequence_size),digit_wt)\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "counter=0\n",
    "cmap=[\"r\" if i==0 else \"b\" for i in range(sample_points)]\n",
    "for z1 in range(latent_dim):\n",
    "    for z2 in range(z1+1,latent_dim):\n",
    "        counter+=1\n",
    "        fig.add_subplot(latent_dim,latent_dim,counter)\n",
    "        plt.title(str(z1)+\"_\"+str(z2))\n",
    "        plt.scatter(x_train_encoded[:, z1][::-1], x_train_encoded[:, z2][::-1],c=cmap[::-1] ,alpha=0.01)\n",
    "\n",
    "        plt.scatter(x_test_encoded[:, z1][::-1], x_test_encoded[:, z2][::-1],c=\"y\" ,alpha=0.3)\n",
    "        plt.scatter(x_train_encoded[0][z1], x_train_encoded[0][z2],c=\"r\" ,alpha=1)\n",
    "        plt.xlabel(\"Latent dim\"+str(z1+1))\n",
    "        plt.ylabel(\"Latent dim\"+str(z2+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rTe9_g75TkT"
   },
   "source": [
    "We can see that there is an interesting branching structure occuring. We can decipher this structure. It is also possible to run the network with more latent variables (usually improves the results by a little). In such cases we can also plot the latent dimensions 3 at a time (instead of 2 at a time above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wF5KIdt35TkU"
   },
   "outputs": [],
   "source": [
    "# for 3d Visualization of latent space if at least three dimensions existed\n",
    "if latent_dim>2:\n",
    "    %matplotlib inline\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    encoder = Model(x, z_mean)\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    #x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    #plt.scatter(x_test_encoded[:, 0], x_test_encoded[: ,1])#, c=y_test)\n",
    "    ax.scatter(x_train_encoded[:, 0], x_train_encoded[:, 1],x_train_encoded[:, 2],c=\"b\",alpha=0.01)\n",
    "    ax.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1],x_test_encoded[:, 2],c=\"y\",alpha=0.75)\n",
    "    ax.scatter(x_train_encoded[0][0], x_train_encoded[0][1],x_train_encoded[0][2], c=\"r\" ,alpha=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5m3IUwE5TkV"
   },
   "source": [
    "To further understand the structure we saw above we first look at the distribution of distances of sequences from each other. We can cluster the data using a simple approach like k-means, and then re-map that onto our plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyMeA1_u5TkW"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=12, random_state=1).fit(training_data[:data_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RKP0LEt5TkX"
   },
   "outputs": [],
   "source": [
    "sample_points=data_set_size\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "counter=0\n",
    "cmap=kmeans.labels_\n",
    "for z1 in range(latent_dim):\n",
    "    for z2 in range(z1+1,latent_dim):\n",
    "        counter+=1\n",
    "        fig.add_subplot(latent_dim,latent_dim,counter)\n",
    "        plt.title(str(z1)+\"_\"+str(z2))\n",
    "        plt.scatter(x_train_encoded[:, z1][::-1], x_train_encoded[:, z2][::-1],c=cmap[::-1] ,alpha=0.01,marker=\"o\")\n",
    "        plt.scatter(x_test_encoded[:, z1][::-1], x_test_encoded[:, z2][::-1],c=\"y\" ,alpha=0.3,marker=\"o\")\n",
    "        plt.scatter(x_test_encoded[0][z1], x_test_encoded[0][z2],c=\"r\" ,alpha=1,s=40,marker=\"s\")\n",
    "        plt.xlabel(\"Latent dim\"+str(z1+1))\n",
    "        plt.ylabel(\"Latent dim\"+str(z2+1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITfsKaiM5Tka"
   },
   "source": [
    "It appears that the k-means clustering strongly aligns with how the network has split the training data into branches. Hence sequence distances are a large factor in this projection onto the latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2kTbligt5Tkb"
   },
   "source": [
    "Next, I compute the likelihood of the wildtype sequence occuring in each position within the latent space. This is interesting because it lets us visually how much the distribution of sequences is favorable to the wildtype at that particular place in the latent space. To see how this probability is computed, see [below](#P_compute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT58D8xy5Tkb"
   },
   "outputs": [],
   "source": [
    "probs=[]\n",
    "points_x=np.arange(min(x_train_encoded[:, 0]),max(x_train_encoded[:, 0]),(max(x_train_encoded[:, 0])-min(x_train_encoded[:, 0]))*0.0025)\n",
    "points_y=np.arange(min(x_train_encoded[:, 1]),max(x_train_encoded[:, 1]),(max(x_train_encoded[:, 1])-min(x_train_encoded[:, 1]))*0.0025)\n",
    "\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_x_decoded_mean= decoder_out(decoder_3(decoder_2d(decoder_2(decoder_1(decoder_input)))))\n",
    "\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "for i in points_x:\n",
    "    for j in points_y:\n",
    "        x_decoded=generator.predict(np.array([i,j]).reshape(1,-1)).reshape(digit_size,sequence_size)\n",
    "        digit_prob=normalize(x_decoded,axis=0, norm='l1')\n",
    "        value_to_append=compute_log_probability(all_test_data[0].reshape(digit_size,sequence_size),digit_prob)\n",
    "        probs.append(value_to_append)\n",
    "       #if interested in seeing which points in the matrix come closest to the wildtype, they can be printed using the code that follows \n",
    "        #if i<wt_encoding[0][0]+0.1 and i>=wt_encoding[0][0]-0.1:\n",
    "         #   if j<wt_encoding[0][1]+0.1 and j>=wt_encoding[0][1]-0.1:\n",
    "          #      print (i,j,value_to_append)\n",
    "            \n",
    "            \n",
    "probs=np.array(probs)\n",
    "probs[probs<-2000]=-2000\n",
    "probs=probs.reshape(len(points_x),len(points_y)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5GtGQP65Tke"
   },
   "source": [
    "We can now plot the probabilities onto the latent space. White areas are where wildtype occurs with the same probability as it’s own coordinate in the latent space. Red means that wildtype is less favored in those locations, and blue means it is more favored (than it’s own location). Notice that in certain datasets (and realizations of the latent space), the positive range is very small and it may appear that there is a flat dark blue area. This is simply because the sequences that are better than wildtype are usually just slightly better, and hence there is little room for the positive side of the colorbar to show its gradient. The wildtype may also be in a blue area because of the \"binning\" or pixelation as a result of the way we compute the probabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XF5CRYau5Tke"
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm \n",
    "fig = plt.figure(figsize=(12,9))\n",
    "orig_cmap = matplotlib.cm.bwr_r\n",
    "midpoint=((np.min(probs)-wt_prob)/(np.min(probs)-np.max(probs)))\n",
    "offset_top=np.max(probs)+5 #better visualization because often the \"beneficial\" coords are very close to wildtype in fitness, coarse graining may result in misleading visuals\n",
    "\n",
    "shrunk_cmap = shiftedColorMap(orig_cmap, start=0.0, midpoint=midpoint, stop=1, name='shrunk')\n",
    "plt.pcolormesh(points_x,points_y, probs,cmap=shrunk_cmap,vmin=np.min(probs), vmax=offset_top)\n",
    "ax=plt.colorbar()\n",
    "ax.set_label(\"log probability of wildtype\")\n",
    "plt.scatter(x_train_encoded[:, 0][::-1], x_train_encoded[:, 1][::-1],c=cmap[::-1] ,alpha=0.1)\n",
    "plt.scatter(x_test_encoded[:, 0][1180:], x_test_encoded[:, 1][1180:],c=\"purple\" ,alpha=0.3)\n",
    "plt.scatter(x_test_encoded[:, 0][1:1180], x_test_encoded[:, 1][1:1180],c=\"g\" ,alpha=0.3)\n",
    "plt.scatter(x_train_encoded[0][0], x_train_encoded[0][1],c=\"r\" ,alpha=1,s=40,marker=\"s\")\n",
    "plt.xlabel(\"latent dim 1\")\n",
    "plt.ylabel(\"latent dim 2\")\n",
    "\n",
    "plt.xlim(min(points_x),max(points_x))\n",
    "plt.ylim(min(points_y),max(points_y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9lu1eM35Tkh"
   },
   "source": [
    "We can zoom in closer to the wildtype, here we removed the cluster colors, and the yellow dots show the test data (single and double mutants). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OLdx_xU5Tki"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.pcolor(points_x,points_y, probs,cmap=shrunk_cmap,vmax=offset_top)\n",
    "ax=plt.colorbar()\n",
    "ax.set_label(\"log probability of wildtype\")\n",
    "\n",
    "plt.scatter(x_train_encoded[:, 0][::-1], x_train_encoded[:, 1][::-1],c=cmap[::-1] ,alpha=0.5)\n",
    "plt.scatter(x_test_encoded[:, 0][1180:], x_test_encoded[:, 1][1180:],c=\"purple\" ,alpha=0.3,label=\"double_mut\")\n",
    "plt.scatter(x_test_encoded[:, 0][1:1180], x_test_encoded[:, 1][1:1180],c=\"g\" ,alpha=0.3,label=\"single_mut\")\n",
    "plt.scatter(x_train_encoded[0][0], x_train_encoded[0][1],c=\"r\" ,alpha=1,s=40,marker=\"s\",label=\"wt\")\n",
    "\n",
    "wt_x=wt_encoding[0][0]\n",
    "wt_y=wt_encoding[0][1]\n",
    "offset=2\n",
    "plt.xlim(wt_x-offset,wt_x+offset);\n",
    "plt.ylim(wt_y-offset,wt_y+offset);\n",
    "\n",
    "plt.legend();\n",
    "plt.xlabel(\"latent dim 1\")\n",
    "plt.xlabel(\"latent dim 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erZC3MRN5Tko"
   },
   "source": [
    "We can see how the reconstruction changes as we walk from a wildtype to a mutant in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0CqPDOc05Tkp"
   },
   "outputs": [],
   "source": [
    "x_decoded=vae.predict(test_data_plus[0:200],batch_size=batch_size)\n",
    "digit_wt = x_decoded[0].reshape(digit_size,sequence_size)\n",
    "digit_wt = normalize(digit_wt,axis=0, norm='l1')\n",
    "\n",
    "mut_sample=100\n",
    "\n",
    "digit_p= x_decoded[mut_sample].reshape(digit_size,sequence_size)\n",
    "digit_p = normalize(digit_p,axis=0, norm='l1')\n",
    "\n",
    "fig = plt.figure(figsize=(12,18))\n",
    "\n",
    "fig.add_subplot(412)\n",
    "\n",
    "plt.pcolor(digit_wt,cmap=\"hot\",vmin=0,vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.title(\"wild_type_reconstruction\")\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "fig.add_subplot(413)\n",
    "\n",
    "plt.pcolor(digit_p,cmap=\"hot\",vmin=0,vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"mutant_reconstruction\")\n",
    "\n",
    "fig.add_subplot(411)\n",
    "\n",
    "plt.pcolor(abs(test_data_plus[0].reshape(digit_size,sequence_size)+(test_data_plus[mut_sample].reshape(digit_size,sequence_size))*2),cmap=\"hot\",vmin=0,vmax=3)\n",
    "\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"one hot encoding of input sequence (wt(red), mut(yellow))\")\n",
    "\n",
    "fig.add_subplot(414)\n",
    "\n",
    "plt.pcolor(-digit_wt+digit_p,cmap=\"bwr\",vmin=-0.05,vmax=0.05)\n",
    "plt.colorbar()\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"difference in distribution (mutant-wt): Notice that the scale is different\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qs9zBLOM5Tkr"
   },
   "source": [
    "Notice the slight change in the probabilities, for other positions in the sequence, not just those that differed in the original input sequence (fourth panel).  \n",
    "\n",
    "\n",
    "Alternatively, we can move in the latent dimension (along the eigen basis) by preturbing coordinates starting at the wildtype and observe the change in probabilities. To do so, we first compute the principal eigenvector for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frjXG3Gx5Tkt"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca_result=pca.fit(x_test_encoded)\n",
    "\n",
    "print(\"Explained variance by first component\", pca.explained_variance_ratio_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ypfBOP05Tkv"
   },
   "source": [
    "We can plot the points along the eigenvector in the latent space (shown in cyan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkNe0jzu5Tkv"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.pcolormesh(points_x,points_y, probs,cmap=shrunk_cmap,vmin=np.min(probs), vmax=offset_top)\n",
    "\n",
    "\n",
    "plt.colorbar() \n",
    "plt.scatter(x_train_encoded[:,0][::-1], x_train_encoded[:, 1][::-1],c=\"b\" ,alpha=0.05)\n",
    "plt.scatter(x_test_encoded[:, 0][::-1], x_test_encoded[:, 1][::-1],c=\"y\" ,alpha=0.3)\n",
    "\n",
    "\n",
    "start_l=-5\n",
    "end_l=5\n",
    "line_start=pca.inverse_transform(np.array([start_l]).reshape(1,-1))\n",
    "line_end=pca.inverse_transform(np.array([end_l]).reshape(1,-1))\n",
    "\n",
    "for i in np.arange(start_l,end_l,0.25):\n",
    "    \n",
    "    perturb=pca.inverse_transform(np.array([i]).reshape(1,-1))\n",
    "    perturbed=perturb\n",
    "    plt.scatter(perturbed[0][0],perturbed[0][1],c=\"c\",alpha=1,s=40)\n",
    "\n",
    "plt.plot([line_start[0][0],line_end[0][0]],[line_start[0][1],line_end[0][1]],c=\"c\")\n",
    "\n",
    "plt.scatter(wt_encoding[0][0],wt_encoding[0][1],c=\"red\",s=40)\n",
    "\n",
    "offset=8\n",
    "plt.xlim(wt_x-offset,wt_x+offset);\n",
    "plt.ylim(wt_y-offset,wt_y+offset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8DC-fi25Tky"
   },
   "source": [
    "If we move along that axis by a tiny amount and reconstruct the sequence, we will see what is being updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBrOBm9w5Tkz"
   },
   "outputs": [],
   "source": [
    "perturb=pca.inverse_transform(np.array([0.01]).reshape(1,-1))\n",
    "\n",
    "x_decoded=generator.predict(perturb)\n",
    "\n",
    "digit_p = x_decoded.reshape(digit_size,sequence_size)\n",
    "digit_p = normalize(digit_p,axis=0, norm='l1')\n",
    "fig = plt.figure(figsize=(12,15))\n",
    "\n",
    "fig.add_subplot(311)\n",
    "plt.title(\"wild_type_reconstruction\")\n",
    "\n",
    "plt.pcolor(digit_wt,cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.ylabel(\"AA\")\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_subplot(312)\n",
    "plt.title(\"mutant_reconstruction\")\n",
    "plt.pcolor(digit_p,cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.ylabel(\"AA\")\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "fig.add_subplot(313)\n",
    "\n",
    "plt.pcolor(-digit_wt+digit_p,cmap=\"bwr\",vmin=-0.05,vmax=0.05)\n",
    "plt.colorbar()\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"difference in distribution (mutant-wt): Notice that the scale is different\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOVbLWjt5Tk1"
   },
   "source": [
    "If you want to see how the walk along the axis changes the sequence, you can use the code below to generate a set of images (like the gif on the blog). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmB2PsNv5Tk1"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "start_l=-15\n",
    "end_l=15\n",
    "line_start=pca.inverse_transform(np.array([start_l]).reshape(1,-1))\n",
    "line_end=pca.inverse_transform(np.array([end_l]).reshape(1,-1))\n",
    "\n",
    "\n",
    "for i in np.arange(start_l,end_l,1):\n",
    "    perturb=pca.inverse_transform(np.array([i]).reshape(1,-1))\n",
    "\n",
    "    x_decoded=generator.predict(perturb)\n",
    "\n",
    "    digit_p = x_decoded.reshape(digit_size,sequence_size)\n",
    "    digit_p = normalize(digit_p,axis=0, norm='l1')\n",
    "    most_likely=most_likely_seq(digit_p)\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    fig.add_subplot(121)\n",
    "    plt.xlabel(\"wt preturbed by \"+str(i))\n",
    "    plt.title(most_likely)\n",
    "    plt.pcolor(digit_p,cmap=\"hot\");\n",
    "    plt.xlim(0,82);\n",
    "    plt.ylim(0,24);\n",
    "    plt.ylabel(\"AA\")\n",
    "    plt.yticks(range(24),ORDER_LIST)\n",
    "    plt.xticks(range(82),list(most_likely))\n",
    "    fig.add_subplot(122)\n",
    "    plt.pcolormesh(points_x,points_y, probs,cmap=shrunk_cmap,vmin=np.min(probs), vmax=offset_top)\n",
    "    plt.colorbar() \n",
    "    \n",
    "    perturbed=perturb\n",
    "\n",
    "    plt.scatter(x_train_encoded[:,0][::-1], x_train_encoded[:, 1][::-1],c=\"b\" ,alpha=0.05)\n",
    "    plt.scatter(x_test_encoded[:, 0][::-1], x_test_encoded[:, 1][::-1],c=\"g\" ,alpha=0.25,s=25)\n",
    "    plt.plot([line_start[0][0],line_end[0][0]],[line_start[0][1],line_end[0][1]],\"--\",c=\"c\",linewidth=2,alpha=0.4)\n",
    "\n",
    "    plt.scatter(wt_encoding[0][0],wt_encoding[0][1],c=\"r\",s=40)\n",
    "    plt.scatter(perturbed[0][0],perturbed[0][1],c=\"c\",s=60,alpha=1)\n",
    "\n",
    "\n",
    "\n",
    "    offset=15\n",
    "    plt.xlim(wt_x-offset,wt_x+offset);\n",
    "    plt.ylim(wt_y-offset,wt_y+offset);\n",
    "    plt.savefig(\"exploration/\"+str(count)+\".png\");\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrc147xo5Tk4"
   },
   "source": [
    "Another approach to study the effects of particular mutations in the latent space is to actually annotate the location that each mutation occupies (on average) within the latent space. Below, I have mutated the same \"G\" amino acid into all of its variants in 3 different positions on the sequence (27,40,44, on the aligned columns). Then I plot the location of those mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQrPgybS5Tk4"
   },
   "outputs": [],
   "source": [
    "g_indices=[27,40, 44]#g\n",
    "#k_indices=[33,41, 43] #k\n",
    "#s_indices=[31,32,42] #s\n",
    "single_mutants_g=[]\n",
    "ref_seq=data.iloc[0][\"seq\"]\n",
    "for ind in g_indices:\n",
    "    for aa in ORDER_LIST:\n",
    "        new_sequence=ref_seq[:ind]+aa+ref_seq[ind+1:]\n",
    "        single_mutants_g.append(new_sequence)\n",
    "        \n",
    "one_hot_single_mutants_g=[]  \n",
    "\n",
    "for mutant in single_mutants_g:\n",
    "    one_hot_single_mutants_g.append(translate_string_to_one_hot(mutant,ORDER_LIST))\n",
    "\n",
    "single_mutants_g_OH=np.array([np.array(list(sample.flatten())).T for sample in one_hot_single_mutants_g])\n",
    "single_mutants_g_padded=np.vstack([single_mutants_g_OH,single_mutants_g_OH[:8]]) #padding so I can feed all of it to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9ciduyF5Tk7"
   },
   "outputs": [],
   "source": [
    "mutant_g_encoded = encoder.predict(single_mutants_g_padded, batch_size=batch_size)\n",
    "mutant_g_encoded=mutant_g_encoded[:72]\n",
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "ax.scatter(x_train_encoded[0][0], x_train_encoded[0][1],c=\"r\" ,alpha=1,s=40,marker=\"s\",label=\"wt\")\n",
    "ax.scatter(mutant_g_encoded[:,0][:24], mutant_g_encoded[:, 1][:24],c=\"b\" ,alpha=0,label=\"pos 27\")\n",
    "\n",
    "for i, txt in enumerate(ORDER_LIST):\n",
    "    ax.annotate(txt, (mutant_g_encoded[:,0][i],mutant_g_encoded[:,1][i]),color=\"b\",size=\"large\")\n",
    "    \n",
    "ax.scatter(mutant_g_encoded[:,0][24:48], mutant_g_encoded[:, 1][24:48],c=\"g\" ,alpha=0,label=\"pos 40\")\n",
    "\n",
    "for i, txt in enumerate(ORDER_LIST):\n",
    "    ax.annotate(txt, (mutant_g_encoded[:,0][i+24],mutant_g_encoded[:,1][i+24]),color=\"g\",size=\"large\")\n",
    "    \n",
    "ax.scatter(mutant_g_encoded[:,0][48:], mutant_g_encoded[:, 1][48:],c=\"g\" ,alpha=0,label=\"pos 44\")\n",
    "\n",
    "for i, txt in enumerate(ORDER_LIST):\n",
    "    ax.annotate(txt, (mutant_g_encoded[:,0][i+48],mutant_g_encoded[:,1][i+48]),color=\"r\",size=\"large\")\n",
    "    \n",
    "plt.title(\"position 27 blue, postition 40 green, position 44 red, wt red square\")\n",
    "plt.xlabel(\"Latent dim 1\")\n",
    "plt.ylabel(\"Latent dim 2\");\n",
    "offset=0.5\n",
    "plt.xlim(wt_x-offset,wt_x+offset);\n",
    "plt.ylim(wt_y-offset,wt_y+offset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Cr67yIx5Tk9"
   },
   "source": [
    "While there is some clustering of amino acids based on similarity and position, this trend is not consistent depending on the amino-acid/mutation and position of choice. We can also project the measured fitness onto the latent space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KxMRS1E5Tk9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors\n",
    "\n",
    "norm = colors.Normalize(vmin=min(target_values_doubles), vmax=1)\n",
    "cmap_v = cm.hot\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap_v)\n",
    "\n",
    "plt.scatter(x_test_encoded[:, 0][1180:], x_test_encoded[:, 1][1180:],c=list(map(m.to_rgba,target_values_doubles[:len(x_test_encoded[:, 1][1180:])])) ,alpha=0.3,label=\"double_mut\")\n",
    "plt.scatter(x_test_encoded[:, 0][1:1180], x_test_encoded[:, 1][1:1180],c=list(map(m.to_rgba,target_values_singles[1:1180])) ,alpha=0.3,label=\"single_mut\")\n",
    "plt.scatter(x_train_encoded[0][0], x_train_encoded[0][1],c=\"w\" ,alpha=1,s=40,marker=\"s\",label=\"wt\")\n",
    "\n",
    "wt_x=wt_encoding[0][0]\n",
    "wt_y=wt_encoding[0][1]\n",
    "offset=2\n",
    "plt.xlim(wt_x-offset,wt_x+offset);\n",
    "plt.ylim(wt_y-offset,wt_y+offset);\n",
    "\n",
    "plt.legend();\n",
    "plt.xlabel(\"latent dim 1\")\n",
    "plt.ylabel(\"latent dim 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HEw19PB5Tk-"
   },
   "source": [
    "There are no obvious trends in the latent space that would correspond to fitness values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJr1ybE85Tk_"
   },
   "source": [
    "<a id='P_compute'></a>\n",
    "\n",
    "\n",
    "## 4. Predicting fitness based on sequence probability\n",
    "\n",
    "We compute the (log) probability of a sequence as:\n",
    "$$ \\log(trace(H^T P)) $$\n",
    "\n",
    "Where $H$ is the one-hot encoding of the sequence of interest, and $P$ is the probability weight matrix generated by feeding the network a sequence. Taking the trace of the matrix product is similar to the dot product of two vectors. We compute the fitness in three highly correlated ways (although depending on the dataset, these correlations change). The difference in these approaches is in how to compute $P$. \n",
    "\n",
    "(1)$P$ is the reconstruction of the same sequence that $H$ represents. (we call this prediction or \"fitness\" in the code)\n",
    "\n",
    "(2) $P$ is the reconstruction of the wildtype sequence.  (we call this prediction_wt or \"fitness_wt\" in the code)\n",
    "\n",
    "(3) $P$ is an average reconstruction of 100 samples of mutants. (we call this prediction_avg or \"fitness_avg\" in the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQnhWqe15Tk_"
   },
   "source": [
    "### 4.1. Single mutants\n",
    "\n",
    "We can now use the trained network to predict the fitnesses. The first step is to compute the fitnesses for all single mutant sequences (and compare it to the wildtype). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0CCNMSz5Tk_"
   },
   "outputs": [],
   "source": [
    "sample_size=batch_size*int(len(test_data_plus)/batch_size)\n",
    "sample_for_averging_size=100\n",
    "sequence_size=PRUNED_SEQ_LENGTH\n",
    "digit_size = len(ORDER_LIST)\n",
    "x_decoded=vae.predict(test_data_plus[0:sample_size],batch_size=batch_size)\n",
    "\n",
    "digit = x_decoded[0].reshape(digit_size,sequence_size)\n",
    "digit_wt = normalize(digit,axis=0, norm='l1')\n",
    "wt_prob=compute_log_probability(test_data_plus[0].reshape(digit_size,sequence_size),digit_wt)\n",
    "#print (\"wt_log_prob: \", wt_prob)\n",
    "\n",
    "wt_probs=[]\n",
    "digit_avg=np.zeros((digit_size,sequence_size))\n",
    "\n",
    "\n",
    "sample_indices=random.sample(range(sample_size),sample_for_averging_size)\n",
    "\n",
    "counter=0\n",
    "for sample in sample_indices:\n",
    "    digit = x_decoded[sample].reshape(digit_size,sequence_size)\n",
    "    digit_wt_i = normalize(digit,axis=0, norm='l1')\n",
    "    \n",
    "    digit_avg+=digit_wt_i*1./sample_for_averging_size\n",
    "    wt_p=compute_log_probability(test_data_plus[sample].reshape(digit_size,sequence_size),digit_wt_i)\n",
    "    wt_probs.append(wt_p)\n",
    "    counter+=1\n",
    "average_wt_p=np.mean(wt_probs)\n",
    "\n",
    "fitnesses_vs_wt=[]\n",
    "fitnesses=[]\n",
    "fitnesses_vs_avg=[]\n",
    "\n",
    "for sample in range(1,sample_size):\n",
    "    digit = x_decoded[sample].reshape(digit_size,sequence_size)\n",
    "    digit = normalize(digit,axis=0, norm='l1')\n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size,sequence_size),digit)-wt_prob\n",
    "    fitnesses.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size,sequence_size),digit_wt)-wt_prob\n",
    "    fitnesses_vs_wt.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_plus[sample].reshape(digit_size,sequence_size),digit_avg)-average_wt_p\n",
    "    fitnesses_vs_avg.append(fitness)\n",
    "    \n",
    "    \n",
    "print (\"Spearman\",spearmanr(fitnesses,target_values_singles[:sample_size-1]))\n",
    "print (\"Pearson\", pearsonr(fitnesses,target_values_singles[:sample_size-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvM51hKN5TlF"
   },
   "source": [
    "As a first step, we draw the distribution of the normalized predicted fitnesses vs. the experimental data. We care about the relative distributions, rather than the exact values, so we can renormalize the predictions to roughly fall in the range of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIis60D55TlF"
   },
   "outputs": [],
   "source": [
    "predictions=np.array(list(map(lambda x: x*1./(4*(max (fitnesses))),fitnesses))) #arbitrary renormalization\n",
    "indip=list(map(lambda x: x*1./max (exp_data_singles[\"effect_prediction_independent\"]),exp_data_singles[\"effect_prediction_independent\"]))\n",
    "epis=list(map(lambda x: x*1./max (exp_data_singles[\"effect_prediction_epistatic\"]),exp_data_singles[\"effect_prediction_epistatic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS8PS8IG5TlH"
   },
   "outputs": [],
   "source": [
    "plt.hist(predictions[predictions > -1E10],alpha=0.7,label=\"predicted fitness\"); #filter out -inf\n",
    "plt.hist(np.array(target_values_singles)-1,alpha=0.5,label=\"experimental fitness\");\n",
    "plt.xlabel(\"relative fitness\");\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cMbA9zS5TlJ"
   },
   "outputs": [],
   "source": [
    "len(predictions),len(target_values_singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpDRTG-q5TlL"
   },
   "source": [
    "Note that the mismatch in lenght is due to batch size partitioning of the data (this can be fixed, but doesn't change much about the results). We can also explicitly look at how these values change together along all possible mutations. To visualize the predictions vs the real data, we use a [smoothing function](http://scipy.github.io/old-wiki/pages/Cookbook/SavitzkyGolay) to be able to visually relate the fitness ascribed to many variants, with those that the network predicted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxqhRavV5TlM"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(18,4), dpi=90)\n",
    "window=9\n",
    "poly=3\n",
    "plt.step(range(len(target_values_singles)),savitzky_golay(list(target_values_singles),window,poly),label=\"experiment\")\n",
    "plt.step(range(len(predictions)),savitzky_golay(predictions,window,poly),linewidth=2,alpha=0.6,label=\"predictions\")\n",
    "plt.legend();\n",
    "plt.xlabel(\"mutations\")\n",
    "plt.ylabel(\"fitness\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lH6J3j0Z5TlN"
   },
   "source": [
    "To remove the information that is purely due to column entropy, we subtract it from our prediction values and compare it to the epistatic predictions from Hopf paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8M1iTJW5TlN"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(18,4), dpi=90)\n",
    "plt.step(range(len(predictions)),savitzky_golay(([a-b for a,b in zip(predictions,indip)]),window,poly))\n",
    "plt.step(range(len(epis)),savitzky_golay(([a-b for a,b in zip(epis,indip)]),window,poly))\n",
    "plt.xlabel(\"mutations\")\n",
    "plt.ylabel(\"fitness\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibn6pB3F5TlP"
   },
   "source": [
    "Finally, we can plot the predictions across the entire mutation landscape and compare it with the two other prediction methods (column entropy and second-order epistasis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rq32v0CZ5TlP"
   },
   "outputs": [],
   "source": [
    "len_seq=batch_size*int(len(test_data_plus)/batch_size)\n",
    "compare_on=False\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(num=4, figsize=(18,8), dpi=90)#, facecolor='w', edgecolor='k'),\n",
    "plt.subplot(311)\n",
    "plt.xlim(0,len_seq*1./3)\n",
    "plt.step(range(int(len_seq*1./3)),savitzky_golay(list(target_values_singles[:int(len_seq*1./3)]),window,poly),label=\"experimental $\\Delta f$ measurement\")\n",
    "plt.step(range(int(len_seq*1./3)),savitzky_golay(predictions[:int(len_seq*1./3)],window,poly), label=\"predicted $\\Delta f$\")\n",
    "if compare_on: \n",
    "    plt.plot(range(int(len_seq*1./3)),savitzky_golay(indip[:int(len_seq*1./3)],window,poly))\n",
    "    plt.plot(range(int(len_seq*1./3)),savitzky_golay(epis[:int(len_seq*1./3)],window,poly))\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.58,1.05))\n",
    "plt.ylabel(\"$\\Delta$ fitness\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.xlim(len_seq*1./3,len_seq*2./3)\n",
    "plt.step(range(int(len_seq*1./3),int(len_seq*2./3)),savitzky_golay(list(target_values_singles[int(len_seq*1./3):int(len_seq*2./3)]),window,poly))\n",
    "plt.step(range(int(len_seq*1./3),int(len_seq*2./3)),savitzky_golay(predictions[int(len_seq*1./3):int(len_seq*2./3)],window,poly),linewidth=2)\n",
    "plt.ylabel(\"$\\Delta$ fitness\")\n",
    "if compare_on: \n",
    "    plt.plot(range(int(len_seq*1./3),int(len_seq*2./3)),savitzky_golay(indip[int(len_seq*1./3):int(len_seq*2./3)],window,poly))\n",
    "    plt.plot(range(int(len_seq*1./3),int(len_seq*2./3)),savitzky_golay(epis[int(len_seq*1./3):int(len_seq*2./3)],window,poly))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.xlim(len_seq*2./3,len_seq)\n",
    "plt.step(range(int(len_seq*2./3),len_seq),savitzky_golay(list(target_values_singles[int(len_seq*2./3):len_seq]),window,poly))\n",
    "plt.step(range(int(len_seq*2./3),len_seq-1),savitzky_golay(predictions[int(len_seq*2./3):len_seq],window,poly),linewidth=2)\n",
    "\n",
    "if compare_on: \n",
    "    plt.plot(range(int(len_seq*2./3),len_seq),savitzky_golay(indip[int(len_seq*2./3):len_seq],window,poly))\n",
    "    plt.plot(range(int(len_seq*2./3),len_seq),savitzky_golay(epis[int(len_seq*2./3):len_seq],window,poly))\n",
    "\n",
    "plt.ylabel(\"$\\Delta$ fitness\")\n",
    "plt.xlabel(\"Position\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "L-Yr4s_x5TlR"
   },
   "source": [
    "We can also visualize the prediction vs. fitness data differently. The red line represents perfect correlation, the blue distribution is the actual comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVGQHGzE5TlR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "x1=predictions\n",
    "\n",
    "y1=target_values_singles[:len(predictions)]\n",
    "\n",
    "plt.scatter(x1,y1,alpha=0.2)\n",
    "plt.plot(sorted(x1),sorted(y1),\"r\",linewidth=4,alpha=0.9)\n",
    "plt.title(str(spearmanr(x1,y1)))\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"experiment\")\n",
    "print(\"Mutual info:\", normalized_mutual_info_score(x1,y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cz7FOq__5TlT"
   },
   "source": [
    "He is a summary of the results in a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehss1PNX5TlT"
   },
   "outputs": [],
   "source": [
    "size=len(predictions)\n",
    "fitness_data=pd.DataFrame(columns=[\"lin\",\"log\",\"pred_wt\",\"pred\",\"pred_avg\"])\n",
    "\n",
    "fitness_data[\"experimental\"]=exp_data_singles[\"linear\"][:size]\n",
    "fitness_data[\"effect_prediction_epistatic\"]=exp_data_singles[\"effect_prediction_epistatic\"][:size]\n",
    "fitness_data[\"effect_prediction_independent\"]=exp_data_singles['effect_prediction_independent'][:size]\n",
    "\n",
    "fitness_data[\"prediction\"]=fitnesses\n",
    "fitness_data[\"prediction_wt\"]=fitnesses_vs_wt\n",
    "fitness_data[\"prediction_avg\"]=fitnesses_vs_avg\n",
    "\n",
    "corr_pred_singles=fitness_data.corr(method=\"spearman\")\n",
    "print(\"spearman rho correlations\")\n",
    "corr_pred_singles[[\"experimental\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uxm3ICVm5TlU"
   },
   "source": [
    "### 4.2. Double mutants\n",
    "We repeat the same analysis for double mutants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzw8fOPV5TlV"
   },
   "outputs": [],
   "source": [
    "x_decoded=vae.predict(test_data_doubles_plus[0:sample_size],batch_size=batch_size)\n",
    "digit_wt = x_decoded[0].reshape(digit_size,sequence_size)\n",
    "digit_wt = normalize(digit_wt,axis=0, norm='l1')\n",
    "\n",
    "mut_sample=100\n",
    "\n",
    "digit_p= x_decoded[mut_sample].reshape(digit_size,sequence_size)\n",
    "digit_p = normalize(digit_p,axis=0, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UW2xqAXA5TlX"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,18))\n",
    "\n",
    "fig.add_subplot(412)\n",
    "\n",
    "plt.pcolor(digit_wt,cmap=\"hot\",vmin=0,vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "plt.title(\"wild_type_reconstruction\")\n",
    "\n",
    "fig.add_subplot(413)\n",
    "\n",
    "plt.pcolor(digit_p,cmap=\"hot\",vmin=0,vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "plt.title(\"mutant_reconstruction\")\n",
    "\n",
    "fig.add_subplot(411)\n",
    "\n",
    "plt.pcolor(abs(test_data_doubles_plus[0].reshape(digit_size,sequence_size)+(test_data_doubles_plus[mut_sample].reshape(digit_size,sequence_size))*2),cmap=\"hot\",vmin=0,vmax=3)\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"One hot encoding of sequences (wt(red), mut(yellow))\")\n",
    "\n",
    "fig.add_subplot(414)\n",
    "\n",
    "plt.pcolor(-digit_wt+digit_p,cmap=\"bwr\",vmin=-0.05,vmax=0.05)\n",
    "plt.colorbar()\n",
    "plt.xlim(0,82);\n",
    "plt.ylim(0,24);\n",
    "plt.yticks(range(24),ORDER_LIST)\n",
    "\n",
    "plt.title(\"difference in distribution (mutant-wt): Notice that the scale is different\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6lf8BzC5TlY"
   },
   "outputs": [],
   "source": [
    "sample_size=batch_size*int(len(test_data_doubles_plus)/batch_size)\n",
    "sample_for_averging_size=100\n",
    "sequence_size=PRUNED_SEQ_LENGTH\n",
    "digit_size = len(ORDER_LIST)\n",
    "x_decoded=vae.predict(test_data_doubles_plus[0:sample_size],batch_size=batch_size)\n",
    "\n",
    "digit = x_decoded[0].reshape(digit_size,sequence_size)\n",
    "digit_wt = normalize(digit,axis=0, norm='l1')\n",
    "wt_prob=compute_log_probability(test_data_doubles_plus[0].reshape(digit_size,sequence_size),digit_wt)\n",
    "print (\"wt_log_prob: \", wt_prob)\n",
    "\n",
    "wt_probs=[]\n",
    "digit_avg=np.zeros((digit_size,sequence_size))\n",
    "\n",
    "\n",
    "sample_indices=random.sample(range(sample_size),sample_for_averging_size)\n",
    "\n",
    "counter=0\n",
    "for sample in sample_indices:\n",
    "    digit = x_decoded[sample].reshape(digit_size,sequence_size)\n",
    "    digit_wt_i = normalize(digit,axis=0, norm='l1')\n",
    "    \n",
    "    digit_avg+=digit_wt_i*1./sample_for_averging_size\n",
    "    wt_p=compute_log_probability(test_data_doubles_plus[sample].reshape(digit_size,sequence_size),digit_wt_i)\n",
    "    wt_probs.append(wt_p)\n",
    "    counter+=1\n",
    "average_wt_p=np.mean(wt_probs)\n",
    "\n",
    "fitnesses_vs_wt=[]\n",
    "fitnesses=[]\n",
    "fitnesses_vs_avg=[]\n",
    "\n",
    "for sample in range(1,sample_size):\n",
    "    digit = x_decoded[sample].reshape(digit_size,sequence_size)\n",
    "    digit = normalize(digit,axis=0, norm='l1')\n",
    "    fitness=compute_log_probability(test_data_doubles_plus[sample].reshape(digit_size,sequence_size),digit)-wt_prob\n",
    "    fitnesses.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_doubles_plus[sample].reshape(digit_size,sequence_size),digit_wt)-wt_prob\n",
    "    fitnesses_vs_wt.append(fitness)\n",
    "    \n",
    "    fitness=compute_log_probability(test_data_doubles_plus[sample].reshape(digit_size,sequence_size),digit_avg)-average_wt_p\n",
    "    fitnesses_vs_avg.append(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_9MVUM35Tla"
   },
   "outputs": [],
   "source": [
    "predictions=np.array(list(map(lambda x: x*1./(4*max (fitnesses)),fitnesses))) #arbitrary renormalization\n",
    "indip=list(map(lambda x: x*1./max (exp_data_singles[\"effect_prediction_independent\"]),exp_data_singles[\"effect_prediction_independent\"]))\n",
    "epis=list(map(lambda x: x*1./max (exp_data_singles[\"effect_prediction_epistatic\"]),exp_data_singles[\"effect_prediction_epistatic\"]))\n",
    "plt.hist(predictions[predictions > -1E10],alpha=0.7,label=\"predicted fitness\"); #filter out -inf\n",
    "plt.hist(np.array(target_values_doubles)-1,alpha=0.5,label=\"experimental fitness\");\n",
    "plt.xlabel(\"relative fitness\");\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "idA7e0LV5Tlc"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "x1=predictions\n",
    "y1=target_values_doubles[:len(predictions)]\n",
    "\n",
    "plt.scatter(x1,y1,alpha=0.1)\n",
    "plt.plot(sorted(x1),sorted(y1),\"r\",linewidth=4,alpha=0.9)\n",
    "plt.title(str(spearmanr(x1,y1)))\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"experiment\")\n",
    "print(\"Mutual info:\", normalized_mutual_info_score(x1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfTm7lYJ5Tld"
   },
   "outputs": [],
   "source": [
    "size=len(predictions)\n",
    "fitness_data=pd.DataFrame(columns=[\"lin\",\"log\",\"pred_wt\",\"pred\",\"pred_avg\"])\n",
    "\n",
    "fitness_data[\"experimental\"]=exp_data_doubles[\"XY_Enrichment_score\"][:size]\n",
    "fitness_data[\"effect_prediction_epistatic\"]=exp_data_doubles[\"effect_prediction_epistatic\"][:size]\n",
    "fitness_data[\"effect_prediction_independent\"]=exp_data_doubles['effect_prediction_independent'][:size]\n",
    "\n",
    "fitness_data[\"prediction\"]=fitnesses\n",
    "fitness_data[\"prediction_wt\"]=fitnesses_vs_wt\n",
    "fitness_data[\"prediction_avg\"]=fitnesses_vs_avg\n",
    "\n",
    "corr_pred_doubles=fitness_data.corr(method=\"spearman\")\n",
    "corr_pred_doubles[[\"experimental\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyuQ-3qH5Tle"
   },
   "source": [
    "## Other datasets\n",
    "\n",
    "We chose this dataset for demonstration because there was the possiblitiy of testing the model on both the single and double mutants. This also happens to be the dataset that our model performs best compared to the epistatic model. We summarize the results from some other datasets in compiled by the Hopf et al. paper below.  \n",
    "\n",
    "**Important remark**: Note in the original version of this experiment we also ran this network on one viral dataset. However the size of the viral DNA (535), and diversity presented (requiring different re-weighting), relative to the size of this network made the training difficult to run to convergence. Hence we could not present the performance with the same confidence as the other datasets we tested our model on. Notably, in the viral case, the network did not perform better than the independent model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Fs1F-w-5Tlf"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2x1EPWA25Tlg"
   },
   "outputs": [],
   "source": [
    "meta_data=pd.DataFrame.from_csv(\"other_datasets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJN5Or0V5Tlh"
   },
   "outputs": [],
   "source": [
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cr3ORqdz5Tli"
   },
   "outputs": [],
   "source": [
    "meta_data[\"ratio\"]=meta_data[\"training_data_size(used)\"]*1./meta_data[\"alignment_length\"]\n",
    "meta_data[\"diff_eps\"]=meta_data[\"prediction_avg\"]/meta_data[\"effect_prediction_epistasis\"]\n",
    "meta_data[\"diff_ind\"]=meta_data[\"prediction_avg\"]/meta_data[\"effect_prediction_independent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4Q-BB1h5Tlj"
   },
   "outputs": [],
   "source": [
    "meta_data=meta_data.sort_values(by=[\"prediction_avg\"])\n",
    "meta_data=meta_data.reset_index()\n",
    "del meta_data[\"index\"]\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ipo4pqm45Tlo"
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"training_data_size(used)\",y=\"diff_eps\",data=meta_data,kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9m1JaN95Tlr"
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plt.plot(meta_data[\"prediction_avg\"],\"rx-\",label=\"prediction\",linewidth=2)\n",
    "#plt.plot(meta_data[\"prediction\"],\"r-\",label=\"prediction\")\n",
    "#plt.plot(meta_data[\"prediction_wt\"],\"r--\",label=\"prediction_wt\")\n",
    "plt.plot(meta_data[\"effect_prediction_epistasis\"],\"bx--\",label=\"epistatic model Hopf et al.\",linewidth=2)\n",
    "plt.plot(meta_data[\"effect_prediction_independent\"],\"bx:\",label=\"Independent model\",linewidth=2)\n",
    "ax.set_xticks(range(len(meta_data)))\n",
    "ax.set_xticklabels(meta_data.dataset,rotation=45)\n",
    "plt.xlabel(\"dataset\")\n",
    "plt.ylabel(\"Spearman rho\")\n",
    "plt.legend(bbox_to_anchor=(1.5,.75));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kv9yjZ5o5Tlt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zGZieRrf5TkP",
    "cJr1ybE85Tk_",
    "UQnhWqe15Tk_",
    "Uxm3ICVm5TlU",
    "SyuQ-3qH5Tle"
   ],
   "name": "VAE_for_protein_function_prediction-TORCH.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
